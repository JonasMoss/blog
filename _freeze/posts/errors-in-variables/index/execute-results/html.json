{
  "hash": "59e47dd5e63a06c01530256d6cf11f67",
  "result": {
    "markdown": "---\ntitle: \"An errors-in-variables model\"\nauthor: \"Jonas Moss\"\nformat: html\neditor: visual\ncache: true\ndate: \"3/2/2023\"\nfreeze: auto\ndate-format: \"MMM D, YYYY\"\ncategories: [statistics]\nbibliography: bibliography.bib\nfootnotes-hover: true\n---\n\n\n## Problem and solution\n\nSuppose we wish to estimate the regression coefficient for\n\n$$\nY_{0}\\mid X_{0}=\\alpha_{0}+\\beta_{0}X_{0}+\\sigma_{0}\\epsilon_{0},\\label{eq:original}\n$$\nwhere $E[\\epsilon\\mid X_{0}]=0$.\n\nHowever, we do not observe $Y_{0}$ and $X_{0}$. Instead, we observe\n$$\nY_{1}=Y_{0}+S_{Y}\\epsilon_{Y}\n$$\nfor some $\\delta$ with $E[\\delta\\mid Y_{0}]=0$ for some random variable\n$S$ and \n$$\nX_{1}=X_{0}+S_{X}\\epsilon_{X},\n$$\nfor some $\\eta$ with $E[\\delta\\mid Y_{0}]=0$.\n\nAs is well known, the regression coefficient $\\beta_{0}=\\frac{\\text{Cov}(X_{1},Y_{1})}{\\text{Var} X_{1}}$.\nHowever, $\\text{Cov}(Y_{1},X_{1})$ equals $\\text{Cov}(Y_{0},X_{0})$. About $\\text{Var} X_{1}$,\nwe can employ the Law of Total Variance \n$$\n\\text{Var} X_{1}=E\\text{Var}(X\\mid S_{X})+\\text{Var} E(X_{0}\\mid S_{X}).\n$$\nThe term $\\text{Var} E(X_{0}\\mid S_{X})$ vanishes, as $E(X_{0}\\mid S_{X})$\nis constant. Moreover, \n$$\n\\text{Var} X_{1}=\\text{Var} X_{0}+\\text{Var} S_{X},\n$$\nhence \n\\begin{equation}\n\\text{Var} X_{0}=\\text{Var} X_{1}-\\text{Var} S_{X}.\\label{eq:adjusted variance}\n\\end{equation}\n\nDefine the regression model \n$$\nY_{1}=\\alpha_{1}+\\beta_{1}X_{1}+(S_{Y}\\delta+\\sigma_{0}\\eta)\n$$\n\nIt follows that\n$$\n\\beta_{0}=\\frac{\\text{Cov}(Y_{1},X_{1})}{\\text{Var} X_{1}-\\text{Var} S_{X}}=\\beta_{1}\\frac{\\text{Var} X_{1}}{\\text{Var} X_{1}-\\text{Var} S_{X}},\n$$\nMoreover, \n\\begin{eqnarray*}\n\\alpha_{0} & = & EY_{0}-\\beta_{0}EX_{0},\\\\\n & = & EY_{1}-\\beta_{1}\\frac{\\text{Var} X_{1}}{\\text{Var} X_{1}-\\text{Var} S_{X}}EX_{1}.\n\\end{eqnarray*}\nIf $EX_{1}$ has been normalized to $0$, then $\\alpha_{0}=EY_{1}=\\alpha_{1}$.\n\nNotice that $Y_{1}$ has known errors. This makes it -- perhaps --\npossible to estimate $\\beta_{1}$ with additional precision, using\nsomething similar to weighted least squares. The weights would be\n$\\sqrt{S_{Y}^{2}+\\sigma^{2}}$. However, as $\\sigma^{2}$ is unknown,\nthe resulting regression would not truly be weighted least squares.\n\n## Verification\n\nLet's simulate a bunch of values from the model.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_fd0a76317c9b41bca59bc9609acda743'}\n\n```{.r .cell-code}\nn = 1000000\ns_x = sqrt(3)*rexp(n)\ns_y = 3*rexp(n)\ns_0 = 1\n\nx_0 = rnorm(n, 1, 2)\nx_1 = x_0 + s_x * rnorm(n)\ny_0 = 0.8 + 0.5 * x_0 + s_0*rnorm(n)\ny_1 = y_0 + s_y * rnorm(n)\n```\n:::\n\n\nThe calculated coefficients are\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_1832db607fccaae30ceca7b1b452d0d6'}\n\n```{.r .cell-code}\nbeta0_hat = cov(y_1, x_1)/(var(x_1) - 2*var(s_x))\nalpha0_hat = mean(y_1) - beta0_hat * mean(x_1)\nc(alpha0_hat, beta0_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7964205 0.4951465\n```\n:::\n:::\n\n\nBut the naive regression $Y_1 \\sim \\alpha_1 + \\beta_1X_1$ yields\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_c27557df9ba8afb21ceb7b781b898b78'}\n\n```{.r .cell-code}\nlm(y_1 ~ x_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y_1 ~ x_1)\n\nCoefficients:\n(Intercept)          x_1  \n     1.0927       0.1988  \n```\n:::\n:::\n\n\nOn the other hand, the correct (but unobserved) regression yields\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_0ef12d84ca31c8c42daed492bd6d52a4'}\n\n```{.r .cell-code}\nlm(y_0~x_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y_0 ~ x_0)\n\nCoefficients:\n(Intercept)          x_0  \n     0.7987       0.4999  \n```\n:::\n:::\n\n\n\n## Inference and literature\nTo do inference on this method, use the delta method and large-sample theory\n(together with the studentized bootstrap), or perhaps the bias-corrected\naccelerated bootstrap [(BCa)](https://stats.stackexchange.com/questions/437477/calculate-accelerated-bootstrap-interval-in-r).\nThe delta method should be fairly easy to derive using the formulation of the \"covariance of the covariance\" foundin e.g. Magnus and Neudecker's  Matrix differential calculus. \n\nThere is a sizable literature on error-in-variable models, and inference for this simple\nmodel has probably been worked out, but a very rudimentary search\nyielded nothing for me. I think it's uncommon to know the variances\nof the $X$ errors. Moreover, the problem can probably be cast in the language of structrual equation models. But I'm unsure if software (such as `lavaan`) will help, because you don't know the item variances in a typical application of structural equations models.\n\nA final option is to assume bivariate normality and use maximum likelihood. This is also likely to be possible using an `R` package, but I'm not sure the estimates would be consistent. Probably you'd have to use a sandwich matrix for correct standard errors. \n\nTo make things easy on yourself, if you're faced with a problem of this kind, I would suggest just going with the BCa + the equations above. The equations are trivial to compute and BCa will be fairly simple as well; it might be possible to calculate using packages such as `bootstrap`. Do something else only if the reviewers demand it.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}