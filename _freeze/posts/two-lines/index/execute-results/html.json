{
  "hash": "217ddd49e8fa607487222546fb22f932",
  "result": {
    "markdown": "---\ntitle: \"Correlation with two lines!\"\nauthor: \"Jonas Moss\"\nformat: html\neditor: visual\ncache: true\ndate: \"1/25/2023\"\nfreeze: auto\ndate-format: \"MMM D, YYYY\"\ncategories: [statistics, joking]\nbibliography: bibliography.bib\nfootnotes-hover: true\n---\n\n\n***What is this?*** A rant about an unimportant problem. Read at your own risk!\n\nEveryone knows the correlation coefficient. It measures the degree of linear dependence between two random variables $X$ and $Y$. It finds the best-fitting line $Y=a+bX$ and measures how far the observed values of $Y$ are from the predicted values of $y$ given $X$.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_2b4f19f50b024a4d6799bc867e864fcd'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Generate and plot some data from a line.\"}\nset.seed(313)\nn = 100\nx = seq(0, 1, length.out = n)\ny = 2 * x + 1 + rnorm(n) * 0.5\nplot(x, y, main = paste0(\"Sample correlation: \", round(cor(x,y), 3)))\nabline(lm(y ~ x))\n```\n\n::: {.cell-output-display}\n![Plot of data simulated from $y = 2x+1$.](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThat should be familiar. But what if there are TWO parallel lines in the data?!\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_aa758db91fa5e35cd0779480ac752acf'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Generate ad plot some data frome two lines!\"}\nset.seed(313)\npar(mfrow=c(1,2))\nn = 50\nx1 = seq(0, 1, length.out = n)\ny1 = 2 * x1 + 1 + rnorm(n) * 0.5\nx2 = seq(0, 1, length.out = n)\ny2 = 2 * x2 + 2 + rnorm(n) * 0.5\nplot(x1, y1, main = paste0(\"Sample correlation: ???\"), col = \"blue\", ylim = c(0, 6), xlab = \"x\", ylab = \"y\")\npoints(x2, y2, col = \"red\")\nabline(a = 1, b = 2, col = \"blue\")\nabline(a = 2, b = 2, col = \"red\")\nx = c(x1, x2)\ny = c(y1, y2)\nz = c(rep(0, 50), rep(1, 50))\nplot(x, y, main = paste0(\"Sample correlation: \", round(cor(x,y), 3)), xlab = \"x\", ylab = \"y\", ylim = c(0, 6))\nabline(a = 3/2, b = 2)\nabline(a = 1, b = 2, lty = 2)\nabline(a = 2, b = 2, lty = 2)\n```\n\n::: {.cell-output-display}\n![Plot of data simulated from $y = 2x+1$ and $y = 2x+2$!](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe data has been simulated from the model \\begin{eqnarray*}\ny\\mid x,z=0 & \\sim & 2x+1+\\frac{1}{2}\\epsilon,\\\\\ny\\mid x,z=1 & \\sim & 2x+2+\\frac{1}{2}\\epsilon.\n\\end{eqnarray*} where $z$ is a group indicator and $\\epsilon$ is standard normal. Moreover, the $x$s are $50$ observations uniformly spaced on $[0,1]$. What is the marginal model, with $z$ integrated out? It's $2x + 3/2$, displayed on the right. The plot on the right certainly doesn't look like it contains two lines.\n\nIf we know the joint distribution of $(X,Y,Z)$ , generalizing the correlation is easy! The ordinary correlation coefficient can be written as $\\operatorname{Cor}(X,Y)=\\operatorname{sign}\\beta\\sqrt{R^{2}}$,where $$R^{2}=1-\\frac{\\min_{a,b}E\\left[(Y-a-bX)^{2}\\right]}{\\min_{\\mu}E\\left[(Y-\\mu)^{2}\\right]},$$ and $\\beta$ is the minimizing slope of the numerator. Now just add the covariate $1[Z=1]$ to the linear regression model in the denominator, and the resulting correlation becomes $$\\operatorname{Cor}(X,Y;Z)=\\operatorname{sign}\\beta\\sqrt{R_\\star^{2}},$$ where $$R^{2}_\\star=1-\\frac{\\min_{a,b}E\\left[(Y-a_{0}-a_{1}1[Z=1]-bX)^{2}\\right]}{\\min_{\\mu}E\\left[(Y-\\mu)^{2}\\right]},$$ and $\\beta$ is the minimizing slope of the numerator (the sign of the slope is unambiguous since we've assumed the lines are parallel).\n\nCalculating this correlation coefficient is easy as pie.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_06112a65b44cf239e93ce2bdbef2f48d'}\n\n```{.r .cell-code}\nmod = lm(y ~ x + z)\nunname(sqrt(summary(mod)$r.sq) * sign(coef(mod)[3]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8495882\n```\n:::\n:::\n\n\nAnd its much larger than the \"one-line correlation\", going from $0.62$ to $0.85$. But we can't calculate it without knowing $Z$!\n\n## Defining a correlation for two parallel lines\n\nSo, we know the joint distribution of $(x,y)$, and nothing more. How can think about the \"two-lines correlation\"? I can only think of one reasonable way to solve the problem. We need to **find lower and upper bounds for** $\\operatorname{Cor}(X,Y;Z)$. Because we might not know $Z$, but maybe we can find bounds for the correlation $\\operatorname{Cor}(X,Y;Z)$ that we could have calculate had we known $Z$.\n\n### Finding bounds\n\nI can't think of an efficient way to calculate upper bounds, but the lower bound is simple enough (in the case when the correlation is positive): It equals $\\operatorname{Cor}(X,Y)$. When dealing with estimation problems, a reasonable estimator of the upper bound would be to make every possible split of the data into into two categories (according to `z)`and fit the regression model `y ~ x + z.` But that would be $2^n$ combinations, which is obviously intractable for large $n$. It might be possible to find exact algorithms that aren't exponential in time, indeed, I would expect it, but I can't justify spending more time on this problem.\n\nHowever, consider the following data\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_ba4b2678cbfb459afaf46c7c97dae5d8'}\n\n```{.r .cell-code}\nn <- 15\nx <- mtcars$drat[1:n]\ny <- mtcars$wt[1:n]\n```\n:::\n\n\nSince $2^{15}= 32768$ we don't need to fit *that* many linear regressions to estimate the upper correlation bound in this case.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_6106da6ca30aa2f8a55e600c4deb8590'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code estimating a bunch of regression models.\"}\nn <- 15\nx <- mtcars$drat[1:n]\ny <- mtcars$wt[1:n]\nn <- length(x)\n\nmat <- cbind(1, x, 0)\nminimum = Inf\n\nfor(k in seq(0:n)) {\n  icomb <- arrangements::icombinations(n, k)\n  choose(n, k)\n  for(i in seq(choose(n, k))) {\n    indices <- icomb$getnext()\n    mat[, 3] <- 0\n    mat[indices, 3] <- 1\n    mod <- lm.fit(mat, y)\n    current <- sum(mod$residuals^2)\n    if (current < minimum) {\n      minimum = current\n      z = indices\n      coef <- mod$coefficients\n    }\n  }  \n}\n\ncor2 <- sqrt(1 - minimum/sum((y - mean(y))^2)) * sign(coef[2])\ncor1 <- cor(x, y)\n```\n:::\n\n\nFrom the code above we find that lower and upper bounds for the two lines correlation:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_40179df886da4f94f97b044d9f19fe8f'}\n\n```{.r .cell-code}\nc(lower = cor2, upper = cor1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   lower.x      upper \n-0.8834406 -0.6358763 \n```\n:::\n:::\n\n\nLet's also have a look at the classification of points that maximizes this correlation.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_2cd15096587486473bc8a277c30889f9'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for plot.\"}\ncol <- rep(\"blue\", n)\ncol[z] <- \"red\"\nplot(x, y, col = col, xlab = \"Rear axle ratio\", ylab = \"Weight\",\n     pch = 20, main = \"Two lines or one line?\")\nabline(a = coef[1], b = coef[2], col = \"blue\")\nabline(a = coef[1] + coef[3], b = coef[2], col = \"red\")\nabline(lm(y ~ x))\n```\n\n::: {.cell-output-display}\n![Plot of correlation with two lines.](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe two lines correlation for this optimal classification equals the lower bound above.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}