{
  "hash": "54d956c2fb5ddadd26ced1983b8dc7ce",
  "result": {
    "markdown": "---\ntitle: \"Matching quantiles to the Kumaraswamy distribution\"\nauthor: \"Jonas Moss\"\nformat: html\neditor: visual\ncache: true\ndate: \"2/24/2023\"\nfreeze: auto\ndate-format: \"MMM D, YYYY\"\ncategories: [effective altruism, fermi estimates, statistics, forecasting]\n---\n\n\n## Introduction\n\nSometimes, in particular when doing informal forecasting, you would like to match quantiles to distributions. For real data, the most common choice is the normal distribution. For positive data with \"multiplicative qualities\", so to speak, the most common choice is the log-normal, but the truncated normal can also be reasonable. But what should you do when dealing with data on the $[0,1]$?\n\nDistributions of (transformed) location-scale families are easy to match quantiles to, involving only algebraic manipulations with the cumulative distribution function. And there are infinitely many transformed location-scale families living on $[0,1]$: For any location-scale family distribution $X$, such as the normal, and cumulative distribution function $G$ (on $\\mathbb{R}$), you can define the transformed family $G(\\mu + \\sigma X)$, living on $[0,1]$. Then you can then match quantiles to the original family by mapping $x\\to G^{-1}(x)$ and $y\\to G^{-1}(y)$. The most natural candidate here is, perhaps, the [logit-normal](https://en.wikipedia.org/wiki/Logit-normal_distribution). These distributions sometimes have poor looks, see e.g., the plot in the linked article, but could be an excellent choice.\n\nAside from transformed location-scale families, the most obvious way would be to match quantiles using the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution). But the beta distribution has a troublesome cumulative distribution function that is not available in every programming language. Moreover, for the beta distribution, matching quantiles is not a simple algebraic manipulation. Instead, you need to do numerical optimization. This increases the complexity of the procedure, both in terms of resources consumed and reliability, as it is hard to make sure the numerical optimization works all the time.\n\nIn this post I describe how to efficiently match quantiles for the [Kumaraswamy distribution](https://en.wikipedia.org/wiki/Kumaraswamy_distribution). The Kumaraswamy distribution is visually indistinguishable from the Beta distribution, but is sometimes easier to work with mathematically. In particular, the cumulative distribution function of a Kumaraswamy distribution with parameters $a,b$ is $F(x;a,b) = 1-(1-x^a)^b$. Compare this to the Beta distribution, whose cumulative distribution function is the [incomplete regularized beta function](https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function) - the difference in complexity is striking. The Kumaraswamy distribution has some downsides though, most notably that the rÃ´les of $a$ and $b$ aren't symmetric. In other words, so you can't be sure that the Kumaraswamy with equal $a$ and $b$ parameters is symmetric. Moreover, it's not a conjugate distribution for the binomial, and the expectations doesn't have as simple closed form as the beta distribution.\n\nBut if your goal is to match quantiles to a general purpose and reasonable-looking distribution, then look no further!\n\n## Doing the matching\n\nWe'll do the matching using Newton--Raphson, but first we'll cover the following steps:\n\n1. **Reduce the number of equations.** By finding an expression for $b$ in terms of $a$, we can reduce the problem from a two-dimensional Newton--Raphson to a one-dimensional Newton--Raphson.\n2. **Do a rough analysis of the objective function.** A rough analysis of the objective function suggests a very natural starting value.\n3. **Do Taylor expansions to prevent numerical instability.** The objective function and its derivative are sometimes poorly behaved. We use Taylor expansions to prevent numerical instability.\n\nThe resulting is a function `matchkumar(x,y,alpha,beta)` that is both fast and reasonably reliable.\n\n### 1. Reduce the number of equations \nThe cumulative distribution function is $F(x)=1-(1-x^{a})^{b}$. Suppose we want to match the quantiles $\\alpha$ and $\\beta$ to $x$ and $y$. That's equivalent to\n\\begin{eqnarray*}\nF(x_{0}) & = & \\alpha=1-(1-x^{a})^{b},\\\\\nF(x_{1}) & = & \\beta=1-(1-y^{a})^{b}.\n\\end{eqnarray*}\nNow we need to solve for $a,b$. To do this, we'll first solve for\n$b$, yielding\n\\begin{eqnarray*}\n1-\\alpha & = & (1-x^{a})^{b},\\\\\n\\log(1-\\alpha) & = & b\\log(1-x^{a}).\n\\end{eqnarray*}\nWriting $b$ as a function of $a$, we find\n$$\nb(a)=\\frac{\\log(1-\\alpha)}{\\log(1-x^{a})}.\n$$\nNow solve for $a$ and find \n$$\na(b)=\\frac{\\log\\left(1-(1-\\beta)^{1/b}\\right)}{\\log(y)}.\n$$\nPlug in the definition of $b(a)$ into the definition of $a(b)$ to\nfind\n$$\na=\\frac{\\log\\left(1-(1-\\beta)^{\\frac{\\log(1-x^{a})}{\\log(1-\\alpha)}}\\right)}{\\log(y)}\n$$\nOur goal is to solve this equation for $a$, then plug the result\ninto $b(a)$ to find $b$. But we can't solve for $b$ analytically. \n\nTo do this, let's define \n$$\n\\delta=(1-\\beta)^{1/\\log(1-\\alpha)},\\quad g(a)=\\delta^{\\log(1-x^{a})},\n$$\nand we need to solve \n$$\nf(a)=a\\log y-\\log(1-g(a))=0.\n$$\nThe derivative of this one is\n$$\nf'(a)=\\log y-\\log(x)\\log\\delta\\frac{x^{a}g(a)}{(1-x^{a})(1-g(a))}.\n$$\n\n### 2. Function analysis\n\nThe function $f(a)$ has an oblique asymptote as $a\\to\\infty$. Using\nTaylor expansions, one may verify that its slope is $\\log(y/x)$ and\nits intercept $-\\log\\log\\delta.$ In other words, $\\lim_{a\\to\\infty}f(a)-h(a)=0$, where $h(a)=-\\log\\log\\delta+\\log(y/x)a$. From what I can see, $h(a)$ approximates $f(a)$ from below. This is handy, as it is guaranteed that $f(a_{0})>0$, where $a_{0}=\\log\\log\\delta/\\log(y/x)$ is the solution to $h(a)=0$. This will typically provide an excellent starting point.\n\n### 3. Taylor approximations\nThe function $g(a)$ is behaves poorly numerically when $x^a$ becomes small, as it gets to close to $1$ and promotes numerical instability. To fix this, we use Taylor approximations for $f$ and $f'$ when $x^a$ is sufficiently small. \n\nThe first-order Taylor expansion of $\\log(1-\\delta^{\\log(1-x^a)})$ (around $x^a=0$), used in $f(a)$, is\n$$\na\\log x+\\log\\log\\delta+\\frac{1}{2}x^{a}(1-\\log\\delta)+O(x^{a})\n$$\n\nThe first-order Taylor expansion of $f'(x^a)$ as a function of $x^a$ is \n$$\\log y - \\log x \\cdot \\frac{1}{2}(1+\\log\\delta)x^a+O(x^{2a})$$\nWe will use these approximations when $x^a<4\\cdot10^{-6}$.\n\nIn addition, we will employ several other tricks to ensure numerical stability.\n\n## Code for `matchkumar`\n\nBelow is `R` code for `matchkumar`. The function does not use any\nR-specific functionality, so it should be easily portable to any language of \nyour choosing, such as JavaScript, Rust, or Crystal. The vectorization used in\nthe functions `f` and `f_deriv` below is not strictly necessary, as they are\nonly called using scalar arguments.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-1_08598f90fb6bba475dab74f2aa8d3135'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for `matchkumar`\"}\n#' Match quantiles for the Kumaraswamy distributon.\n#'\n#' @param alpha,beta Upper and lower quantiles to match.\n#' @param x,y Values at those quantiles.\n#' @param eps,iter_lim Used in the Newton-Raphson algorithm.\n#' @param bound When x^a is smaller than this bound, use first-order\n#'    Taylor approximation.\n#' @return The  a and b parameters for the Kumaraswamy distribution so that\n#'  extraDistr::qkumar(alpha, a, b) = x and\n#'  extraDistr::qkumar(beta, a, b) = y\n\nmatchkumar <- \\(alpha, beta, x, y, eps = 1e-04, iter_lim = 100, bound = 4e-6) {\n  log_y <- log(y)\n  log_x <- log(x)\n  g <- \\(x_a) (1 - beta)^(log(1 - x_a) / log(1 - alpha))\n  log_delta <- log(1 - beta) / log(1 - alpha)\n\n  f <- \\(a) {\n    f_taylor <- \\(a) {\n      log_y * a - (a * log(x) + log(log_delta) + 0.5 * x^a * (1 - log_delta))\n    }\n\n    f_not_taylor <- \\(a) {\n      log_y * a - log(1 - g(x^a))\n    }\n\n    vec <- (x^a < bound)\n    out <- rep(0, length(a))\n    out[vec] <- f_taylor(a[vec])\n    out[!vec] <- f_not_taylor(a[!vec])\n    out\n  }\n\n  f_deriv <- \\(a) {\n    f_deriv_not_taylor <- \\(a) {\n      x_a <- x^a\n      delta_xa <- g(x_a)\n      log_y - log_x * log_delta * x_a * delta_xa / ((1 - x_a) * (1 - delta_xa))\n    }\n\n    f_deriv_taylor <- \\(a) {\n      x_a <- x^a\n      -log_x + log_y + 0.5 * (log_delta - 1) * log_x * x_a\n    }\n\n    vec <- (x^a < bound)\n    out <- rep(0, length(a))\n    out[vec] <- f_deriv_taylor(a[vec])\n    out[!vec] <- f_deriv_not_taylor(a[!vec])\n    out\n  }\n\n  minimum <- .Machine$double.eps\n\n  a_to_b <- \\(a) {\n    x_a <- x^a\n    attempt <- log(1 - alpha) / log(1 - x_a)\n    result <- if (is.infinite(attempt)) {\n      log(1 - alpha) * (-1 / x_a + 1 / 2 + x_a / 12 + x_a^2 / 24)\n    } else {\n      attempt\n    }\n    max(result, minimum)\n  }\n\n  a0 <- log(log_delta) / log(y / x)\n\n  for (i in seq(iter_lim)) {\n    a1_ <- a0 - f(a0) / f_deriv(a0)\n    a1 <- if (is.na(a1_)) minimum else max(a1_, minimum)\n    rel_error <- (a0 - a1) / a1\n    if (abs(rel_error) < eps | a0 == minimum) break\n    a0 <- a1\n  }\n\n  result <- c(a = a0, b = a_to_b(a0))\n  attr(result, \"iter\") <- i\n  result\n}\n```\n:::\n\n\n## Example\n\nLet's see how the function performs on an example:\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-2_ec6eef34ed7cd578e07864bb5eade419'}\n\n```{.r .cell-code}\nx <- 0.55\ny <- 0.6\nalpha <- 0.1\nbeta <- 0.90\nmatched <- matchkumar(alpha, beta, x, y)\nresults <- c(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n```\n:::\n\n\nNow we see that the results are as intended:\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-3_4642bb7e690185aced5a47a62152a473'}\n\n```{.r .cell-code}\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_matched y_matched \n     0.55      0.60 \n```\n:::\n:::\n\n\nWe typically do not need a large number of iterations; in this case it's just 1!\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-4_dcbb9668b2b1b73b90b43bf340435f6e'}\n\n```{.r .cell-code}\nattr(matched, \"iter\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nHere's a plot of the resulting density.\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-5_ad2d0f814b6b0d2840888b538dcbe5fb'}\n::: {.cell-output-display}\n![](kumaraswamy_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Performance\n\nThe example above is not sufficient to prove the method works. To verify a \nsolution works, let's check the squared distance between the implied quantiles\nand the desired quantiles.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-6_74fbf855d5a1400b917e1f8c8ab78259'}\n\n```{.r .cell-code}\nverify <- \\(args) {\n  x <- args[1]\n  y <- args[2]\n  alpha <- args[3]\n  beta <- args[4]\n  matched <- matchkumar(alpha, beta, x, y)\n  x_matched <- extraDistr::qkumar(alpha, matched[1], matched[2])\n  y_matched <- extraDistr::qkumar(beta, matched[1], matched[2])\n  (x - x_matched)^2 + (y - y_matched)^2\n}\n```\n:::\n\n\nGenerating a bunch of random values here.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-7_8647330c52114193201b6a41c97f27e4'}\n\n```{.r .cell-code}\nset.seed(313)\nn <- 1000\nargs <- matrix(replicate(n, c(sort(runif(2)), sort(runif(2)))),\n               ncol = 4, byrow = TRUE)\nresults <- apply(args, 1, verify)\nplot(results, log = \"y\", ylab = \"Error\", xlab = \"Index\")\n```\n\n::: {.cell-output-display}\n![](kumaraswamy_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Sources of problems\n\nThe function works very well for most randomly generated values, but not for absolutely all of them. Sadly, I don't think it's possible to solve this problem without using arbitrary precision numbers. For the problem lies not in the calculation of $a$ itself, which works very well, but in the calculation of $b$. For some values of $a$ we are not able to calculate the value of $b$ sufficiently precisely using floating point arithmetic. Moreover, `extraDistr::qkumar` is not able to handle these extreme inputs. I strongly doubt this is would be a problem in practice, as the random values exhibiting these problems are quite unrealistic, and can be avoided altogether by scaling the Kumaraswamy distribution.\n\nConsider the following.\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-8_651aa985b649711a286936954cd00378'}\n\n```{.r .cell-code}\nx = 0.6503457\ny = 0.6656772\nalpha = 0.2139453\nbeta = 0.894129\n```\n:::\n\n\nHere $x$ and $y$ are very close, but $\\alpha$ and $\\beta$ far apart, which forces a large amount of mass in a very small space. In these cases $a$ appears to be calculated to great accuracy.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-9_a89c5093dcb2e3727fedd4b98c54aebc'}\n\n```{.r .cell-code}\nmatched = matchkumar(alpha, beta, x, y)\nmatched \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           a            b \n9.583478e+01 1.944493e+17 \nattr(,\"iter\")\n[1] 1\n```\n:::\n:::\n\nBut the exceedingly large $a,b$s cannot be handled by the `extraDistr::qkumar`.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-10_325ccf90b4e27b79f67240ca9a80651c'}\n\n```{.r .cell-code}\nc(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_matched y_matched \n        0         0 \n```\n:::\n:::\n\n\nAnother sort of trouble-maker occurs when $a$ is too close to $0$, i.e., closer than the machine epsilon. \n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-11_0a8508fb4b01c0571675b93d8b895ba8'}\n\n```{.r .cell-code}\nx = 0.1383447\ny = 0.794063\nalpha = 0.9214318\nbeta = 0.9266641\n```\n:::\n\n\nIt's not a surprise that these values are hard to fit, as the $x,y$ values are far apart but $\\alpha$ and $\\beta$ very, very, close. In this case, `matchkumar` yields\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-12_44e2b2284c979080c52759b3bb7945b3'}\n\n```{.r .cell-code}\nmatched = matchkumar(alpha,beta,x,y)\nmatched\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           a            b \n2.220446e-16 7.195903e-02 \nattr(,\"iter\")\n[1] 11\n```\n:::\n:::\n\n\nBut the matched quantiles are not as close as we would like.\n\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-13_d476d3c61009f3cdcfbd250f8a3d805b'}\n\n```{.r .cell-code}\nc(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_matched y_matched \n0.1353353 0.3678794 \n```\n:::\n:::\n\n\nBut as the Kumaraswamy distribution is parameterized, we can't get the $a$ much closer to 0 than this.\n\n### Number of iterations\nMost of the time, the number of iterations (capped at $100$) is small. However, the cap is sometimes reached. In most cases, this is not cause for alarm. This is because we evaluate the solutions using the distance from the desired quantiles, but they are evaluated using the relative error in the Newton--Raphson loop. And the first will often be small even if the second is \"large\". Moreover, the large numbers of iterations happens when $a$ is very, very small, which is unlikely to happen in applications.\n\n::: {.cell hash='kumaraswamy_cache/html/unnamed-chunk-14_660d1c4e042314ecdc99f32240a9b8d2'}\n::: {.cell-output-display}\n![](kumaraswamy_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}