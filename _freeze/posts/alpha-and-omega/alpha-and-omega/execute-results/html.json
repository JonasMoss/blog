{
  "hash": "cfa264a96838edba01bffdd92cdc06b6",
  "result": {
    "markdown": "---\ntitle: \"No one would have invented coefficient alpha today\"\nauthor: \"Jonas Moss\"\neditor: visual\ndate: \"10/2/2022\"\ndate-format: \"MMM D, YYYY\"\ncrossref:\n  eq-prefix: eq.\ncategories: [statistics, psychometrics]\nbibliography: alpha-and-omega.bib\n---\n\n\nCoefficient alpha is the most famous coefficient in psychometrics -- Cronbach's paper *[Coefficient alpha and the internal structure of tests](https://scholar.google.no/scholar?hl=en&as_sdt=0%2C5&q=Coefficient+alpha+and+the+internal+structure+of+tests&btnG=)* has been cited around $60,00$ times after all. It's supposed to measure *reliability*. What does that mean? Intuitively, a psychometric scale is supposed to measure some kind of psychological construct, such as intelligence, in a reliable way. You don't want it to be noisy. You don't want two intelligence tests administered at slightly different times to give widely different results. You also want the test to actually measure intelligence, and not something else, such emotionality. But that's validity, not reliability.\n\nIn classical test theory they think about psychometric tests in the context of a true score $T$ and observed score $X$. Then they write $X = T +\\epsilon$ for some error term $\\epsilon$. (This is always possible). For instance, if $X$ is the observed score on an IQ test, $T$ is the true, underlying intelligence.\n\nNow we're ready to define classical reliability! The reliability is defined as $R=\\text{Cor}(X, T)^2$, the squared correlation between the true score and the observed score. That's a pretty reasonable definition! But the problem is that you don't know the true score, so you can't estimate this correlation directly.\n\nAnd that's where coefficient alpha comes in. Suppose that $X$ is a *sum score*, i.e., on the form $X = X_1 + X_2 + \\ldots + X_k$. Then assume that $$X_i = \\mu_i + \\lambda_i T + \\sigma_i \\epsilon_i$$ {#eq-congeneric} for some error terms with variance equal to $1$. In addition, assume that $\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0$ when $i = j$, i.e., the error terms are uncorrelated. Then $X$ follows the *congeneric measurement model*. Now define coefficient alpha as $$\\alpha=\\frac{k}{k-1}\\left(1-\\frac{\\text{tr}\\Sigma}{{1}^{T}\\Sigma{1}}\\right),$$ where $\\Sigma$ is the covariane matrix of the $X_i$s. This alpha has two important properties:\n\n1.  When the congeneric measurement model holds, $R\\geq \\alpha$.In other words, $\\alpha$ is a lower bound for the true reliability.\n2.  If, in addition, if the true model is $\\tau$-equivalent, then $R = \\alpha$.\n\nThe condition of $\\tau$-equivalence means that all the factor loadings $\\lambda_i$ are equal, i.e., the congeneric model @eq-congeneric is reduced to $$X_i = \\mu_i + \\lambda T + \\sigma_i \\epsilon_i.$$ {#eq-tau-equivalent} There is widespread agreement that $\\tau$-equivalence never holds.\n\nAlpha is also the mean of all possible split-half reliabilities, but no one seem to care much about that today. It might have been important earlier on though. For more details see [@Cho2021-gk].\n\n## Why no one would have invented alpha today\n\nTake a look at the congeneric model (@eq-congeneric) again. This is a linear one-factor model with no correlation among the errors. In the early 20th century, there was no feasible way to estimate such models, at least not for ordinary psychometric researchers. But that's not the case anymore. Anyone able to install `R` and run the easiest `lavaan` script can estimate such a model. For example, we can estimate the parameters for the agreeableness part of the `psychTools::bfi` data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"lavaan\")\nmodel <- \" f =~ A1 + A2 + A3 + A4 + A5 \"\nbfi <- psychTools::bfi[, 1:5]\nbfi[, 1] <- -bfi[, 1] # Reverse-coded question.\nobj <- lavaan::cfa(model, data = bfi, std.lv = TRUE)\nknitr::kable(round(coef(obj), 3))\n```\n\n::: {.cell-output-display}\n|       |     x|\n|:------|-----:|\n|f=~A1  | 0.528|\n|f=~A2  | 0.774|\n|f=~A3  | 0.994|\n|f=~A4  | 0.717|\n|f=~A5  | 0.791|\n|A1~~A1 | 1.693|\n|A2~~A2 | 0.784|\n|A3~~A3 | 0.714|\n|A4~~A4 | 1.694|\n|A5~~A5 | 0.965|\n:::\n:::\n\n\nThe `std.lv = TRUE` argument forces the latent variable $T$ to have variance equal to $1$. This option makes it easier to interpret the remaining parameters. Using the estimated parameters of the `lavaan` object we can estimate the reliabiltiy $R$ with little problems. Straight-forward calculations show that, when we assume the congeneric measurement model, $$R = \\text{Cor}^{2}(X_1 + X_2+\\cdots+X_k,T)=\\frac{k\\overline{\\lambda}^{2}}{k\\overline{\\lambda}^{2}+\\overline{\\sigma^{2}}},$$ {#eq-reliability} where $\\overline{x}$ denotes the mean of the vector $x$.\n\nUsing this formulation of the reliability in terms of $\\lambda$ and $\\sigma$, the natural estimator if the reliability is the *plug-in estimator* $$\\hat{R} = \\frac{k\\overline{\\hat{\\lambda}}^{2}}{k\\overline{\\hat{\\lambda}}^{2}+\\overline{\\hat{\\sigma}^{2}}},$$ where $\\hat{\\lambda}$ and $\\hat{\\sigma}$ are estimators of your choice. If you're using `lavaan`, you would calculate it using something like the following function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Estimate the reliability from a `lavaan` object.\n#' \n#' Estimate the reliability assuming a congeneric measurement model using the\n#'   plug-in estimator.\n#' @param obj A `lavaan` object.\n#' @return The estimated reliability coefficient.\nreliability <- function(obj) {\n  params <- lavaan::lavInspect(obj, what = \"coef\")\n  lambda <- params$lambda\n  sigma2 <- diag(params$theta)\n  k <- length(lambda)\n  k * mean(lambda) ^ 2 / (k * mean(lambda) ^ 2 + mean(sigma2))\n}\n```\n:::\n\n\nFor the agreeableness data of `psychTools::bfi`, our reliability becomes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreliability(obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.712129\n```\n:::\n:::\n\n\nIt's not hard to do inference for the reliability coefficient either -- it is merely an application of the [delta method](https://en.wikipedia.org/wiki/Delta_method). Moreover, by the invariance principle of maximum likelihood estimation, it is the maximum likelihood estimator of the reliability provided $\\lambda$ and $\\sigma$ are estimated using maximum likelihood. In this sense the estimator is *natural*.\n\nTo sum up:\n\n1.  There exists a natural estimator of the reliability under the congeneric measurement model.\n2.  It is easy to estimate and it's efficient under normality.\n3.  Its asymptotic theory is well-understood. That's just a by-product of the immense amount of research on asymptotic theory for general structural equation models.\n\nI doubt anyone would have wanted to investigate alternatives to this 100% reasonable, simple, and efficient estimator. For why would they? It's like finding an alternative estimator of the $R^2$ that requires a little less computation power but is only consistent for the $R^2$ under extraordinarily unlikely assumptions.\n\nMaybe someone would have uncovered coefficient alpha today, but its discovery would be treated as a novelty, not something of widespread importance, worth $60,000$ citations.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}