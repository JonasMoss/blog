[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Jonas Moss, a statistics post doc at BI Norwegian Business School. I’m interested in programming, the replication crisis in psychology, theoretical statistics, forecasting, effective altruism, and rationalism (as in Lesswrong)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog posts",
    "section": "",
    "text": "effective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2022\n\n\nJonas Moss\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html",
    "href": "posts/value-estimation/value-estimation.html",
    "title": "Estimating value from pairwise comparisons",
    "section": "",
    "text": "How can you estimate the value of research output? One way is to do pairwise comparisons, e.g., to ask specialists how much more valuable Darwin’s The Original of Species is than Dembski’s Intelligent Design. You can use these relative valuations to estimate absolute valuations."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#summary",
    "href": "posts/value-estimation/value-estimation.html#summary",
    "title": "Estimating value from pairwise comparisons",
    "section": "Summary",
    "text": "Summary\n\nEstimating values is hard. One way to elicit value estimates is ask researchers to compare to different items \\(A\\) and \\(B\\), asking how much better \\(A\\) is than \\(B\\). This makes the problem more concrete than just asking “what is the value of \\(A\\)?”. The Quantified Uncertainty Institute has made an app for doing this kind of thing, described here.\nNuño Sempere had a post about eliciting comparisons of research value from \\(6\\) effective altruism researchers.\nThis post proposes some technical solutions to problems introduced to me in Nuño’s post. In particular, it includes principled ways to\n\nEstimate subjective values,\nMeasure consistency in pairwise value judgments,\nMeasure agreement between the raters,\nAggregate subjective values.\n\nI use these methods on the data from Nuño’s post.\n\nI’m assuming you have read Nuño’s post recently. I think this post will be hard to read if you haven’t.\nThis is written as a self-contained Quarto file. I’ve hidden most of the R code to make it readable, but it’s available in source.\n\nWhat’s this about\nThis table contains the \\(36\\) responses from Gavin Leech. As you can see, he values Superintelligence \\(100\\) more than the Global Priorities Institute’s Research Agenda.\n\n\n\n\n\n\n\n\n\n\nsource\ntarget\ndistance\n\n\n\n\nThinking Fast and Slow\nThe Global Priorities Institute’s Research Agenda\n100\n\n\nThe Global Priorities Institute’s Research Agenda\nThe Mathematical Theory of Communication\n1000\n\n\nSuperintelligence\nThe Mathematical Theory of Communication\n10\n\n\nCategorizing Variants of Goodhart’s Law\nThe Vulnerable World Hypothesis\n10\n\n\nShallow evaluations of longtermist organizations\nThe motivated reasoning critique of effective altruism\n10\n\n\nShallow evaluations of longtermist organizations\nCategorizing Variants of Goodhart’s Law\n100\n\n\nThe motivated reasoning critique of effective altruism\nCategorizing Variants of Goodhart’s Law\n10\n\n\nShallow evaluations of longtermist organizations\nThinking Fast and Slow\n10\n\n\nThinking Fast and Slow\nThe motivated reasoning critique of effective altruism\n1\n\n\nThe motivated reasoning critique of effective altruism\nThe Global Priorities Institute’s Research Agenda\n1000\n\n\nCategorizing Variants of Goodhart’s Law\nThe Global Priorities Institute’s Research Agenda\n100\n\n\nThe Vulnerable World Hypothesis\nThe Global Priorities Institute’s Research Agenda\n10\n\n\nReversals in Psychology\nA Model of Patient Spending and Movement Building\n10\n\n\nDatabase of orgs relevant to longtermist/x-risk work\nWhat are some low-information priors that you find practically useful for thinking about the world?\n5\n\n\nReversals in Psychology\nDatabase of orgs relevant to longtermist/x-risk work\n1\n\n\nDatabase of orgs relevant to longtermist/x-risk work\nA Model of Patient Spending and Movement Building\n10\n\n\nWhat are some low-information priors that you find practically useful for thinking about the world?\nA Model of Patient Spending and Movement Building\n2\n\n\nCenter for Election Science EA Wiki stub\nExtinguishing or preventing coal seam fires is a potential cause area\n1000\n\n\nA comment on setting up a charity\nCenter for Election Science EA Wiki stub\n10\n\n\nA comment on setting up a charity\nReversals in Psychology\n200\n\n\nCenter for Election Science EA Wiki stub\nReversals in Psychology\n20\n\n\nReversals in Psychology\nExtinguishing or preventing coal seam fires is a potential cause area\n50\n\n\nDatabase of orgs relevant to longtermist/x-risk work\nExtinguishing or preventing coal seam fires is a potential cause area\n50\n\n\nWhat are some low-information priors that you find practically useful for thinking about the world?\nExtinguishing or preventing coal seam fires is a potential cause area\n10\n\n\nA Model of Patient Spending and Movement Building\nExtinguishing or preventing coal seam fires is a potential cause area\n1\n\n\nA comment on setting up a charity\nShallow evaluations of longtermist organizations\n1000\n\n\nCenter for Election Science EA Wiki stub\nShallow evaluations of longtermist organizations\n100\n\n\nReversals in Psychology\nShallow evaluations of longtermist organizations\n100\n\n\nDatabase of orgs relevant to longtermist/x-risk work\nShallow evaluations of longtermist organizations\n100\n\n\nThinking Fast and Slow\nThe Mathematical Theory of Communication\n1e+05\n\n\nWhat are some low-information priors that you find practically useful for thinking about the world?\nShallow evaluations of longtermist organizations\n100\n\n\nShallow evaluations of longtermist organizations\nA Model of Patient Spending and Movement Building\n1\n\n\nThinking Fast and Slow\nA Model of Patient Spending and Movement Building\n1000\n\n\nThe motivated reasoning critique of effective altruism\nA Model of Patient Spending and Movement Building\n5\n\n\nA Model of Patient Spending and Movement Building\nCategorizing Variants of Goodhart’s Law\n10\n\n\nExtinguishing or preventing coal seam fires is a potential cause area\nCategorizing Variants of Goodhart’s Law\n10000\n\n\nThinking Fast and Slow\nThe Mathematical Theory of Communication\n1e+05\n\n\nThe Global Priorities Institute’s Research Agenda\nSuperintelligence\n100\n\n\n\n\n\nMy first goal is to take relative value judgments such these and use them to estimate the true subjective values. In this case, I want to estimate the value that Gavin Leech places on every item in the table above, i.e.,\n\n\n\n\n\n\n\n\n\n\nlevels\n\n\n\n\n1\nA comment on setting up a charity\n\n\n2\nA Model of Patient Spending and Movement Building\n\n\n3\nCategorizing Variants of Goodhart’s Law\n\n\n4\nCenter for Election Science EA Wiki stub\n\n\n5\nDatabase of orgs relevant to longtermist/x-risk work\n\n\n6\nExtinguishing or preventing coal seam fires is a potential cause area\n\n\n7\nReversals in Psychology\n\n\n8\nShallow evaluations of longtermist organizations\n\n\n9\nSuperintelligence\n\n\n10\nThe Global Priorities Institute’s Research Agenda\n\n\n11\nThe Mathematical Theory of Communication\n\n\n12\nThe motivated reasoning critique of effective altruism\n\n\n13\nThe Vulnerable World Hypothesis\n\n\n14\nThinking Fast and Slow\n\n\n15\nWhat are some low-information priors that you find practically useful for thinking about the world?\n\n\n\n\n\nwhere Categorizing Variants of Goodhart’s Law has value fixed to \\(1\\). I will use the number scheme above throughout the post."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "href": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "title": "Estimating value from pairwise comparisons",
    "section": "A model with multiplicative error terms",
    "text": "A model with multiplicative error terms\n\nMotivation and setup\nLet \\(\\eta_i\\) be the true subjective value of item \\(i\\), counting starting from \\(1\\). We will let \\(\\eta_3=1\\) in our setup, as Manheim and Garabrant’s Categorizing Variants of Goodhart’s Law was fixed to \\(1\\) in Nuño’s study, but we could have fixed any other item if we wanted to. Ideally, we would have observed the “distances” \\(d_{ij}=\\eta_i/\\eta_j\\) directly, but we don’t. Instead, we observe the distances with noise, \\(\\hat{d}_{ij}\\). We’ll assume a multiplicative model for these noise measurements:\n\\[\n\\hat{d}_{ij} = \\frac{\\eta_i}{\\eta_j}\\cdot e^{\\sigma \\epsilon_{ij}},\n\\] where \\(e^{\\sigma \\epsilon_{ij}}\\) is a positive noise term with standard deviation \\(\\sigma\\) on the log-scale. Now define \\(Y_{ij} = \\log \\hat{d}_{ij}\\) and \\(\\beta_i = \\log \\eta_i\\). Observe that \\(\\beta_3 = 0\\) by assumption. Now take logarimths on both sides of the equation above to get\n\\[\nY_{ij} = \\beta_i - \\beta_j + \\sigma\\epsilon_{ij},\n\\]\nwhich is a linear regression model. It looks like a two-way analysis of variance, but isn’t quite that, as we are only dealing with one factor here (the evaluated research) which appears twice in each equation. That said, the only difficulty in estimating this model is to make a model matrix for the regression coefficients.\n\n\nIncidence matrices\nThe questions Gavin answered in the table above can be understood as a directed graph; I’ll call it the question graph. Gavin’s question graph can be seen below.\n\n\n\n\n\nDirected graphs can be defined by their incidence matrices. If \\(G\\) is a directed graph with \\(k\\) nodes and \\(n\\) edges its incidence matrix \\(B\\) is the \\(n\\times k\\) matrix with elements \\[B_{ij}=\\begin{cases}\n-1 & \\text{if edge }e_{j}\\text{ leaves vertex }v_{i},\\\\\n1 & \\text{if edge }e_{j}\\text{ enters vertex }v_{i},\\\\\n0 & \\text{otherwise.}\n\\end{cases}\\]\nFor instance, Gavin’s incidence matrix is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n-1\n0\n0\n0\n\n\n0\n0\n0\n-1\n0\n1\n1\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n-1\n0\n0\n0\n0\n1\n1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n-1\n-1\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n0\n-1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-1\n0\n0\n0\n0\n0\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n-1\n0\n0\n0\n-1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nNow we can verify that \\[Y = B^T\\beta + \\sigma \\epsilon.\\] But there is one more thing to handle: We need to remove the fixed \\(\\beta\\), in our case \\(\\beta_3\\), to estimate the model. Define \\(B_\\star\\) and \\(\\beta_\\star\\) as the incidence matrix and coefficient vector with the fixed item removed. Then \\(Y = B_\\star^T\\beta_\\star + \\sigma \\epsilon\\) is ready to be estimated using linear regression.\n\n\nExample\nWe fit a linear regression to Gavin’s data. Here are the resulting estimates on the log-scale, rounded to the nearest whole number.\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n-12\n-3\n0\n-10\n-8\n-5\n-7\n-4\n7\n3\n8\n-3\n1\n-4\n-6\n\n\n\n\n\nWe can also make confidence intervals for the questions using the confint function. Here’s a plot showing confidence intervals for all the \\(\\beta\\)s along with their estimates \\(\\hat{\\beta}\\).\n\n\n\n\n\nThe \\(95\\%\\) confidence intervals are approximately equally wide on the log-scale, with the exception of question 3, which is fixed to \\(1\\). Let’s take a look at question 11, that of Shannon’s A Mathematical Theory of Communication. The confidence interval is (135, 15022) – that’s wide!\n\n\nAll the raters\nThe raters have IDs given in this table.\n\n\nlinch  finn gavin jamie misha ozzie \n    1     2     3     4     5     6 \n\n\nWe fit the model for all the raters and plot the resulting estimates.\n\n\n\n\n\nIt seems that the raters agree quite a bit."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "href": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring agreement",
    "text": "Measuring agreement\nOne of the easiest and most popular ways to measure agreement among two raters is Lin’s concordance coefficient (aka quadratically weighted Cohen’s kappa). It has an unpublished multirater generalization \\[\\frac{1^{T}\\Sigma1-\\text{tr}\\Sigma}{(R-1)\\text{tr}\\Sigma+R^{2}\\left(\\overline{\\mu^{2}}-\\overline{\\mu}^{2}\\right)}\\] Where \\(\\Sigma\\) is the covariance matrix of the estimated log rating, \\(\\mu_i\\) is the mean log rating by the \\(i\\)th rater, and \\(R\\) is the number of raters. I can explain the reasoning behind this measure in more detail if you want, but it’s the essentially unique extension of Lin’s concordance coefficient to multiple raters, as several generalizations yield the same formula. It’s bounded above by \\(1\\), which signifies perfect agreement. It’s defined in a way that’s very similar to the \\(R^2\\), so it’s OK to interpret the numbers as you would have interpreted an \\(R^2\\).\n\nconcordance = function(x) {\n  n = nrow(x)\n  r = ncol(x)\n  sigma = cov(x) * (n - 1) / n\n  mu = colMeans(x)\n  trace = sum(diag(sigma))\n  top = sum(sigma) - trace\n  bottom = (r - 1) * trace + r ^ 2 * (mean(mu^2) - mean(mu)^2)\n  top / bottom\n}\nconcordance(log(parameters))\n\n[1] 0.698547\n\n\nI’m impressed by the level of agreement among the raters.\nWe can also construct a matrix of pairwise agreements.\n\nconcordances <- outer(seq(6), seq(6), Vectorize(\\(i,j) concordance(\n  cbind(log(parameters)[, i], log(parameters)[, j]))))\ncolnames(concordances) <- names(x)\nrownames(concordances) <- names(x)\nconcordances\n\n          linch      finn     gavin     jamie     misha     ozzie\nlinch 1.0000000 0.5442018 0.7562977 0.7705449 0.7779404 0.7016672\nfinn  0.5442018 1.0000000 0.4031429 0.5385316 0.4685602 0.5985986\ngavin 0.7562977 0.4031429 1.0000000 0.7237382 0.9106108 0.5996458\njamie 0.7705449 0.5385316 0.7237382 1.0000000 0.8253265 0.8926464\nmisha 0.7779404 0.4685602 0.9106108 0.8253265 1.0000000 0.7804517\nozzie 0.7016672 0.5985986 0.5996458 0.8926464 0.7804517 1.0000000\n\n\nNow we notice, e.g., that (i) Gavin agrees with Misha, (ii) Finn doesn’t agree much with anyone, (iii) Ozzie agrees with Jamie."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "href": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "title": "Estimating value from pairwise comparisons",
    "section": "Identification of the parameters",
    "text": "Identification of the parameters\nThe parameters \\(\\beta_\\star\\) are identified if and only if the question graph is connected. This has the practical consequence that the estimation is guaranteed to work whenever you design the question graph well enough. For instance, you do not need to think about avoiding cycles, having only one question per pair, etc.\nNow, it should be intuitively clear that \\(\\beta_\\star\\) cannot be identified when the graph fails to be connected, as there is no point(s) anchoring the scale of every \\(\\beta\\). Think about it this way. Suppose \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) form a connected component disconnected from \\(\\beta_{3}\\). If \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) satisfy \\(Y=B_{[1,2,4]}^{T}\\beta_{[1,2,4]}+\\sigma\\epsilon\\), where \\([1,2,3]\\) denotes the appropriate indexing, then surely \\(\\gamma_{i}=\\beta_{i}+c\\) does so too for any \\(c\\), as every row of \\(B_\\star^{T}\\beta\\) is a difference \\(\\beta_{i}-\\beta_{j}\\), hence \\(\\gamma_{i}-\\gamma_{j}=\\beta_{i}+c-(\\beta_{j}+c)=\\beta_{i}-\\beta_{j}\\). The other way around is slightly trickier. It’s a theorem of algebraic graph theory that the rank of \\(B\\) equals \\(k-c\\), where \\(c\\) is the number of connected components. Suppose the graph is connected, so that the rank of \\(B\\) is \\(k-1\\). Since \\(B\\) does not have full rank (i.e., \\(k\\)), every row can be written as a linear combination of two other rows. In particular, the row associated with the fixed element can be removed without affecting the rank, hence the rank of \\(B_\\star\\) is \\(k-1\\) too. But there are \\(k-1\\) rows in \\(B_\\star\\), hence \\(B_\\star\\) has full rank. It follows that the parameters are identified."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "href": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring inconsistency",
    "text": "Measuring inconsistency\nRecall the multiplicative equation for the reported distance: \\[D_{ij} = \\frac{X_i}{X_j}\\cdot e^{\\sigma \\epsilon_{ij}}\\] It’s clear that the distance will be noise-free if and only if \\(\\sigma = 0\\). Moreover, the distances will behave more and more erratically the larger \\(\\sigma\\) gets. If the distances tend to have erratically, the valuations will be inconsistent. Thus it’s natural to consider inconsistency estimators that are strictly increasing functions of \\(\\sigma\\). We’ll just use \\(\\sigma\\) for simplicity’s sake.\nThe consistencies of our 6 player are\n\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n0.87\n1.04\n2.22\n0.86\n0.87\n0.93\n\n\n\n\n\nAll of these are roughly the same, except Gavin’s. That might be surprising since Nuño claimed Gavin is the most consistent of the raters. His inconsistency score is probably unfavourable since he has some serious outliers in his ratings, not because he’s inconsistent across the board. Ratings \\(33\\) and \\(36\\) appear to be especially inconsistent.\n\n\n\n\n\nCompared it to the same plot for Jaime Sevilla.\n\n\n\n\n\nLet’s see what happens if we remove the observations \\(31, 33, 34, 36\\) from Gavin’s data then.\n\n\n\n\n\nThe residual plot looks better now, and the inconsistency score becomes \\(\\sigma \\approx 0.87\\), in line with the other participants.\nMy take-away is that it would be beneficial to use robust linear regressions when estimating \\(\\beta\\). I’m not prioritizing studying this right now, but if someone were to invest serious amount of time in studying and applying statistical methods for this problem, I would strongly suggest taking a look at e.g. rlm.\n\nYou shouldn’t strive for consistency\nStriving for consistency requires you to follow a method. For instance, you can write down or try hard to remember what you have answered on previous questions, then use the right formula to deduce a consistent answer. I would advice against doing this though. When you compare two items against each other, just follow the priming of the shown items and let the statistical method do its work! If you’re trying hard to be consistent you’ll probably introduce some sort of bias, as you’ll essentially make the ratings dependent on their ordering. Also see the crowd within. The value-elicitation framework is similar to psychometrics, where you want every measurement to be as independent of every other measurement as possible when you condition on the latent variables.\nI also see little reason to use algorithms that prohibits cyclical comparisons, as there is no statistical reason to avoid them. (Only a psychological one, if you feel like you have to be consistent.) It’s also fine the ask the same question more than once – at least if you add some addition correlation term into the model. And have some time distance between the questions."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#aggregation",
    "href": "posts/value-estimation/value-estimation.html#aggregation",
    "title": "Estimating value from pairwise comparisons",
    "section": "Aggregation",
    "text": "Aggregation\nSuppose the vectors \\(\\beta_{j}\\sim N(\\beta,\\tau I)\\) for some variance parameter \\(\\tau\\). We wish to estimate the global mean \\(\\beta\\). This can be done using mixed effects models. \\[\\begin{eqnarray*}\nY_{j} & = & D_{j}^{T}\\beta_{j}+\\sigma\\epsilon,\\\\\n\\beta_{j} & = & \\beta+\\tau\\delta.\n\\end{eqnarray*}\\]\nConceptually, this model implies that there is a true underlying \\(\\beta\\) for each question, but the raters only have incomplete access to it when they form their subjective valuation. So we have two sources of noise: First, the raters have a latent, noisy and subjective estimate of \\(\\beta\\), which we call \\(\\beta_j\\). Second, we only observe noisy measurements of \\(\\beta_j\\)s through our pairwise comparisons model.\nUsing lme4 I fit a mixed effects model to the data without an intercept. Check out the source if you want to know exactly what I’ve done.\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00216372 (tol = 0.002, component 1)\n\n\n\n\n\nThe confidence intervals in the plot are reasonably sized, but remember the \\(y\\)-axis is on the log-scale. Let’s take a look at the confidence interval for A Mathematical Theory of Communication again:\n\n\n 2.5 % 97.5 % \n   198  57567 \n\n\nThe uncertainty of the aggregate value is smaller than that of Gavin’s subjective value. But the uncertainty is still very, very large. I think the level of uncertainty is wrong though. Fixing it would probably require a modification of the model to allow for items of different difficulty, or maybe a non-multiplicative error structure."
  }
]