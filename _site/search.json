[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Jonas Moss, a statistics post doc at BI Norwegian Business School. I’m interested in programming, the replication crisis in psychology, theoretical statistics, forecasting, effective altruism, and rationalism (as in Lesswrong)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonas Moss' blog",
    "section": "",
    "text": "effective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nteaching\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nJonas Moss\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html",
    "href": "posts/alpha-and-omega/alpha-and-omega.html",
    "title": "No one would have invented coefficient alpha today",
    "section": "",
    "text": "Coefficient alpha is the most famous coefficient in psychometrics – Cronbach’s paper Coefficient alpha and the internal structure of tests has been cited around \\(60,00\\) times after all. It’s supposed to measure reliability. What does that mean? Intuitively, a psychometric scale is supposed to measure some kind of psychological construct, such as intelligence, in a reliable way. You don’t want it to be noisy. You don’t want two intelligence tests administered at slightly different times to give widely different results. You also want the test to actually measure intelligence, and not something else, such emotionality. But that’s validity, not reliability.\nIn classical test theory they think about psychometric tests in the context of a true score \\(T\\) and observed score \\(X\\). Then they write \\(X = T +\\epsilon\\) for some error term \\(\\epsilon\\). (This is always possible). For instance, if \\(X\\) is the observed score on an IQ test, \\(T\\) is the true, underlying intelligence.\nNow we’re ready to define classical reliability! The reliability is defined as \\(R=\\text{Cor}(X, T)^2\\), the squared correlation between the true score and the observed score. That’s a pretty reasonable definition! But the problem is that you don’t know the true score, so you can’t estimate this correlation directly.\nAnd that’s where coefficient alpha comes in. Suppose that \\(X\\) is a sum score, i.e., on the form \\(X = X_1 + X_2 + \\ldots + X_k\\). Then assume that \\[X_i = \\mu_i + \\lambda_i T + \\sigma_i \\epsilon_i \\tag{1}\\] for some error terms with variance equal to \\(1\\). In addition, assume that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) when \\(i = j\\), i.e., the error terms are uncorrelated. Then \\(X\\) follows the congeneric measurement model. Now define coefficient alpha as \\[\\alpha=\\frac{k}{k-1}\\left(1-\\frac{\\text{tr}\\Sigma}{{1}^{T}\\Sigma{1}}\\right),\\] where \\(\\Sigma\\) is the covariane matrix of the \\(X_i\\)s. This alpha has two important properties:\nThe condition of \\(\\tau\\)-equivalence means that all the factor loadings \\(\\lambda_i\\) are equal, i.e., the congeneric model eq. 1 is reduced to \\[X_i = \\mu_i + \\lambda T + \\sigma_i \\epsilon_i. \\tag{2}\\] There is widespread agreement that \\(\\tau\\)-equivalence never holds.\nAlpha is also the mean of all possible split-half reliabilities, but no one seem to care much about that today. It might have been important earlier on though. For more details see (Cho 2021)."
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "href": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "title": "No one would have invented coefficient alpha today",
    "section": "Why no one would have invented alpha today",
    "text": "Why no one would have invented alpha today\nTake a look at the congeneric model (eq. 1) again. This is a linear one-factor model with no correlation among the errors. In the early 20th century, there was no feasible way to estimate such models, at least not for ordinary psychometric researchers. But that’s not the case anymore. Anyone able to install R and run the easiest lavaan script can estimate such a model. For example, we can estimate the parameters for the agreeableness part of the psychTools::bfi data.\n\nlibrary(\"lavaan\")\nmodel <- \" f =~ A1 + A2 + A3 + A4 + A5 \"\nbfi <- psychTools::bfi[, 1:5]\nbfi[, 1] <- -bfi[, 1] # Reverse-coded question.\nobj <- lavaan::cfa(model, data = bfi, std.lv = TRUE)\nknitr::kable(round(coef(obj), 3))\n\n\n\n\n\nx\n\n\n\n\nf=~A1\n0.528\n\n\nf=~A2\n0.774\n\n\nf=~A3\n0.994\n\n\nf=~A4\n0.717\n\n\nf=~A5\n0.791\n\n\nA1~~A1\n1.693\n\n\nA2~~A2\n0.784\n\n\nA3~~A3\n0.714\n\n\nA4~~A4\n1.694\n\n\nA5~~A5\n0.965\n\n\n\n\n\nThe std.lv = TRUE argument forces the latent variable \\(T\\) to have variance equal to \\(1\\). This option makes it easier to interpret the remaining parameters. Using the estimated parameters of the lavaan object we can estimate the reliabiltiy \\(R\\) with little problems. Straight-forward calculations show that, when we assume the congeneric measurement model, \\[R = \\text{Cor}^{2}(X_1 + X_2+\\cdots+X_k,T)=\\frac{k\\overline{\\lambda}^{2}}{k\\overline{\\lambda}^{2}+\\overline{\\sigma^{2}}}, \\tag{3}\\] where \\(\\overline{x}\\) denotes the mean of the vector \\(x\\).\nUsing this formulation of the reliability in terms of \\(\\lambda\\) and \\(\\sigma\\), the natural estimator if the reliability is the plug-in estimator \\[\\hat{R} = \\frac{k\\overline{\\hat{\\lambda}}^{2}}{k\\overline{\\hat{\\lambda}}^{2}+\\overline{\\hat{\\sigma}^{2}}},\\] where \\(\\hat{\\lambda}\\) and \\(\\hat{\\sigma}\\) are estimators of your choice. If you’re using lavaan, you would calculate it using something like the following function.\n\n#' Estimate the reliability from a `lavaan` object.\n#' \n#' Estimate the reliability assuming a congeneric measurement model using the\n#'   plug-in estimator.\n#' @param obj A `lavaan` object.\n#' @return The estimated reliability coefficient.\nreliability <- function(obj) {\n  params <- lavaan::lavInspect(obj, what = \"coef\")\n  lambda <- params$lambda\n  sigma2 <- diag(params$theta)\n  k <- length(lambda)\n  k * mean(lambda) ^ 2 / (k * mean(lambda) ^ 2 + mean(sigma2))\n}\n\nFor the agreeableness data of psychTools::bfi, our reliability becomes\n\nreliability(obj)\n\n[1] 0.712129\n\n\nIt’s not hard to do inference for the reliability coefficient either – it is merely an application of the delta method. Moreover, by the invariance principle of maximum likelihood estimation, it is the maximum likelihood estimator of the reliability provided \\(\\lambda\\) and \\(\\sigma\\) are estimated using maximum likelihood. In this sense the estimator is natural.\nTo sum up:\n\nThere exists a natural estimator of the reliability under the congeneric measurement model.\nIt is easy to estimate and it’s efficient under normality.\nIts asymptotic theory is well-understood. That’s just a by-product of the immense amount of research on asymptotic theory for general structural equation models.\n\nI doubt anyone would have wanted to investigate alternatives to this 100% reasonable, simple, and efficient estimator. For why would they? It’s like finding an alternative estimator of the \\(R^2\\) that requires a little less computation power but is only consistent for the \\(R^2\\) under extraordinarily unlikely assumptions.\nMaybe someone would have uncovered coefficient alpha today, but its discovery would be treated as a novelty, not something of widespread importance, worth \\(60,000\\) citations."
  },
  {
    "objectID": "posts/monolithic-education/monolithic-education.html",
    "href": "posts/monolithic-education/monolithic-education.html",
    "title": "Monolithic education",
    "section": "",
    "text": "Curriculum. What should you teach the students? It’s not possible for a non-expert to know what’s important for him to learn; so an expert or group of expert is necessary here. But that person is not typically the same as the one who’s best suited to teach.\nTeaching system. How should the students learn whatever it is you want to teach? With lectures, guided exercise solution, class discussion, or something else? Maybe you should go for something along the lines Bikini Calculus.\nTeaching material. Someone has to make the teaching material. And what should that look like? No matter what format you choose, this is a whole lot of work.\nEvaluation. How should it be tested? All year round or just at a certain time and place? Should the students pay a fee for the exam? Can SRS be used somehow? Multiple choice?\n\nThese four pillars are not separated enough in practice. Even though they are clearly distinct and require different talents and different interests.\nIn high schools these components are, at least partly, at least in Norway, handled by different people. The curriculum is designed by the government. The teaching material is usually books, perhaps 2-3 to choose from for each subject. Evaluation is handled by the government. So you can, as a teacher, spend all your time teaching.\nIn universities, on the other hand, one professor is often tasked with doing everything. Design the curriculum. Figure out the teaching system. Perhaps even develop his own teaching materials – at the very least his own slides. And, of course, he must make his own exams.\n\nOn curriculum development\nI’m placing curriculum at the top of the list as it is both the most important and the most neglected. How often do you see students call for better curricula? If you ignore calls for anti-colonialization, probably never. But this is where you find the greatest potential for improvement. You’ll often find that introductory math and engineering courses, for instance, follow a pattern that was well-established even back in the 80s. They involve giving lip service to intuitions and proofs, focussing the entirety of the curriculum on gaining enough practice with a couple of calculation techniques required to pass the exam. And why is that? Probably because someone has to make the exam, and the more “examy” stuff the students know, the easier it is to make one.\nLet’s take a classical calculus course for example. A calculus course will often teach you how to solve integrals using partial fraction expansions. That is probably because it is a simple technique that does help you solve, by hand, a larger class of integrals than what you would have been able to had you not knowm it, thus expanding the pool of possible exercises to give the students at the exam. But does it actually help the student?\nPartial fraction expansion is not a “deep” part of integration, such as substitution and integration by parts. You need to learn these as they are essential both for conceptual understanding and most proofs. But partical fraction expansion is not. The student can use WolframAlpha when he’s calculating integrals on his own.\nDesigning a curriculum should involve carefully picking out the parts of a subject that are most important to understand and master in order to achieve a set of goals. The goals of calculus are, roughly, (i) to build mathematical maturity, (ii) to be comfortable with what limits, derivatives and integrals are, e.g., develop an inuitive understanding of why they are linear, (iii) understand their basic applications in optimization problems and physics, and then, (iv) to gain an intuition of what integrals are easy to calculate and when that matters.\nI’ve only spend 5 minutes thinking about these goals, and I’m open to counters. But calculus should not be about doing the maximal amount of integral calculations, competing in how fast you can use the chain rule, and so on."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html",
    "href": "posts/value-estimation/value-estimation.html",
    "title": "Estimating value from pairwise comparisons",
    "section": "",
    "text": "How can you estimate the value of research output? You could use pairwise comparisons, e.g., to ask specialists how much more valuable Darwin’s The Original of Species is than Dembski’s Intelligent Design. Then you can use these relative valuations to estimate absolute valuations."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#summary",
    "href": "posts/value-estimation/value-estimation.html#summary",
    "title": "Estimating value from pairwise comparisons",
    "section": "Summary",
    "text": "Summary\n\nEstimating values is hard. One way to elicit value estimates is ask researchers to compare two different items \\(A\\) and \\(B\\), asking how much better \\(A\\) is than \\(B\\). This makes the problem more concrete than just asking “what is the value of \\(A\\)?”. The Quantified Uncertainty Institute has made an app for doing this kind of thing, described here.\nNuño Sempere had a post about eliciting comparisons of research value from \\(6\\) effective altruism researchers. This is a more recent post about AI risk, but it uses distributions instead of point estimates.\nThis post proposes some technical solutions to problems introduced to me in Nuño’s post. In particular, it includes principled ways to\n\nestimate subjective values,\nmeasure consistency in pairwise value judgments,\nmeasure agreement between the raters,\naggregate subjective values.\nI also propose to use weighted least squares when the raters supply distributions instead of numbers. It is not clear to me it is worth it to ask for distributions in these kinds of questions though, as your uncertainty level can be modelled implicitly by comparing different pairwise comparisons.\n\nI use these methods on the data from the 6 researchers post.\n\nI’m assuming you have read the 6 researchers post recently. I think this post will be hard to read if you haven’t.\nNote: This document is a compiled Quarto file with source functions outside of the main document. The functions can be found in the source folder for this post. Also, thanks to Nuño Sempere for his comments on a draft of the post!\n\nWhat’s this about\nTable 1 contains the first \\(6\\) out of \\(36\\) responses from Gavin Leech. As you can see, he values Superintelligence \\(100\\) more than the Global Priorities Institute’s Research Agenda.\n\n\nA list of questions in the data set.\nknitr::kable(head(gavin[, 1:3]))\n\n\n\n\nTable 1: First 6 questions Gavin answered.\n\n\n\n\n\n\n\nsource\ntarget\ndistance\n\n\n\n\nThinking Fast and Slow\nThe Global Priorities Institute’s Research Agenda\n100\n\n\nThe Global Priorities Institute’s Research Agenda\nThe Mathematical Theory of Communication\n1000\n\n\nSuperintelligence\nThe Mathematical Theory of Communication\n10\n\n\nCategorizing Variants of Goodhart’s Law\nThe Vulnerable World Hypothesis\n10\n\n\nShallow evaluations of longtermist organizations\nThe motivated reasoning critique of effective altruism\n10\n\n\nShallow evaluations of longtermist organizations\nCategorizing Variants of Goodhart’s Law\n100\n\n\n\n\n\n\nMy first goal is to take relative value judgments such these and use them to estimate the true subjective values. In this case, I want to estimate the value that Gavin Leech places on every article in the data set, as contained in Table 2.\n\n\nA list of questions in the data set.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nknitr::kable(cbind(1:15, levels))\n\n\n\n\nTable 2: All \\(15\\) articles valuated in the data set. Question \\(3\\) is fixed to \\(1\\).\n\n\n\n\n\n\n\nlevels\n\n\n\n\n1\nA comment on setting up a charity\n\n\n2\nA Model of Patient Spending and Movement Building\n\n\n3\nCategorizing Variants of Goodhart’s Law\n\n\n4\nCenter for Election Science EA Wiki stub\n\n\n5\nDatabase of orgs relevant to longtermist/x-risk work\n\n\n6\nExtinguishing or preventing coal seam fires is a potential cause area\n\n\n7\nReversals in Psychology\n\n\n8\nShallow evaluations of longtermist organizations\n\n\n9\nSuperintelligence\n\n\n10\nThe Global Priorities Institute’s Research Agenda\n\n\n11\nThe Mathematical Theory of Communication\n\n\n12\nThe motivated reasoning critique of effective altruism\n\n\n13\nThe Vulnerable World Hypothesis\n\n\n14\nThinking Fast and Slow\n\n\n15\nWhat are some low-information priors that you find practically useful for thinking about the world?\n\n\n\n\n\n\nThe article Categorizing Variants of Goodhart’s Law has value fixed to \\(1\\). I will use the numbering above throughout this post."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "href": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "title": "Estimating value from pairwise comparisons",
    "section": "A model with multiplicative error terms",
    "text": "A model with multiplicative error terms\n\nMotivation and setup\nLet \\(\\eta_i\\) be the true subjective value of item \\(i\\), counting starting from \\(1\\). We will let \\(\\eta_3=1\\) in our setup, as Manheim and Garabrant’s Categorizing Variants of Goodhart’s Law was fixed to \\(1\\) in Nuño’s study, but we could have fixed any other item if we wanted to. Ideally, we would have observed the “distances” \\(d_{ij}=\\eta_i/\\eta_j\\) directly, but we don’t. Instead, we observe the distances with noise, \\(\\hat{d}_{ij}\\). We’ll assume a multiplicative model for these noise measurements:\n\\[\n\\hat{d}_{ij} = \\frac{\\eta_i}{\\eta_j}\\cdot e^{\\sigma \\epsilon_{ij}},\n\\] where \\(e^{\\sigma \\epsilon_{ij}}\\) is a positive noise term with standard deviation \\(\\sigma\\) on the log-scale. Now define \\(Y_{ij} = \\log \\hat{d}_{ij}\\) and \\(\\beta_i = \\log \\eta_i\\). Observe that \\(\\beta_3 = 0\\) by assumption. Now take logarimths on both sides of the equation above to get\n\\[\nY_{ij} = \\beta_i - \\beta_j + \\sigma\\epsilon_{ij},\n\\]\nwhich is a linear regression model. It looks like a two-way analysis of variance, but isn’t quite that, as we are only dealing with one factor here (the evaluated research) which appears twice in each equation. That said, the only difficulty in estimating this model is to make a model matrix for the regression coefficients. Observe that the residual standard deviation is fixed across items. We’ll take a look at how to reasonably relax this later on.\n\n\nIncidence matrices\nThe questions Gavin answered in the table above can be understood as a directed graph; I’ll call it the question graph. Figure 1 below contains Gavin’s question graph.\n\n\nPlotting question graph for Gavin.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nsource <- as.numeric(factor(gavin$source, levels = levels))\ntarget <- as.numeric(factor(gavin$target, levels = levels))\ngraph <- igraph::graph_from_edgelist(cbind(source, target))\nplot(graph)\n\n\n\n\n\nFigure 1: Question graph for Gavin.\n\n\n\n\nDirected graphs can be defined by their incidence matrices. If \\(G\\) is a directed graph with \\(k\\) nodes and \\(n\\) edges its incidence matrix \\(B\\) is the \\(n\\times k\\) matrix with elements \\[B_{ij}=\\begin{cases}\n-1 & \\text{if edge }e_{j}\\text{ leaves vertex }v_{i},\\\\\n1 & \\text{if edge }e_{j}\\text{ enters vertex }v_{i},\\\\\n0 & \\text{otherwise.}\n\\end{cases}\\]\nTable 3 contains Gavin’s incidence matrix.\n\n\nCalculation of incidence matrix for Gavin.\nn <- nrow(gavin)\nk <- 15\nb <- matrix(data = 0, nrow = n, ncol = k)\n\nfor (i in seq(n)) {\n  b[i, source[i]] <- -1\n  b[i, target[i]] <- 1\n}\n\nknitr::kable(t(b))\n\n\n\n\nTable 3: Incidence matrix for Gavin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n-1\n0\n0\n0\n\n\n0\n0\n0\n-1\n0\n1\n1\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n-1\n0\n0\n0\n0\n1\n1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n-1\n-1\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n0\n-1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-1\n0\n0\n0\n0\n0\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n-1\n0\n0\n0\n-1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nNow we can verify that \\[Y = B^T\\beta + \\sigma \\epsilon.\\] But there is one more thing to handle: We need to remove the fixed \\(\\beta\\), in our case \\(\\beta_3\\), to estimate the model. Define \\(B_\\star\\) and \\(\\beta_\\star\\) as the incidence matrix and coefficient vector with the fixed item removed. Then \\(Y = B_\\star^T\\beta_\\star + \\sigma \\epsilon\\) is ready to be estimated using linear regression.\n\n\nExample\nWe fit a linear regression to Gavin’s data. Table 4 contains the resulting estimates on the log-scale, rounded to the nearest whole number.\n\n\nParameter estimates for Gavin.\nmod <- pairwise_model(gavin, fixed = 3, keep_names = FALSE)\nvals <- round(c(coef(mod)[1:2], q3 = 0, coef(mod)[3:14]))\nknitr::kable(t(vals))\n\n\n\n\nTable 4: Parameter estimates for Gavin.\n\n\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nq9\nq10\nq11\nq12\nq13\nq14\nq15\n\n\n\n\n-12\n-3\n0\n-10\n-8\n-5\n-7\n-4\n7\n3\n8\n-3\n1\n-4\n-6\n\n\n\n\n\n\nWe can also make confidence intervals for the questions using the confint function. Figure 2 plots confidence intervals for all the \\(\\beta\\)s along with their estimates \\(\\hat{\\beta}\\).\n\n\nPlot of parametes and error bars.\nexped = exp(confint(mod))\nconfints = rbind(exped[1:2, ], c(1, 1), exped[3:14, ])\nrownames(confints) <- 1:15\nparams <- setNames(c(coef(mod)[1:2], 1, coef(mod)[3:14]), 1:15)\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 2: Plot of parametes and error bars for Gavin.\n\n\n\n\nThe \\(95\\%\\) confidence intervals are approximately equally wide on the log-scale, with the exception of question 3, which is fixed to \\(1\\). Let’s take a look at question 11, that of Shannon’s A Mathematical Theory of Communication. The confidence interval is (135, 15022) – that’s wide!\n\n\nAll the raters\nThe raters have IDs given in this table.\n\nx <- setNames(1:6, names(data_list))\nknitr::kable(t(x))\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nWe fit the model for all the raters and plot the resulting estimates in Figure 3. Notice the log-scale.\n\n\nFunction for plotting results for all raters.\nparameters = sapply(\n  data_list,\n  \\(data) {\n    coefs <- exp(coef(pairwise_model(data)))\n    c(coefs[1:2], 1, coefs[3:14])\n  })\nmatplot(parameters, log = \"y\", type = \"b\", ylab = \"Values\")\n\n\n\n\n\nFigure 3: Plot of parametes estimates for all raters.\n\n\n\n\nIt seems that the raters agree quite a bit."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "href": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring agreement",
    "text": "Measuring agreement\nOne of the easiest and most popular ways to measure agreement among two raters is Lin’s concordance coefficient (aka quadratically weighted Cohen’s kappa). It has an unpublished multirater generalization \\[\\frac{1^{T}\\Sigma1-\\text{tr}\\Sigma}{(R-1)\\text{tr}\\Sigma+R^{2}\\left(\\overline{\\mu^{2}}-\\overline{\\mu}^{2}\\right)}\\] Where \\(\\Sigma\\) is the covariance matrix of the estimated log rating, \\(\\mu_i\\) is the mean log rating by the \\(i\\)th rater, and \\(R\\) is the number of raters. I can explain the reasoning behind this measure in more detail if you want, but it’s the essentially unique extension of Lin’s concordance coefficient to multiple raters, as several generalizations yield the same formula. It’s bounded above by \\(1\\), which signifies perfect agreement. It’s defined in a way that’s very similar to the \\(R^2\\), so it’s OK to interpret the numbers as you would have interpreted an \\(R^2\\).\n\n\nCalculate concordance of the parameters.\nconcordance = function(x) {\n  n = nrow(x)\n  r = ncol(x)\n  sigma = cov(x) * (n - 1) / n\n  mu = colMeans(x)\n  trace = sum(diag(sigma))\n  top = sum(sigma) - trace\n  bottom = (r - 1) * trace + r ^ 2 * (mean(mu^2) - mean(mu)^2)\n  top / bottom\n}\nconcordance(log(parameters))\n\n\n[1] 0.698547\n\n\nI’m impressed by the level of agreement among the raters.\nWe can also construct a matrix of pairwise agreements.\n\n\nDefines the concordanc matrix using Lin’s coefficient.\nconcordances <- outer(seq(6), seq(6), Vectorize(\\(i,j) concordance(\n  cbind(log(parameters)[, i], log(parameters)[, j]))))\ncolnames(concordances) <- names(x)\nrownames(concordances) <- names(x)\nconcordances\n\n\n          linch      finn     gavin     jamie     misha     ozzie\nlinch 1.0000000 0.5442018 0.7562977 0.7705449 0.7779404 0.7016672\nfinn  0.5442018 1.0000000 0.4031429 0.5385316 0.4685602 0.5985986\ngavin 0.7562977 0.4031429 1.0000000 0.7237382 0.9106108 0.5996458\njamie 0.7705449 0.5385316 0.7237382 1.0000000 0.8253265 0.8926464\nmisha 0.7779404 0.4685602 0.9106108 0.8253265 1.0000000 0.7804517\nozzie 0.7016672 0.5985986 0.5996458 0.8926464 0.7804517 1.0000000\n\n\nNow we notice, e.g., that (i) Gavin agrees with Misha, (ii) Finn doesn’t agree much with anyone, (iii) Ozzie agrees with Jamie."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "href": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "title": "Estimating value from pairwise comparisons",
    "section": "Identification of the parameters",
    "text": "Identification of the parameters\nThe parameters \\(\\beta_\\star\\) are identified if and only if the question graph is connected. This has the practical consequence that the estimation is guaranteed to work whenever you design the question graph well enough. For instance, you do not need to think about avoiding cycles, having only one question per pair, etc.\nNow, it should be intuitively clear that \\(\\beta_\\star\\) cannot be identified when the graph fails to be connected, as there is no point(s) anchoring the scale of every \\(\\beta\\). Think about it this way. Suppose \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) form a connected component disconnected from \\(\\beta_{3}\\). If \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) satisfy \\(Y=B_{[1,2,4]}^{T}\\beta_{[1,2,4]}+\\sigma\\epsilon\\), where \\([1,2,3]\\) denotes the appropriate indexing, then surely \\(\\gamma_{i}=\\beta_{i}+c\\) does so too for any \\(c\\), as every row of \\(B_\\star^{T}\\beta\\) is a difference \\(\\beta_{i}-\\beta_{j}\\), hence \\(\\gamma_{i}-\\gamma_{j}=\\beta_{i}+c-(\\beta_{j}+c)=\\beta_{i}-\\beta_{j}\\). The other way around is slightly trickier. It’s a theorem of algebraic graph theory that the rank of \\(B\\) equals \\(k-c\\), where \\(c\\) is the number of connected components. Suppose the graph is connected, so that the rank of \\(B\\) is \\(k-1\\). Since \\(B\\) does not have full rank (i.e., \\(k\\)), every row can be written as a linear combination of two other rows. In particular, the row associated with the fixed element can be removed without affecting the rank, hence the rank of \\(B_\\star\\) is \\(k-1\\) too. But there are \\(k-1\\) rows in \\(B_\\star\\), hence \\(B_\\star\\) has full rank. It follows that the parameters are identified."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "href": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring inconsistency",
    "text": "Measuring inconsistency\nRecall the multiplicative equation for the reported distance: \\[D_{ij} = \\frac{X_i}{X_j}\\cdot e^{\\sigma \\epsilon_{ij}}\\] It’s clear that the distance will be noise-free if and only if \\(\\sigma = 0\\). Moreover, the distances will behave more and more erratically the larger \\(\\sigma\\) gets. If the distances tend to have erratically, the valuations will be inconsistent. Thus it’s natural to consider inconsistency estimators that are strictly increasing functions of \\(\\sigma\\). We’ll just use \\(\\sigma\\) for simplicity’s sake.\nThe consistencies of our 6 player are\n\n\nDefines the consistencies of all raters.\nconsistencies = lapply(data_list, \\(data) summary(pairwise_model(data))$sigma)\nknitr::kable(tibble::as_tibble(consistencies), digits = 2)\n\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n0.87\n1.04\n2.22\n0.86\n0.87\n0.93\n\n\n\n\n\nAll of these are roughly the same, except Gavin’s. That might be surprising since Nuño claimed Gavin is the most consistent of the raters. His inconsistency score is probably unfavourable since he has some serious outliers in his ratings, not because he’s inconsistent across the board. Ratings \\(33\\) and \\(36\\) appear to be especially inconsistent.\n\nplot(mod, which = 1, main = \"Plot for Gavin\")\n\n\n\n\nCompared it to the same plot for Jaime Sevilla.\n\nplot(pairwise_model(jamie), which = 1, main = \"Plot for Jaime\")\n\n\n\n\nLet’s see what happens if we remove the observations \\(31, 33, 34, 36\\) from Gavin’s data then.\n\ngavin_ <- gavin[setdiff(seq(nrow(gavin)), c(31, 33, 34, 36)), ]\nplot(pairwise_model(gavin_), which = 1, main = \"Plot for Gavin with outliers removed\")\n\n\n\n\nThe residual plot looks better now, and the inconsistency score becomes \\(\\sigma \\approx 0.87\\), in line with the other participants.\nMy take-away is that it would be beneficial to use robust linear regressions when estimating \\(\\beta\\). I’m not prioritizing studying this right now, but if someone were to invest serious amount of time in studying and applying statistical methods for this problem, I would strongly suggest taking a look at e.g. rlm.\n\nYou shouldn’t strive for consistency\nStriving for consistency requires you to follow a method. For instance, you can write down or try hard to remember what you have answered on previous questions, then use the right formula to deduce a consistent answer. I would advice against doing this though. When you compare two items against each other, just follow the priming of the shown items and let the statistical method do its work! If you’re trying hard to be consistent you’ll probably introduce some sort of bias, as you’ll essentially make the ratings dependent on their ordering. Also see the crowd within. The value-elicitation framework is similar to psychometrics, where you want every measurement to be as independent of every other measurement as possible when you condition on the latent variables.\nI also see little reason to use algorithms that prohibits cyclical comparisons, as there is no statistical reason to avoid them. (Only a psychological one, if you feel like you have to be consistent.) It’s also fine the ask the same question more than once – at least if you add some addition correlation term into the model. And have some time distance between the questions."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#aggregation",
    "href": "posts/value-estimation/value-estimation.html#aggregation",
    "title": "Estimating value from pairwise comparisons",
    "section": "Aggregation",
    "text": "Aggregation\nWe estimate \\(\\beta\\) using a mixed effect model. \\[\\begin{eqnarray*}\nY_{j} & = & D_{j}^{T}\\beta_{j}+\\sigma\\epsilon,\\\\\n\\beta_{j} & \\sim & N(\\beta,\\Sigma).\n\\end{eqnarray*}\\]\nConceptually, this model implies that there is a true underlying \\(\\beta\\) for each question, but the raters only have incomplete access to it when they form their subjective valuation. So we have two sources of noise: First, the raters have a latent, noisy and subjective estimate of \\(\\beta\\), which we call \\(\\beta_j\\). Second, we only observe noisy measurements of \\(\\beta_j\\)s through our pairwise comparisons model. The matrix \\(\\Sigma\\) can be constrained to be diagonal, which makes estimation go faster.\nUsing lme4, I made a function pairwise_mixed_model that fits a mixed effects model to the data without an intercept. Check out the source if you want to know exactly what I’ve done.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3)\n\nboundary (singular) fit: see help('isSingular')\n\n\nUsing the mod object, we can plot (Figure 4) confidence intervals and estimates for the aggregate ratings.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[16:29, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 4: Confidence bars mixed effects model with uncorrelated random effects.\n\n\n\n\nThe confidence intervals in the plot are reasonably sized, but remember the \\(y\\)-axis is on the log-scale. Let’s take a look at the confidence interval for A Mathematical Theory of Communication again:\n\n\nConfidence interval for Mathematical Theory of Communication with uncorrelated random effects.\nround(exp(confint(mod, method = \"Wald\")[16:29, ])[10, ])\n\n\n 2.5 % 97.5 % \n    44  10740 \n\n\nThe uncertainty of the aggregate value is smaller than that of Gavin’s subjective value. But the uncertainty is still very, very large. I think the level of uncertainty is wrong though. Fixing it would probably require a modification of the model to allow for items of different difficulty, or maybe a non-multiplicative error structure. But there is also a counterfactual aspect here. It’s hard to say how quickly someone else would’ve invented information theory weren’t it for A Mathematical Theory. Different “concepts” about counterfactuals could potentially lead to different true \\(\\beta\\)s, as some readers consider them and some don’t. (See Linch’s comment)\n\nUsing correlated random effects\nWe can run a model with correlated random effects too.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3, uncorrelated = FALSE)\n\nboundary (singular) fit: see help('isSingular')\n\n\nThe corresponding plot (Figure 5) is similar but not indistinguishable from Figure 4. The confidence intervals appear to smaller, but I don’t know if this is an estimation artifact or not. I also don’t know which model is better. Or if it will ever matter in practice which you use, but the uncorrelated model is faster to fit, as the correlated takes a couple of seconds. And that might matter in a production setting.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[(120 - 13):120, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x =  1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 5: Confidence bars mixed effects model with correlated random effects.\n\n\n\n\nThe resulting confidence interval for A Mathematical Theory of Communication gets smaller now.\n\n\nConfidence interval for Mathematical Theory of Communication with correlated random effects.\nround(exp(confint(mod, method = \"Wald\")[116, ]))\n\n\n 2.5 % 97.5 % \n    68   9602"
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "href": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "title": "Estimating value from pairwise comparisons",
    "section": "Incorporating uncertainty",
    "text": "Incorporating uncertainty\nInstead of rating the ratio \\(\\eta_i/\\eta_j\\) with a number, you might want give a distribution over \\(\\eta_i/\\eta_j\\), indicating your uncertainty, as done in e.g. this post. How could we work with such uncertain measurements? One possibility is to extract a log-mean and log-standard deviation from the distributions and then use the same method as I’ve described, but with weighted least squares instead of least squares. The weights will be \\(1/\\sigma_{ij}\\), where \\(\\sigma_{ij}\\) are the log-standard deviations of the distributions.\nThe formal reasoning behind this proposal goes as follows. If \\(\\eta_i/\\eta_j\\) is log-normal with some log-mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\), its logarithm is normal with mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\). This model implies that\n\\[\n\\frac{\\eta_i}{\\eta_j} = e^{\\mu_{ij}}e^{\\sigma_{ij}\\cdot\\epsilon_{ij}},\n\\]\nfor some normally distributed \\(\\epsilon_{ij}\\). Taking logarithms on both sides, we obtain\n\\[\n\\beta_i - \\beta_j = \\mu_{ij} + \\sigma_{ij}\\epsilon_{ij}.\n\\]\nSince \\(\\epsilon_{ij}\\) is symmetric around \\(0\\), we have \\(\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{ij}\\epsilon_{ij}\\). If we add the “safety measure constant” \\(\\psi\\), with\n\\[\\mu_{ij} = \\beta_i - \\beta_j  + \\psi\\sigma_{ij}\\epsilon_{ij}, \\tag{1}\\] this is a weighted least squares problem, with weights equal to \\(1/\\sigma_{ij}\\) and residual standard deviation \\(\\psi\\). It’s not clear to me if distributions are worth it.\nIn the summary I wrote that it’s not clear to me that it’s worth it to ask for distributions in pairwise comparisons, as your uncertainty level can be modeled implicitly by comparing different pairwise comparisons. What does this mean? Let’s simplify the model in Equation 1 so it contains two error terms, \\(\\sigma_i\\) and \\(\\sigma_j\\), one belonging to each question.\n\\[\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{i}\\epsilon_{i}+\\sigma_{j}\\epsilon_{j}. \\tag{2}\\]\nThis models allows you to have different uncertainties for different items, but doesn’t allow for idiosyncratic errors depending on interactions between the \\(i\\)th and \\(j\\)th items. Estimation of the model in Equation 2 easy to do efficiently, but I haven’t looked at the details. An idea would be to use the equation \\(\\operatorname{Var}(\\mu_{ij})=\\sigma_{i}^{2}+\\sigma_{j}^{2}\\) and iteratively refit weighted least squares models."
  }
]