[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Jonas Moss, a statistics post doc at BI Norwegian Business School. I’m interested in programming, the replication crisis in psychology, theoretical statistics, forecasting, effective altruism, and rationalism (as in Lesswrong)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonas Moss' blog",
    "section": "",
    "text": "effective altruism\n\n\nstatistics\n\n\nfermi estimates\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\neconomics\n\n\nmarketing\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nteaching\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nJonas Moss\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html",
    "href": "posts/alpha-and-omega/alpha-and-omega.html",
    "title": "No one would have invented coefficient alpha today",
    "section": "",
    "text": "Coefficient alpha is the most famous coefficient in psychometrics – Cronbach’s paper Coefficient alpha and the internal structure of tests has been cited around \\(60,00\\) times after all. It’s supposed to measure reliability. What does that mean? Intuitively, a psychometric scale is supposed to measure some kind of psychological construct, such as intelligence, in a reliable way. You don’t want it to be noisy. You don’t want two intelligence tests administered at slightly different times to give widely different results. You also want the test to actually measure intelligence, and not something else, such emotionality. But that’s validity, not reliability.\nIn classical test theory they think about psychometric tests in the context of a true score \\(T\\) and observed score \\(X\\). Then they write \\(X = T +\\epsilon\\) for some error term \\(\\epsilon\\). (This is always possible). For instance, if \\(X\\) is the observed score on an IQ test, \\(T\\) is the true, underlying intelligence.\nNow we’re ready to define classical reliability! The reliability is defined as \\(R=\\text{Cor}(X, T)^2\\), the squared correlation between the true score and the observed score. That’s a pretty reasonable definition! But the problem is that you don’t know the true score, so you can’t estimate this correlation directly.\nAnd that’s where coefficient alpha comes in. Suppose that \\(X\\) is a sum score, i.e., on the form \\(X = X_1 + X_2 + \\ldots + X_k\\). Then assume that \\[X_i = \\mu_i + \\lambda_i T + \\sigma_i \\epsilon_i \\tag{1}\\] for some error terms with variance equal to \\(1\\). In addition, assume that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) when \\(i = j\\), i.e., the error terms are uncorrelated. Then \\(X\\) follows the congeneric measurement model. Now define coefficient alpha as \\[\\alpha=\\frac{k}{k-1}\\left(1-\\frac{\\text{tr}\\Sigma}{{1}^{T}\\Sigma{1}}\\right),\\] where \\(\\Sigma\\) is the covariane matrix of the \\(X_i\\)s. This alpha has two important properties:\nThe condition of \\(\\tau\\)-equivalence means that all the factor loadings \\(\\lambda_i\\) are equal, i.e., the congeneric model eq. 1 is reduced to \\[X_i = \\mu_i + \\lambda T + \\sigma_i \\epsilon_i. \\tag{2}\\] There is widespread agreement that \\(\\tau\\)-equivalence never holds.\nAlpha is also the mean of all possible split-half reliabilities, but no one seem to care much about that today. It might have been important earlier on though. For more details see (Cho 2021)."
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "href": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "title": "No one would have invented coefficient alpha today",
    "section": "Why no one would have invented alpha today",
    "text": "Why no one would have invented alpha today\nTake a look at the congeneric model (eq. 1) again. This is a linear one-factor model with no correlation among the errors. In the early 20th century, there was no feasible way to estimate such models, at least not for ordinary psychometric researchers. But that’s not the case anymore. Anyone able to install R and run the easiest lavaan script can estimate such a model. For example, we can estimate the parameters for the agreeableness part of the psychTools::bfi data.\n\nlibrary(\"lavaan\")\nmodel <- \" f =~ A1 + A2 + A3 + A4 + A5 \"\nbfi <- psychTools::bfi[, 1:5]\nbfi[, 1] <- -bfi[, 1] # Reverse-coded question.\nobj <- lavaan::cfa(model, data = bfi, std.lv = TRUE)\nknitr::kable(round(coef(obj), 3))\n\n\n\n\n\nx\n\n\n\n\nf=~A1\n0.528\n\n\nf=~A2\n0.774\n\n\nf=~A3\n0.994\n\n\nf=~A4\n0.717\n\n\nf=~A5\n0.791\n\n\nA1~~A1\n1.693\n\n\nA2~~A2\n0.784\n\n\nA3~~A3\n0.714\n\n\nA4~~A4\n1.694\n\n\nA5~~A5\n0.965\n\n\n\n\n\nThe std.lv = TRUE argument forces the latent variable \\(T\\) to have variance equal to \\(1\\). This option makes it easier to interpret the remaining parameters. Using the estimated parameters of the lavaan object we can estimate the reliabiltiy \\(R\\) with little problems. Straight-forward calculations show that, when we assume the congeneric measurement model, \\[R = \\text{Cor}^{2}(X_1 + X_2+\\cdots+X_k,T)=\\frac{k\\overline{\\lambda}^{2}}{k\\overline{\\lambda}^{2}+\\overline{\\sigma^{2}}}, \\tag{3}\\] where \\(\\overline{x}\\) denotes the mean of the vector \\(x\\).\nUsing this formulation of the reliability in terms of \\(\\lambda\\) and \\(\\sigma\\), the natural estimator if the reliability is the plug-in estimator \\[\\hat{R} = \\frac{k\\overline{\\hat{\\lambda}}^{2}}{k\\overline{\\hat{\\lambda}}^{2}+\\overline{\\hat{\\sigma}^{2}}},\\] where \\(\\hat{\\lambda}\\) and \\(\\hat{\\sigma}\\) are estimators of your choice. If you’re using lavaan, you would calculate it using something like the following function.\n\n#' Estimate the reliability from a `lavaan` object.\n#' \n#' Estimate the reliability assuming a congeneric measurement model using the\n#'   plug-in estimator.\n#' @param obj A `lavaan` object.\n#' @return The estimated reliability coefficient.\nreliability <- function(obj) {\n  params <- lavaan::lavInspect(obj, what = \"coef\")\n  lambda <- params$lambda\n  sigma2 <- diag(params$theta)\n  k <- length(lambda)\n  k * mean(lambda) ^ 2 / (k * mean(lambda) ^ 2 + mean(sigma2))\n}\n\nFor the agreeableness data of psychTools::bfi, our reliability becomes\n\nreliability(obj)\n\n[1] 0.712129\n\n\nIt’s not hard to do inference for the reliability coefficient either – it is merely an application of the delta method. Moreover, by the invariance principle of maximum likelihood estimation, it is the maximum likelihood estimator of the reliability provided \\(\\lambda\\) and \\(\\sigma\\) are estimated using maximum likelihood. In this sense the estimator is natural.\nTo sum up:\n\nThere exists a natural estimator of the reliability under the congeneric measurement model.\nIt is easy to estimate and it’s efficient under normality.\nIts asymptotic theory is well-understood. That’s just a by-product of the immense amount of research on asymptotic theory for general structural equation models.\n\nI doubt anyone would have wanted to investigate alternatives to this 100% reasonable, simple, and efficient estimator. For why would they? It’s like finding an alternative estimator of the \\(R^2\\) that requires a little less computation power but is only consistent for the \\(R^2\\) under extraordinarily unlikely assumptions.\nMaybe someone would have uncovered coefficient alpha today, but its discovery would be treated as a novelty, not something of widespread importance, worth \\(60,000\\) citations."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html",
    "href": "posts/attenuation-survey/attenuation-survey.html",
    "title": "Inference for correlations corrected for attenuation",
    "section": "",
    "text": "You have two psychometric instruments, \\(\\hat{Z_1}\\) and \\(\\hat{Z_2}\\), measuring the true scores \\(Z_1\\) and \\(Z_2\\) with error. The estimators are linear in \\(Z_1,Z_2\\) with independent error terms, i.e. \\(\\hat{Z_1} = Z_1 + \\epsilon_1\\) and \\(\\hat{Z_2} = Z_2 + \\epsilon_2\\). You only observe the correlation between the measurements \\(\\hat{Z_1}\\) and \\(\\hat{Z_2}\\), but you’re interested in the correlation between the true scores \\(Z_1\\) and \\(Z_2\\). What should you do? The Spearman (Spearman 1904) attenuation formula states that \\[\\operatorname{Cor}(Z_1, Z_2) = \\frac{\\operatorname{Cor}(\\hat{Z_1}, \\hat{Z_2})}{\\operatorname{Cor}(Z_1,\\hat{Z_1})\\operatorname{Cor}(Z_2,\\hat{Z_2})}\\]A lot has been written about correction for attenuation. For instance, many people care about the easily verifiable and veritable horror that the sample disattenuated correlationmay be greater than \\(1\\)! But there’s not a lot written much about inference. This is a very short review of what I’ve read.\n\n\nThe Spearman attenuation formula does a *double correction* as it’s a formula for the correlation between both true scores \\(Z_1\\) and \\(Z_2\\). But it’s also possible to calculate the correlation between an estimator \\(\\hat{Z}_1\\) and the true score \\(Z_2\\). Then the single correction formula states\n\\[\\operatorname{Cor}(\\hat{Z}_1, Z_2) = \\frac{\\operatorname{Cor}(\\hat{Z_1}, \\hat{Z_2})}{\\operatorname{Cor}(Z_2,\\hat{Z_2}).} \\tag{1}\\] motivation for using the single correction is simple enough. The correlation between two variables quantifies how well you can predict one from the other. If you want to predict the true score, say intelligence, from an estimated score of openness, the single prediction formula should be used (Guilford 1954, 401; cited in Muchinsky 1996)."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#inference",
    "href": "posts/attenuation-survey/attenuation-survey.html#inference",
    "title": "Inference for correlations corrected for attenuation",
    "section": "Inference",
    "text": "Inference\nThere are some factors that make inference difficult:\n\nDifferent sample sizes. The sample sizes \\(n_1,n_2,n_3\\) for the two reliabilities and the correlation in the numerator can be different. This should be taken into account when constructing confidence intervals.\nWhich reliability method was used? Different reliability methods will not have the same asymptotic variance. This becomes clear when reading the ancient literature on this topic, as people still used split-half reliabilities at the time of writing of, e.g., Forsyth and Feldt (1969).\nWhat kind of model assumptions can we make? Simple asymptotics for the most commonly used reliability coefficient, coefficient alpha, is only available for pseudo-elliptically distributed variables under the parallel model (Yuan and Bentler 2002). But in order to use that, we must know the common kurtosis coefficient in addition to the value of alpha, and that is never reported. The asymptotically distribution-free interval is consistent in general, but is almost never used, but we could in principle deduce the asymptotic variance of the reliability estimator from a common asymptotic normality based interval if we wanted to. If we assume multivariate normality and the parallel model, the asymptotics of coefficient alpha is really simple though, depending only on the value of coefficient alpha itself (Zyl, Neudecker, and Nel 2000), and that’s probably the most reasonable thing to do in practice.\n\nThe most widely used inference method is the Hunter–Schmidt method (Schmidt and Hunter 1999), which ignores the errors in the reliability estimates, i.e., the estimators of \\(\\operatorname{Cor}(Z_1,\\hat{Z_1})\\) and \\(\\operatorname{Cor}(Z_2,\\hat{Z_2})\\). This method performs pretty well, at least when the sample size is sufficiently large. But it’s very crude. And come on – this is not such a hard problem that it can’t be properly solved, taking the variability of the reliabilities into account!"
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-hakstian1988-gz-paper",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-hakstian1988-gz-paper",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The Hakstian, Schroeder, and Rogers (1988) paper",
    "text": "The Hakstian, Schroeder, and Rogers (1988) paper\nThe authors take a look at an estimator that looks slightly different from what I am used to. Citing Rogers (1976), they employ the formula\n\\[\\hat{\\rho}(Z_{1},Z_{2})=\\frac{\\frac{1}{2}\\left[\\hat{\\rho}(\\hat{Z}_{1}^{1},\\hat{Z}_{2}^{1})+\\hat{\\rho}(\\hat{Z}_{1}^{2},\\hat{Z}_{2}^{2})\\right]}{\\hat{\\rho}(Z_{1},\\hat{Z}_{1})\\hat{\\rho}(Z_{2},\\hat{Z}_{2})} \\tag{2}\\] where \\(\\hat{\\rho}\\) denotes an estimator of the correlation. As before, the correlations in the denominator (i.e., the roots of the reliabilities) can’t be estimated directly, but the correlations in the numerator can.\nThe estimator of the correlation is a mean of two independently obtained estimators of the correlation. Which is fair enough, provided one has two samples with equally many participants in both… Which never happens! They do not mention the problem of different \\(n_1\\) and \\(n_2\\). Honestly, I don’t think there is much to gain from this paper. The paper of Rogers (1976) is similar; not much to pick up."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-charles2005-ze-paper",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-charles2005-ze-paper",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The Charles (2005) paper",
    "text": "The Charles (2005) paper\nThis paper has a decent literature overview, but its technical contributions are not very strong, It’s founded on a misconception:\n\nBy their original conception, confidence intervals give bounds for sample values likely to be produced by a population with known parameters (Neyman, 1934/1967), and I believe this is what past attempts at creating confidence intervals for [correction for attenuation due to measurement error] have been.\n\nNow this is just wrong. A confidence interval for a parameter \\(\\theta\\) is a random set \\(C\\) so that \\(P_\\theta(\\theta\\in C)\\geq 1-\\alpha\\). It’s not a bound for sample value likely to produced by population with known parameters."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-moss2019-kb-preprint",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-moss2019-kb-preprint",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The (Moss 2019) preprint",
    "text": "The (Moss 2019) preprint\nI wrote a note about correcting for attenuation in 2017. It uses an unconventional method to construct the confidence sets which is guaranteed to have coverage larger than the nominal (modulo uniformity requirements), but it’s extremely conservative and basically useless."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#what-now",
    "href": "posts/attenuation-survey/attenuation-survey.html#what-now",
    "title": "Inference for correlations corrected for attenuation",
    "section": "What now?",
    "text": "What now?\nWe can use the multivariate central limit theorem to deduce the limiting distribution of three independent measurements, the correlation, reliability 1, and reliability 2, even when their sample sizes are different. (I didn’t realize that when I wrote my preprint above.) Then we can use the delta method along with desired transformations, such as the Fisher transform, to make reasonable confidence sets."
  },
  {
    "objectID": "posts/monolithic-education/monolithic-education.html",
    "href": "posts/monolithic-education/monolithic-education.html",
    "title": "Monolithic education",
    "section": "",
    "text": "Curriculum. What should you teach the students? It’s not possible for a non-expert to know what’s important for him to learn; so an expert or group of expert is necessary here. But that person is not typically the same as the one who’s best suited to teach.\nTeaching system. How should the students learn whatever it is you want to teach? With lectures, guided exercise solution, class discussion, or something else? Maybe you should go for something along the lines Bikini Calculus.\nTeaching material. Someone has to make the teaching material. And what should that look like? No matter what format you choose, this is a whole lot of work.\nEvaluation. How should it be tested? All year round or just at a certain time and place? Should the students pay a fee for the exam? Can SRS be used somehow? Multiple choice?\n\nThese four pillars are not separated enough in practice. Even though they are clearly distinct and require different talents and different interests.\nIn high schools these components are, at least partly, at least in Norway, handled by different people. The curriculum is designed by the government. The teaching material is usually books, perhaps 2-3 to choose from for each subject. Evaluation is handled by the government. So you can, as a teacher, spend all your time teaching.\nIn universities, on the other hand, one professor is often tasked with doing everything. Design the curriculum. Figure out the teaching system. Perhaps even develop his own teaching materials – at the very least his own slides. And, of course, he must make his own exams.\n\nOn curriculum development\nI’m placing curriculum at the top of the list as it is both the most important and the most neglected. How often do you see students call for better curricula? If you ignore calls for anti-colonialization, probably never. But this is where you find the greatest potential for improvement. You’ll often find that introductory math and engineering courses, for instance, follow a pattern that was well-established even back in the 80s. They involve giving lip service to intuitions and proofs, focussing the entirety of the curriculum on gaining enough practice with a couple of calculation techniques required to pass the exam. And why is that? Probably because someone has to make the exam, and the more “examy” stuff the students know, the easier it is to make one.\nLet’s take a classical calculus course for example. A calculus course will often teach you how to solve integrals using partial fraction expansions. That is probably because it is a simple technique that does help you solve, by hand, a larger class of integrals than what you would have been able to had you not knowm it, thus expanding the pool of possible exercises to give the students at the exam. But does it actually help the student?\nPartial fraction expansion is not a “deep” part of integration, such as substitution and integration by parts. You need to learn these as they are essential both for conceptual understanding and most proofs. But partical fraction expansion is not. The student can use WolframAlpha when he’s calculating integrals on his own.\nDesigning a curriculum should involve carefully picking out the parts of a subject that are most important to understand and master in order to achieve a set of goals. The goals of calculus are, roughly, (i) to build mathematical maturity, (ii) to be comfortable with what limits, derivatives and integrals are, e.g., develop an inuitive understanding of why they are linear, (iii) understand their basic applications in optimization problems and physics, and then, (iv) to gain an intuition of what integrals are easy to calculate and when that matters.\nI’ve only spend 5 minutes thinking about these goals, and I’m open to counters. But calculus should not be about doing the maximal amount of integral calculations, competing in how fast you can use the chain rule, and so on."
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html",
    "href": "posts/peek-pairwise/peek-pairwise.html",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "",
    "text": "I had a peek at value estimation in economics and marketing. There is a sizable literature here, and more work is needed to figure out what exactly is relevant for effective altruists. Discrete choice models are applied a lot in economics, but these models are not able to estimate the scaling of the values. Marketing researchers prefer graded pairwise comparisons, which is equivalent to the pairwise method used here, but with limits on how much you can prefer one choice to another.\nI’m enthusiastic about the prospects of doing larger-scale paired comparison studies on EA topics. The first step would be to finish the statistical framework I started on here, then do a small-scale study suitable for a methodological journal in e.g. psychology or economics. Then we could run a study on a larger scale.\nMost examples I’ve seen in health economics, environmental economics, and marketing are only tangentially related to effective altruism. (I don’t claim they don’t exist – there’s probably many studies in health economics relevant to EA). But the topics of cognitive burden and experimental design is relevant for anyone who’s involved with value estimation. It would be good to have at least a medium effort report on these topics – I would certainly appreciate it! The literature probably contains a good deal of valuable insights for those sufficiently able and motivated to trudge through it.\nThere is a reasonable number of statistical papers on the graded comparisons. But mostly from the \\(50\\)s - \\(70\\)s. These will be very difficult to read unless you’re at the level of a capable master student of statistics. But summarizing and extending their research could potentially be an effective thesis!"
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html#context",
    "href": "posts/peek-pairwise/peek-pairwise.html#context",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "Context",
    "text": "Context\nIn my earlier post Estimating value from pairwise comparisons I wrote about a reasonable statistical model for the pairwise comparison experiments that Nuño Sempere at QURI have been doing (see also his sequence on estimating value). While writing that post I started thinking about fields where utility extraction is important, and decided to take a look at health economics and environmental economics. This post is a write-up of my attempt at a light survey of the literature on this topic, with particular attention paid on pairwise experiments.\nWhat do I mean by pairwise comparisons? Suppose I ask you “Do you prefer to lose your arm or your leg?” That’s a binary pairwise comparison between the two outcomes \\(A\\), \\(B\\), where \\(A = \\text{lose a leg}\\) and \\(B=\\text{lose an arm}\\). Such comparison studies are truly wide-spread, going back at least to Mcfadden (1973), which has \\(23000\\) Google Scholar citations! Models such as these are called discrete choice models, and I will refer to them as dichotomous (binary) comparisons as well, which is terminology I’ve seen in the economics literature. These models cannot measure the scale of the preferences though, only their ordering. There are many reasons why we care about the scale of preferences/utilities. For instance, we need scaling to compare preferences between different studies, and we need scales when we face uncertainty, as part of expected utility theory.\nTo take scale into account we can ask questions such as “How many times worse would it be to lose an arm than losing a leg?”. Then you might answer, say, \\(1/10\\), so you think losing a leg is ten times worse than losing an arm. Or \\(10\\), so you think losing an arm is ten times worse than losing a leg. These questions are harder than the corresponding binary questions though, and I can image respondents being flabbergasted by them. Questions of this kind are called graded (or ratio) comparisons in the literature. The idea is old – it goes way back to Thurstone (1927)!\nWe can mix graded and binary comparisons using stock standard maximum likelihood theory or Bayesian statistics. I haven’t figured out the exact conditions, but assuming we have enough graded question, we will be able to fix the scale reasonably well and gain information through the binary part only, reducing the cognitive load of the participants.\nI’m excited about the prospect of using pairwise comparisons on a large scale. Here are some applications:\n\nEstimate the value of research, both in the context of academia and effective altruism. This post presents a small-scale experiment in the EA context. It would be interesting to do a similar experiment inside of academia. Probably more rigorous and lengthy though. In my experience many academics do not feel that their or other people’s work is important. They research whatever is publishable since it’s their job. Attempting to quantify researchers understanding of the value of their and other people’s research could at least potentially push some researchers into a more effective direction.\nEstimating the value of EA projects. This should be pretty obvious. One of the potentials of the pairwise value estimation method is crowd-sourcing – since it’s so easy to say “I prefer \\(A\\) to \\(B\\)”, or perhaps “\\(A\\) is \\(10\\) times better than \\(B\\)” – the bar for participation is likely to be lower than, say, participating in Metaculus, which is a real hassle. Possible applications would be crowd-sourcing of valuation of small projects, e.g. something like Relative Impact of the First 10 EA Forum Prize Winners.\nDescriptive ethics. You could estimate moral weights for various species. You could get an understanding about how people vary in the their moral valuations. You could run experiments akin to the experiments underlying moral foundations theory, but with a much more quantitative flavor. I haven’t thought deeply about it, but I imagine studies of this sort would be important in the context of moral uncertainty.\n\nMoreover, I’m thinking about making pairwise estimation into an academic medium-sized project of its own. Very roughly, I’m thinking two steps would have to be completed.\n\nDevelopment of reasonable statistical techniques. The statistics aren’t that hard, it’s basically elementary techniques such as linear regression and Probit regression. Combined too! But it could be challenging to find optimal allocations of questions. This is the part where I, as a statistician, shine. Of course, it’s important to familiarize oneself with the literature on the topic one wishes to work on. Hence this post.\nDesign studies to see how well pairwise value estimation works. Is it worth bothering with it at all? Or maybe just in a couple of contexts? A possible course of action would be to conduct serious interviews with some professors about what research is valuable and why, then go around and estimate the pairwise model on PhD students. It would be nice to have a problem with both a ground truth and incentives to perform – sports prediction might be a better place to start. I don’t shine at stuff like this and would need help."
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html#small-literature-survey",
    "href": "posts/peek-pairwise/peek-pairwise.html#small-literature-survey",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "Small literature survey",
    "text": "Small literature survey\nI’ve spent a couple of hours surfing through the literature on choice modeling, estimation of value, and so on. I followed no methodology in choosing which papers to write about.\n\nGraded paired comparisons in statistics\nA Google Scholar search reveals plenty of statistical papers written in the 50s-70s, including a paper by the great statistician Scheffé (1952), who studies problems on the form\n\nIn a 7-point scoring system the judge presented with the ordered pair \\((i,j)\\) makes one of the following 7 statements:\n\n(3) I prefer \\(i\\) to \\(j\\) strongly.\n(2) I prefer \\(i\\) to \\(j\\) moderately.\n(1) I prefer \\(i\\) to \\(j\\) slightly.\n(0) No preference.\n(-1) I prefer \\(j\\) to \\(i\\) slightly.\n(-2) I prefer\\(j\\) to \\(i\\) moderately.\n(-3) I prefer \\(j\\) to \\(i\\) strongly.\n\n\nObserve that these numbers correspond roughly to taking logs of ratios, as I did in my previous post. He proceeds to analyze the problem in essentially the same way as I did, but he uses another “contrast” (in the notation of the post linked above, I force \\(\\beta_i\\) = \\(1\\) for some fixed \\(i\\), but he makes \\(\\sum \\beta_i = 0\\) instead; this should be familiar if you have taken a course in linear regression that included categorical covariates). But he also adds a test for what he calls “subtractivity”, i.e., the assumption that there is no interaction term involved in ratios. I’m very ready to just assume this, however. I don’t understand why he needs to restrict the possible values to \\(\\{-3,-2,-1,0,1,2,3\\}\\) though. It just doesn’t seem to matter, so I suppose it’s for he presentation’s sake.\nThere has been done plenty of methodological work on these methods in statistics. Sufficiently capable and interestedd methodologists should probably look at the literature and see what insights it contains. This will be hard though, as the literature is old, hard to read, and probably written in an unnecessarily convoluted way (at least judging from Scheffé’s paper) with lots of sums of squares and similar horrors.\n\n\nMarketing research\nGraded paired comparison studies are popular in marketing research for evaluating brand preference, but they are presented in a slightly different way: Now you have a fixed number of points to divide between two options, usually \\(9\\), I think. If you have \\(3\\) points to distribute, it would be formally equivalent to Scheffé’s setup above, but with the potential benefit that there is no implicit order of comparison. I think Scholz, Meissner, and Decker (2010) might be a decent entry-point to the marketing literature. Consider the following example from De Beuckelaer, Toonen, and Davidov (2013):\n\nPlease consider all presented bottled water brands [Spa, Sourcy, Evian, Chaudfontaine, and Vittel] equal in price and content (0.5 L). Which brand do you prefer? Please distribute x preference point/points.” (with x being replaced by one, five, seven, nine, or eleven).\n\nMaybe this is just a bad example, but I honestly can’t see the point of using the points here. Why not just ask which brand of bottled water you prefer? Also, why do you need a fixed number of points. Maybe to prevent respondents from saying “I like Coke \\(1000\\) times more than Pepsi!!!”? Or perhaps to emulate the feeling of having a fixed amount of money to spend?\nMuch of the methodological work in marketing appears to be about “complex” products with several attributes to rate. See e.g. this image from Scholz, Meissner, and Decker (2010), about the different attributes of a phone (display, brand, navigation). Is this relevant to EA? I’d say that it’s very likely to be relevant – we do care about attributes such as scale, neglectedness, and tractability after all.\nGraded paired comparisons are used in psychometrics too. Here they are used to reduce “cheating” in e.g. personality surveys due to social desirability bias, and the fact that you have fixed pool of points to spend matters. Brown and Maydeu-Olivares (2018) and Bürkner (2022) studied the statistical aspects. I can’t see any immediate application of these models for effective altruism though.\n\n\nEconomics\nI have comments on four papers. Baker et al. (2010), Vass, Rigby, and Payne (2017), and Ryan, Watson, and Entwistle (2009) are from health economics and Hanley, Mourato, and Wright (2001) from environmental economics.\n\nThe report of Baker et al. (2010)\nI think this report is about using surveying techniques to figure of if a “QALY is a QALY”, e.g., are some people’s QALYs woth less due to their age? But it’s pretty long and I only looked somewhat closely at their methodology. This report discusses two kinds of studies.\nDiscrete choice studies. (Chapter 4). Recall the definition of this kind of model. Here you choose between \\(A\\) and \\(B\\). You don’t say “\\(A\\) is ten times as good as \\(B\\)!” You just choose one of them. In this paper they use the discrete choice model to estimate the quality of life (QoL) using surveys.\nMatching studies. (Chapter 5). You have two options \\(A\\) and \\(B\\), and you’re asked which you prefer. If you say \\(A\\) , you’ll asked if prefer \\(\\frac{1}{2}A\\) to \\(B\\). We continue modifying \\(A\\) and \\(B\\) until you don’t prefer one to the other. This seems to be equivalent to graded comparisons, but the report is so verbose I can barely comprehend what they are doing. Their application is a kind of empirical ethics regarding the definition of QALYs.\n\nTo recap, a QALY is 1 year in full health and years spent in less than full health are ‘quality adjusted’ in order to take account of the morbidity associated with disability, disease or illness. As QALYs combine morbidity (QoL) and mortality (length of life) on a single scale, they allow comparisons to be made across interventions with different types of health outcomes (e.g. QoL enhancing versus life extending). In the standard ‘unweighted’ QALY model, all QALYs are of equal value. For example, the age of recipients does not matter, as long as the QALY gain is the same. Likewise, the standard model assumes that equal QALY gains are of equal value regardless of how severely ill the patients are prior to treatment. The aim of the matching – and DCE studies – is then to address the question of whether a QALY is a QALY is a QALY. Or put another way, is the ‘standard’ model correct?\n\nI haven’t read how they actually estimate these modified QALYs though.\n\n\nThe survey of Vass, Rigby, and Payne (2017)\nVass, Rigby, and Payne (2017) discusses the use of qualitative research in discrete choice experiments. This seems like a decent entry point to the literature in health economics, and I will probably rely on it in the future, if only for their reference list. The topic is also plausibly relevant to EAs, as the qualitative aspects of a discrete choice experiment essentially lies in its preparation – find the right questions, ask the right people, and so on.\nThey have a reasonably clear motivation for the use of discrete choice models in medicine.\n\nIn healthcare, decision making may involve careful assessment of the health benefits of an intervention. However, decision makers may wish to go beyond traditional clinical measures and incorporate ‘’non-health’’ values such as those derived from the process of healthcare delivery. DCEs allow for estimation of an individual’s preferences for both health benefits and non-health benefits and can explain the relative value of the different sources.\n\nThey mention, along with many other authors, the problem of cognitive burden.\n\nAny increases in the cognitive burden of the task could result in poorer quality data and should be considered carefully.\n\nAnd that’s the reason why don’t think it’s a good idea to ask respondents distributions, and would prefer to use discrete choice models as much as possible. (For remember that we can mix them with graded response models to keep the scale fixed).\n\n\nThe paper of Ryan, Watson, and Entwistle (2009)\nThis is a paper about how value elicitation experiments often reveal values incompatible with expected utility theory. They also include a discussion of methodologies used.\n\nThe four most commonly applied stated preference methods in health economics are contingent valuation (CV), discrete choice experiments (DCEs), standard gamble (SG), and time trade-off (TTO). Researchers have used quantitative tests to investigate whether responses to tasks set within each of these methods are consistent with the axioms of utility theory.\n\nThey include some examples of these methods in the context of statistical testing.\n\nContingent valuation. Hanley, Wright, and Koop (2002) use the terminology “choice experiment” instead. And when they describe their method it’s plain to see that they actually use a discrete choice model. But Ryan, Watson, and Entwistle (2009) later write that “CV asks respondents to trade the health good and money”. The term is really bad, as “contingent” can mean just about anything without proper context.\nDiscrete choice experiments. “DCEs ask respondents to trade between attributes.”\nStandard gamble and time trade-off. “SG and TTO test whether respondents make trade-offs between health status and risk or time”.\n\nStatistically speaking, CV, DCE, SG and TTO should be understood as the same thing, the only difference being “what is compared to what”, where DCE compares attributes to attributes and CV compares attributes vs money. Perhaps nice to know if you’re going to dive into the literature on this topic.\n\n\nPaper of Hanley, Mourato, and Wright (2001)\nThe author discuss the “contingent valuation method”, which appears to be a willingness-to-pay experiment (Ahlert, Breyer, and Schwettmann 2013). I don’t think we can make use of that though.\n\nBy means of an appropriately designed questionnaire, a hypothetical market is described where the good or service in question can be traded. […] Respondents are then asked to express their maximum willingness to pay […]\n\nThen they discuss choice modelling.\n\n[Choice modelling] is a family of survey-based methodologies for modelling preferences for goods, where goods are described in terms of their attributes and of the levels these take.\n\nIn Table 2 they describe four kinds of choice modelling:\n\nChoice experiments. This is equivalent to dichotomous comparisons.\nContingent ranking. Rank a bunch of different alternatives, e.g. “What do you prefer? Losing an arm, a leg, or a tooth? Rank from best to worst.”.\nContingent rating. This is direct scoring of values. Hanley, Mourato, and Wright (2001) mentions a \\(10\\) point scale, but we would probably allow them to use \\(\\mathbb{R}^+\\) instead.\nPaired comparisons. This is graded comparisons.\n\nThey claim that option (1) yields “welfare consistent estimates”, but the others probably don’t. I don’t know if this is important or not, as I don’t know what it means.\nAll of these models are possible to combine, but we won’t be able to fix the scale without using graded paired comparisons. Option (2) might possibly be worth investigating, especially if the number of rankings are small, where the additional cognitive burden is low. But Hanley, Mourato, and Wright (2001) states that “choices seem to be unreliable and inconsistent across ranks,” making it less likely that this is worth exploring. The Schulze method is also based on ranks, and could be relevant here."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#summary",
    "href": "posts/quantiles/quantiles.html#summary",
    "title": "Deriving distributions from quantiles",
    "section": "1 Summary",
    "text": "1 Summary\n\nGood methods for eliciting densities from quantiles should satisfy six conditions.\nI propose a class of method for constructing densities for quantiles based on transformations and penalized monotone B-splines. It satisfies 3 or 4 of the conditions with appropriate tweaking.\nSome of the weaknesses of my proposed method may be ameliorated. I make some suggestions about how to do this.\nThere are infinitely many ways to translate quantiles to densities. I sketch another one at the end."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#introduction",
    "href": "posts/quantiles/quantiles.html#introduction",
    "title": "Deriving distributions from quantiles",
    "section": "2 Introduction",
    "text": "2 Introduction\nWhile doing Fermi estimation (“guesstimation”) you often want to construct a distribution from quantile knowledge. Dealing with two quantiles is quite easy, as you have plenty of distribution families that are easy to fit. Location-scale families, such as the normal distribution, logistic distribution, Cauchy distribution, or the shifted exponential distribution are particularly easy to fit. Moreover, a monotonically transformed location-scale distribution is equally easy to work with, e.g. the log-normal and log-logistic distributions.\n\n\n\n\n\n\nSome details on location-scale distributions\n\n\n\n\n\nA variable \\(Y\\) belongs to a location-scale family of distributions if it can be written on the form \\(Y=\\mu+\\sigma X\\). If \\(F\\) is the distribution function \\(X\\), then \\(F\\left(\\frac{y-\\mu}{\\sigma}\\right)\\) is the distribution function of \\(Y\\), as \\[P(Y\\leq y)    =   P(\\mu+\\sigma X\\leq y)\n    =   P(X\\leq(y-\\mu)/\\sigma)\n    =   F\\left(\\frac{y-\\mu}{\\sigma}\\right).\\]\nWe also find that the quantile function of \\(Y\\) equals \\(Q_Y(p)=\\mu+\\sigma Q(p)\\), where \\(Q\\) is the quantile function of \\(X\\).\nWe can derive \\(\\mu\\) and \\(\\sigma\\) by matching quantiles: \\[\\begin{eqnarray*}\nq_{1} & = & \\mu+\\sigma F^{-1}(p_{1}),\\\\\nq_{2} & = & \\mu+\\sigma F^{-1}(p_{2}).\n\\end{eqnarray*}\\]\nIf \\(g\\) is a strictly increasing transformation then \\(Y=g(\\mu + \\sigma X)\\) is a transformed location-scale distribution. You can derive its quantiles in the same way as you would a location-scale distribution, just be sure to apply \\(g^{-1}\\). Usually \\(g=\\exp\\) and \\(X\\) is normal, yielding the log-normal distribution.\n\n\n\nBut what do we do with more than two quantiles? We could fit a \\(k\\)-parameter distribution, but there are few canonical densities to choose from with \\(k>2\\) parameters. And it’s hard to justify your choice in any case. For two (or one) parameters, the normal distribution can often be justified for data on \\([-\\infty, \\infty]\\) based on the central limit theorem; likewise, the log-normal can be justified based on a product argument – and the exponential and half-normal with their own reasoning methods. Three or more though? There’s just no standard way to do it."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#what-would-we-like",
    "href": "posts/quantiles/quantiles.html#what-would-we-like",
    "title": "Deriving distributions from quantiles",
    "section": "3 What would we like?",
    "text": "3 What would we like?\nHow should we go about deciding on a class of distribution to fit quantiles to? Here are 6 conditions I like.\n\nControl of tail behavior. The exact shape of the distributions “in the middle” – where we are likely to have the strongest intuitions – often doesn’t matter too much. If we’re dealing with, say, insurance, having control of the tails is just as important as fine-grained control over how the curve looks in the center.\nGood looks. This is a vague requirement, but can probably be replaced with the almost equally vague requirement of the densities being smooth. Smoothness is not just about aesthetics, as we expect the best predictive densities to be smooth, with continuous second derivatives at the very least. We would also like to avoid multimodality when possible. It’s easy to make densities with bad looks, and severe discontinuities, see e.g. Figure 2. We don’t want those\nPrincipled derivation. We do not want the densities to be ad-hoc but derived using sound principles such as maximum entropy. It’s not trivial to combine this with the good-looks demand, as maximum entropy + quantile information yields discontinuous densities!\nBounded away from \\(0\\). We don’t want out densities to be \\(0\\) at arbitrary places! See Figure 6 for an example of such bad behavior.\nEasy to sample from. Especially relevant for Squiggle applications.\nQuick to calculate, cheap to store, etc. We always want convenience, but it’s hard to say as an outsider what is most important! Quick computation would matter for interfaces used by products such as Metaculus.\n\nHow can you construct densities that match aribtrary quantiles? The most obvious starting points arew mixture distributions and quantile mixtures.\nMixture distributions can be written on the form \\(G(x) = \\sum_i^n \\lambda_i F_i(x, \\theta_i)\\) for a collection of parameterized basis distributions \\(F_i(x;\\theta_i)\\). By making \\(n\\) big enough and \\(F_i(\\theta_i)\\) sufficiently flexible, we can match any quantile to it. The most famous member of this family are normal mixtures. Mixture distributions have been studied a lot. There might be reasonably simple iterative methods to fit mixtures of location-scale distributions, but it seems hard, as the quantile function has no closed form. But different forms of “basis distributions” can be fitted quite easily, at least on \\([0,1]\\). Perhaps best known among these are the Bernstein polynomials (themselves special instances of the Beta distribution), which will fit any distribution arbitrarily well in the limit by the Weierstrass approximation theorem. Another good option is to use splines, which can be constrained to be positive and increasing without too much difficulty.\nQuantile-parameterized distributions have their quantile distributions written on the form \\(Q(x) = \\sum_i^n \\lambda_i q_i(x; \\theta_i)\\) instead. This class of distributions purportedly includes the metalog distribution with its bloated wikipedia page, but in reality they require minor a conceptual modification to fit into the framework, as its component are not bona fide quantile functions. In principle, such “quantile mixtures” can be matched to any quantile using non-negative least squares, provided the basis quantiles are flexible enough. This is entirely feasible to do – provided the domain of the vaiables are bounded on \\([0,1]\\), using them same methods as we would use for fitting cumulative distribution functions. For quantile functions and distributions functions on \\([0,1]\\) are the same thing - increasing and positive functions bounded by \\([0,1]\\), so we have freedom in choosing which to model.\nSince it is possible to fit easily fit quantiles on the unit interval the following strategy, as illustrated in Figure 1, sounds promising:\n\nStart out with a base distribution, such as the log-normal, with distribution function \\(F\\), and density function \\(f\\). Then transform the quantiles \\(q=q_{1},q_{2},\\ldots,q_{k}\\) to the unit interval using \\(F\\); call the transformed quantiles \\(x\\) and the associated probabilities \\(y\\). We can then use these quantiles to construct a distribution \\(S\\) (and associated density \\(s\\)) on the unit interval with the desired quantiles.\nFit the quantiles on the unit interval. We will look at the piecewise linear approximation, monotone interpolating splines, and penalized monotone B-splines. The penalized monotone B-splines is the only promising approach.\nTransform the fitted distribution back, using the quantile function. This yields the density \\(g(x)=f(x)s(F(x))\\), cumulativedistribution function \\(G(x)=S(F(x))\\), and quantile function \\(G^{-1}(x)=F^{-1}(S^{-1}(x))\\).\n\n\n\n\n\n\nflowchart TD\n  B([Quantiles]) --> C\n  A([Base \\n distribution])-->C[Transformed \\n quantiles]\n  D([Method to create\\n distribution on\\n unit interval]) --> E[Distribution on\\n unit interval]\n  C --> E\n  A --> F[Final distribution]\n  E --> F[Final distribution]\n  \n\n\n\n\n\nFigure 1: Diagram of the proposed method. Boxes with rounded edges are input variables. Most of the complexity lies in the method used to create a distribution on the unit interval.\n\n\n\n\nLet’s take a look at the piecewise linear approach and the monotone interpolating splines to see why they are not promising.\n\n3.1 Piecewise linear\nOur base distribution is the log-normal.\n\np <- plnorm; d <- dlnorm; q <- qlnorm\n\nThe quantiles are semi-randomly chosen to be as follows\n\nqq <- c(0, q(0.1), q(0.85), q(0.9), Inf)\ny <- c(0, 0.1, 0.5, 0.9, 1)\nx <- p(qq)\nround(qq, 2)\n\n[1] 0.00 0.28 2.82 3.60  Inf\n\n\nI’ll use the log-normal base distribution and these quantiles throughout this post.\nThe piecewise linear approximation and its associated histogram can be calculated using splines.\n\n\nFunction to the piecewise linear approximation and its associated histogram. The implementation was fast to write, but there are more efficient implementations available.\npiecewise <- function(x, y) {\n  sf <- \\(y,...) splines2::iSpline(\n    y, \n    intercept = TRUE, \n    knots = x[2:(length(x) - 1)],\n    degree = 0,\n    Boundary.knots = c(x[1], x[length(x)]),\n    ...)\n  xmat <- sf(x)\n  coefs <- nnls::nnls(A = xmat, b = y)$x\n  pdf_untrans = \\(w) sf(w, derivs = 1) %*% coefs\n  cdf_untrans = \\(w) sf(w) %*% coefs\n  list(\n    x = x,\n    y = y,\n    pdf = \\(w, d = dunif, p = punif) d(w) * pdf_untrans(p(w)),\n    cdf = \\(w, p = punif) cdf_untrans(p(w))\n  )\n}\n\n\nThe resulting distribution, density, and target density can be seen below.\n\n\nCode for plotting the piecewise linear CDF and PDF on the transformed scale and the PDF on the untransformed scale.\nobj <- piecewise(x, y)\nlayout(matrix(c(1,2,3,3, 3, 3), nrow = 3, ncol = 2, byrow = TRUE), \n       heights = c(3, 2))\nz <- seq(0, 1, by = 0.001)\nplot(z, obj$cdf(z), ylim = c(0, 1), type = \"l\", ylab = \"Cumulative probability\", xlab = \"x\",\n     main = \"Transformed distribution function\")\nabline(h = y, v = x, lty = 3)\nplot(z, obj$pdf(z), type = \"l\", ylab = \"Density\", xlab = \"x\", \n     main = \"Transformed density\")\n\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 2: Plot of a piecewise linear transformed CDF and piecewise densities.\n\n\n\n\nThe target density is discontinuous.\n\n\n3.2 Monotone interpolation splines\nMonotone interpolation splines are used to interpolate a sequence of points with a monotone function. There are two variants implemented in R, the Hyman (1983) splines and the Fritsch–Carlson (1980) splines. Both yield functions with continuous first derivative. Since we’re fitting the cumulative distribution function, this guarantees that the density function is continuous.\n\n\nCode for Hyman splines and Fritsch–Carlson splines\nf1 <- splinefun(x, y, method = \"hyman\")\nf2 <- splinefun(x, y, method = \"monoH.FC\")\nz <- seq(0, 7, by = 0.001)\nplot(z, dlnorm(z) * f1(plnorm(z), deriv = 1), type = \"l\", \n     main = \"Target density with Hyman (black) and Fritsch-Carlson (blue) splines\")\nlines(z, dlnorm(z) * f2(plnorm(z), deriv = 1), type = \"l\", col = \"blue\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 3: Plot of target densities with Hyman splines and Fritsch–Carlson splines.\n\n\n\n\nBoth methods yield ugly target densities, probably because the resulting interpolating splines only have continuous first derivatives, not continuous second derivatives (as most cubic splines have). Moreover, the Hyman method yields a density that fails to be bounded away from \\(0\\). Since there is no obvious way to fix these splines, I’m moving on to another method."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#fitting-monotone-b-splines-with-penalties",
    "href": "posts/quantiles/quantiles.html#fitting-monotone-b-splines-with-penalties",
    "title": "Deriving distributions from quantiles",
    "section": "4 Fitting monotone B-splines with penalties",
    "text": "4 Fitting monotone B-splines with penalties\nRecall that \\(x_i\\) are the transformed quantiles. We need to find and \\(F\\) that satisfies \\(F(x_i) = y_i\\). Since this isn’t enough to identify the \\(F\\), we can try minimize the squared distance between \\(F\\) and the identity function \\(\\iota\\) defined by \\(\\iota(x) = x\\) for some function class \\(\\mathcal{F}\\) that is contained in the set of increasing functions. \\[\\min_{F(x_{i})=y_{i},F\\in\\mathcal{F}}\\|F-\\iota \\|^2 \\tag{1}\\]\nMy choice of \\(\\mathcal{F}\\) is the set of monotone B-splines with \\(m\\) uniformly spaced internal knots and boundary knots at \\(0\\) and \\(1\\). These splines are most likely guaranteed to interpolate your quantiles when \\(m\\) is large enough, but I don’t know the details. The result is a quadratic program with linear constraints; see the code in Section 4.1. (I can add mathematical details upon request.)\nWhen using splines of degree \\(3\\) and \\(m = 40\\) on the quantiles \\(x\\) introduced above, we get the following result.\n\n\n\n\n\nFigure 4: Monotone B-sline density without penalization.\n\n\n\n\nThe second bump looks beautiful and smooth! But the first bump is to irregular, lacking smoothness at its end and going up and down too much. Moreover, the density is \\(0\\) almost all the way from \\(0\\) to \\(3\\), which we typically do not want.\nTo fix the smoothness problems we add a penalty term for the integrated squared second derivative together with a parameter \\(\\lambda\\) regulating the influence of the penalty term. (Just like the \\(\\lambda\\) in ridge regression regression). To fix \\(0\\) problem, we add a penalty term to the integrated squared first derivative, parameterized by \\(\\mu\\). Both of these terms can be added to the quadratic implied by Equation 1.\n\n4.1 The fitter function\nThe R function below fits the splines using the arguments \\(m\\) (number of knots), the degree of the B-splines (degree), \\(\\lambda\\) (the penalty for the second derivative), \\(\\mu\\) (the penalty for the first derivative).\n\n\nThe fitter functions and its helpers.\n#' Fits a density to quantiles.\n#'\n#' The method is based on monotone splines. We minimize the squared distance\n#'    between the spline approximation and the identity function. Thus you\n#'    minimize the distance between the CDF and the uniform CDF in a\n#'    q-transformed space. The parameters `lambda` and `mu` are to tweak the\n#'    resulting density into the desired shape.\n#'\n#' The function returns\n#' @param x,y Vector of probabilities (`x`) and quantiles (`y`). The quantiles\n#'    must have been transformed to the unit interval using your desired `q`.\n#' @param m Number of internal knots in the spline.\n#' @param degree Degree of the spline function. Defaults to `3`, which\n#'   corresponds to cubic splines.\n#' @param lambda The penalty term for the squared second derivative.\n#' @param mu The penalty term for the squared derivative.\n#' @return A list containing the fitted spline together with density functions,\n#'   and so on.\n\nfitter <- function(x, y, m = 20, degree = 3, lambda = 0.5, mu = 0) {\n  knots <- (2:m) / (m + 1)\n  boundary_knots <- c(y[1], y[length(y)])\n  \n  sf <- \\(x, derivs = 0) splines2::bSpline(\n    x,\n    intercept = TRUE,\n    knots = knots,\n    degree = degree,\n    Boundary.knots = boundary_knots,\n    derivs = derivs\n  )\n  \n  matrices <- get_penalties(knots, boundary_knots, degree)\n  x_mat <- sf(x)\n  f <- Vectorize(\\(x, i) x * sf(x)[i])\n  k <- ncol(x_mat)\n  sx <- sapply(seq(k), \\(i) integrate(f, lower = 0, upper = 1, i = i)$value)\n  constraints <- increasing(ncol(x_mat) - 1)\n  \n  fit <- quadprog::solve.QP(\n    Dmat = matrices$s2_mat + lambda * matrices$p_mat + mu * matrices$p1_mat,\n    dvec = sx,\n    Amat = t(rbind(x_mat, constraints$amat)),\n    bvec = c(y, constraints$bvec),\n    meq = length(y)\n  )\n  \n  coefs <- fit$solution\n  pdf_untrans = \\(w) sf(w, derivs = 1) %*% coefs\n  cdf_untrans = \\(w) sf(w) %*% coefs\n  \n  list(\n    x = x,\n    y = y,\n    m = m,\n    degree = degree,\n    lambda = lambda,\n    mu = mu,\n    fit = fit,\n    pdf = \\(w, d = dunif, p = punif) d(w) * pdf_untrans(p(w)),\n    cdf = \\(w, p = punif) cdf_untrans(p(w))\n  )\n}\n\n#' Get penalty matrices for the fitter.\n#' @param knots The internal knots.\n#' @param boundary_knots The boundary knots.\n#' @param degree Degree of the B-spline.\n#' @return List of penalty matrices.\nget_penalties <- function(knots, boundary_knots, degree) {\n  basis_obj <- fda::create.bspline.basis(\n    rangeval = boundary_knots,\n    norder = degree + 1,\n    breaks = c(boundary_knots[1], knots, boundary_knots[2])\n  )\n  inds <- seq(basis_obj$nbasis)\n  list(\n    s2_mat = fda::bsplinepen(basis_obj, Lfdobj = 0)[inds, inds],\n    p_mat = fda::bsplinepen(basis_obj, Lfdobj = 2)[inds, inds],\n    p1_mat = fda::bsplinepen(basis_obj, Lfdobj = 1)[inds, inds]\n  )\n}\n\n#' Increasing constraints matrix and vector\n#'\n#' @keywords internal\n#' @param m The number of knots in the spline.\n#' @param intercept If `TRUE`, the model includes an intercept.\n#' @return The increasing constraint matrix.\n\nincreasing <- function(m, intercept = TRUE) {\n  amat <- cbind(0, -cbind(diag(m - 1), 0) + cbind(0, diag(m - 1)))\n  amat <- rbind(0, amat)\n  amat <- rbind(amat, 0)\n  amat[m + 1, c(1, m + 1)] <- -1\n  amat[1, 2] <- 1\n  amat <- rbind(0, amat)\n  amat[1, 1] <- 1\n  \n  if (!intercept) {\n    amat <- amat[2:nrow(amat), 2:ncol(amat)]\n  }\n  \n  bvec <- rep(0, nrow(amat))\n  bvec[nrow(amat)] <- -1\n  \n  list(\n    amat = amat,\n    bvec = bvec\n  )\n}\n\n\n\n\n4.2 Fixing the density\nUsing \\(m = 40\\), a degree of \\(3\\), \\(\\lambda = 0.01\\), and \\(\\mu = 5\\) fixes Figure 4.\n\n\nConstructing and plotting a monotone B-spline with a good penalization.\nobj <- fitter(x, y, m = 40, degree = 3, lambda = 0.01, mu = 5)\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 5: A good-looking density.\n\n\n\n\nThis example suggests that the penalized monotone B-spline method can be good-looking, avoid the 0 problem, control the tail behavior, and be at least reasonably quick to calculate (as quadratic programs of this small size are fast to solve). I’m not sure how easy they will be to sample from though, and I’m even less sure they have a principled derivation. But their biggest problem is parameter tuning.\nWe can reintroduce the \\(0\\) problem by modifying \\(\\lambda\\). Setting \\(\\lambda = 0.03\\) instead of \\(\\lambda = 0.01\\) gives us the following. It appears that smoothing too much reintroduces the \\(0\\) problem. I’m not sure why though.\n\n\nConstructing and plotting a monotone B-spline with a bad penalization.\nobj <- fitter(x, y, m = 30, degree = 4, lambda = 0.03, mu = 5)\n\nlayout(matrix(c(1,2,3,3,3,3), nrow = 3, ncol = 2, byrow = TRUE))\nz <- seq(0, 1, by = 0.001)\nplot(z, obj$cdf(z), ylim = c(0, 1), type = \"l\", ylab = \"Cumulative probability\", xlab = \"x\",\n     main = \"Transformed distribution function\")\nabline(h = y, v = x, lty = 3)\n\nplot(z, obj$pdf(z), type = \"l\", ylab = \"Density\", xlab = \"x\", \n     main = \"Transformed density\")\n\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 6: A bad density that’s equal to \\(0\\) at places we don’t want it to when smoothing too much.\n\n\n\n\n\n\n4.3 Changing the base distributions\nThe choice of base distribution has a big effect on the target distribution. Figure 7 shows an example.\n\n\nPlotting many target densities.\nplotter <- function(d, p, q, main) {\n  x <- p(c(0, 1, 2, 3, Inf))\n  y <- c(0, 0.1, 0.5, 0.9, 1)\n  obj <- fitter(x, y, m = 90, degree = 5, lambda = 0.1, mu = 5)\n  w <- seq(0, 5, by = 0.001)\n  plot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\",\n       main = main)\n  abline(v = q(x), lty = 2, col = \"red\")\n}\n\nlayout(matrix(c(1,2,1,2,3,4,3,4,5,6,5,6), nrow = 6, ncol = 2, byrow = TRUE))\n\nplotter(extraDistr::drayleigh, extraDistr::prayleigh, extraDistr::qrayleigh, main = \"Rayleigh\")\nplotter(dexp, pexp, qexp, main = \"Exponential\")\nplotter(dlnorm, plnorm, qlnorm, main = \"Log-normal\")\nplotter(extraDistr::dhcauchy, extraDistr::phcauchy, extraDistr::qhcauchy, main = \"Half-Cauchy\")\nplotter(extraDistr::dhnorm, extraDistr::phnorm, extraDistr::qhnorm, main = \"Half-Normal\")\nplotter(extraDistr::dfrechet, extraDistr::pfrechet, extraDistr::qfrechet, main = \"Fr\\u{E9}chet\")\n\n\n\n\n\nFigure 7: Target densities based on \\(6\\) base distributions. The degree is \\(5\\), the number of internal knots is \\(m = 90\\), \\(\\lambda = 0.1\\), and \\(\\mu = 5\\).\n\n\n\n\n\n\n4.4 Potential modifications\nIt could be worth it to attempt a modification of the penalties. As currently computed, the penalties, involving the squared integrals \\(\\int [f^{(p)}(x)]^2dx\\), are calculated on the transformed level. They should ideally be calculated on the untransformed level instead, i.e., \\(\\int [g^{(p)}(x)]^2dx\\) in our notation. But this is unlikely to be feasible, at least in a production setting. (Numerical calculations could be possible, but are slow). Maybe it would work to use a finite difference approximations similar to the one used by \\(P\\)-splines instead.\nChange the minimand in Equation 1 to the piecewise linear approximation \\(F_l\\) instead of the identity function. It’s likely that this would remove the problem of the density being equal to \\(0\\) of Figure 1. We could also try to change the minimand something wild, such as the least concave majorant or the greatest convex minorant. The least concave majorant corresponds to the best-fitting increasing density compatible with the supplied quantiles.\nWe can guarantee identical tail behavior for the base distribution and the quantile-transformed distribution by forcing \\(S\\) to be linear in when \\(x\\) is sufficiently close to \\(0\\) or \\(1\\), e.g., \\(\\epsilon\\) away from the edges. That would involve modifying the input data and append lines to the edges of the output function\nIt is possible to enforce shape constraints on the densities, such as being increasing, decreasing, or unimodal. This can be done by adding linear constraints to the quadratic program.\n\n\n4.5 What remains to be done\n\nWe need to implement automatic scaling, i.e., we need to choose the appropriate parameters for the base distribution, probably by matching its parameters to the supplied quantiles. This is probably required for the methods to work in high generality (you can’t use the fitter function (Section 4.1) directly when the numbers are too large, e.g., choosing quantiles of several hundreds in Figure 7 won’t work).\nAutomatic selection of the tuning parameters \\(\\lambda\\), \\(\\mu\\), the number of knots \\(m\\), and the degree of the splines.\nInvert the function \\(S\\). We need the quantile function to generate random samples.\nInvestigate some of the proposals in the previous section."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#fit-splines-without-transformations",
    "href": "posts/quantiles/quantiles.html#fit-splines-without-transformations",
    "title": "Deriving distributions from quantiles",
    "section": "5 Fit splines without transformations",
    "text": "5 Fit splines without transformations\nYou can fit splines without transformations too. This would involve choosing the form of the tails, then match the derivatives of the splines to the derivatives of the tails, on either side. This approach lets us enforce good looks on the right scale, and could work much better than the transform approach. But it should be somewhat harder to implement, as the derivative matching requires some manual work. The approach is also quite principled; it can be framed as an approximation to the maximum entropy solution to the elicitation problem when the tails are known with smoothness penalties."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html",
    "href": "posts/value-estimation/value-estimation.html",
    "title": "Estimating value from pairwise comparisons",
    "section": "",
    "text": "How can you estimate the value of research output? You could use pairwise comparisons, e.g., to ask specialists how much more valuable Darwin’s The Original of Species is than Dembski’s Intelligent Design. Then you can use these relative valuations to estimate absolute valuations."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#summary",
    "href": "posts/value-estimation/value-estimation.html#summary",
    "title": "Estimating value from pairwise comparisons",
    "section": "Summary",
    "text": "Summary\n\nEstimating values is hard. One way to elicit value estimates is ask researchers to compare two different items \\(A\\) and \\(B\\), asking how much better \\(A\\) is than \\(B\\). This makes the problem more concrete than just asking “what is the value of \\(A\\)?”. The Quantified Uncertainty Institute has made an app for doing this kind of thing, described here.\nNuño Sempere had a post about eliciting comparisons of research value from \\(6\\) effective altruism researchers. This is a more recent post about AI risk, but it uses distributions instead of point estimates.\nThis post proposes some technical solutions to problems introduced to me in Nuño’s post. In particular, it includes principled ways to\n\nestimate subjective values,\nmeasure consistency in pairwise value judgments,\nmeasure agreement between the raters,\naggregate subjective values.\nI also propose to use weighted least squares when the raters supply distributions instead of numbers. It is not clear to me it is worth it to ask for distributions in these kinds of questions though, as your uncertainty level can be modelled implicitly by comparing different pairwise comparisons.\n\nI use these methods on the data from the 6 researchers post.\n\nI’m assuming you have read the 6 researchers post recently. I think this post will be hard to read if you haven’t.\nNote: Also posted at the EA Forum. This document is a compiled Quarto file with source functions outside of the main document. The functions can be found in the source folder for this post. Also, thanks to Nuño Sempere for his comments on a draft of the post! Update 6/10/2022. Modified Gavin’s confidence interval for A Mathematical Theory to show the correct number.\n\nWhat’s this about\nTable 1 contains the first \\(6\\) out of \\(36\\) responses from Gavin Leech. As you can see, he values Superintelligence \\(100\\) more than the Global Priorities Institute’s Research Agenda.\n\n\nA list of questions in the data set.\nknitr::kable(head(gavin[, 1:3]))\n\n\n\n\nTable 1: First 6 questions Gavin answered.\n\n\n\n\n\n\n\nsource\ntarget\ndistance\n\n\n\n\nThinking Fast and Slow\nThe Global Priorities Institute’s Research Agenda\n100\n\n\nThe Global Priorities Institute’s Research Agenda\nThe Mathematical Theory of Communication\n1000\n\n\nSuperintelligence\nThe Mathematical Theory of Communication\n10\n\n\nCategorizing Variants of Goodhart’s Law\nThe Vulnerable World Hypothesis\n10\n\n\nShallow evaluations of longtermist organizations\nThe motivated reasoning critique of effective altruism\n10\n\n\nShallow evaluations of longtermist organizations\nCategorizing Variants of Goodhart’s Law\n100\n\n\n\n\n\n\nMy first goal is to take relative value judgments such these and use them to estimate the true subjective values. In this case, I want to estimate the value that Gavin Leech places on every article in the data set, as contained in Table 2.\n\n\nA list of questions in the data set.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nknitr::kable(cbind(1:15, levels))\n\n\n\n\nTable 2: All \\(15\\) articles valuated in the data set. Question \\(3\\) is fixed to \\(1\\).\n\n\n\n\n\n\n\nlevels\n\n\n\n\n1\nA comment on setting up a charity\n\n\n2\nA Model of Patient Spending and Movement Building\n\n\n3\nCategorizing Variants of Goodhart’s Law\n\n\n4\nCenter for Election Science EA Wiki stub\n\n\n5\nDatabase of orgs relevant to longtermist/x-risk work\n\n\n6\nExtinguishing or preventing coal seam fires is a potential cause area\n\n\n7\nReversals in Psychology\n\n\n8\nShallow evaluations of longtermist organizations\n\n\n9\nSuperintelligence\n\n\n10\nThe Global Priorities Institute’s Research Agenda\n\n\n11\nThe Mathematical Theory of Communication\n\n\n12\nThe motivated reasoning critique of effective altruism\n\n\n13\nThe Vulnerable World Hypothesis\n\n\n14\nThinking Fast and Slow\n\n\n15\nWhat are some low-information priors that you find practically useful for thinking about the world?\n\n\n\n\n\n\nThe article Categorizing Variants of Goodhart’s Law has value fixed to \\(1\\). I will use the numbering above throughout this post."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "href": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "title": "Estimating value from pairwise comparisons",
    "section": "A model with multiplicative error terms",
    "text": "A model with multiplicative error terms\n\nMotivation and setup\nLet \\(\\eta_i\\) be the true subjective value of item \\(i\\), counting starting from \\(1\\). We will let \\(\\eta_3=1\\) in our setup, as Manheim and Garabrant’s Categorizing Variants of Goodhart’s Law was fixed to \\(1\\) in Nuño’s study, but we could have fixed any other item if we wanted to. Ideally, we would have observed the “distances” \\(d_{ij}=\\eta_i/\\eta_j\\) directly, but we don’t. Instead, we observe the distances with noise, \\(\\hat{d}_{ij}\\). We’ll assume a multiplicative model for these noise measurements:\n\\[\n\\hat{d}_{ij} = \\frac{\\eta_i}{\\eta_j}\\cdot e^{\\sigma \\epsilon_{ij}},\n\\] where \\(e^{\\sigma \\epsilon_{ij}}\\) is a positive noise term with standard deviation \\(\\sigma\\) on the log-scale. Now define \\(Y_{ij} = \\log \\hat{d}_{ij}\\) and \\(\\beta_i = \\log \\eta_i\\). Observe that \\(\\beta_3 = 0\\) by assumption. Now take logarimths on both sides of the equation above to get\n\\[\nY_{ij} = \\beta_i - \\beta_j + \\sigma\\epsilon_{ij},\n\\]\nwhich is a linear regression model. It looks like a two-way analysis of variance, but isn’t quite that, as we are only dealing with one factor here (the evaluated research) which appears twice in each equation. That said, the only difficulty in estimating this model is to make a model matrix for the regression coefficients. Observe that the residual standard deviation is fixed across items. We’ll take a look at how to reasonably relax this later on.\n\n\nIncidence matrices\nThe questions Gavin answered in the table above can be understood as a directed graph; I’ll call it the question graph. Figure 1 below contains Gavin’s question graph.\n\n\nPlotting question graph for Gavin.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nsource <- as.numeric(factor(gavin$source, levels = levels))\ntarget <- as.numeric(factor(gavin$target, levels = levels))\ngraph <- igraph::graph_from_edgelist(cbind(source, target))\nplot(graph)\n\n\n\n\n\nFigure 1: Question graph for Gavin.\n\n\n\n\nDirected graphs can be defined by their incidence matrices. If \\(G\\) is a directed graph with \\(k\\) nodes and \\(n\\) edges its incidence matrix \\(B\\) is the \\(n\\times k\\) matrix with elements \\[B_{ij}=\\begin{cases}\n-1 & \\text{if edge }e_{j}\\text{ leaves vertex }v_{i},\\\\\n1 & \\text{if edge }e_{j}\\text{ enters vertex }v_{i},\\\\\n0 & \\text{otherwise.}\n\\end{cases}\\]\nTable 3 contains Gavin’s incidence matrix.\n\n\nCalculation of incidence matrix for Gavin.\nn <- nrow(gavin)\nk <- 15\nb <- matrix(data = 0, nrow = n, ncol = k)\n\nfor (i in seq(n)) {\n  b[i, source[i]] <- -1\n  b[i, target[i]] <- 1\n}\n\nknitr::kable(t(b))\n\n\n\n\nTable 3: Incidence matrix for Gavin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n-1\n0\n0\n0\n\n\n0\n0\n0\n-1\n0\n1\n1\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n-1\n0\n0\n0\n0\n1\n1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n-1\n-1\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n0\n-1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-1\n0\n0\n0\n0\n0\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n-1\n0\n0\n0\n-1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nNow we can verify that \\[Y = B^T\\beta + \\sigma \\epsilon.\\] But there is one more thing to handle: We need to remove the fixed \\(\\beta\\), in our case \\(\\beta_3\\), to estimate the model. Define \\(B_\\star\\) and \\(\\beta_\\star\\) as the incidence matrix and coefficient vector with the fixed item removed. Then \\(Y = B_\\star^T\\beta_\\star + \\sigma \\epsilon\\) is ready to be estimated using linear regression.\n\n\nExample\nWe fit a linear regression to Gavin’s data. Table 4 contains the resulting estimates on the log-scale, rounded to the nearest whole number.\n\n\nParameter estimates for Gavin.\nmod <- pairwise_model(gavin, fixed = 3, keep_names = FALSE)\nvals <- round(c(coef(mod)[1:2], q3 = 0, coef(mod)[3:14]))\nknitr::kable(t(vals))\n\n\n\n\nTable 4: Parameter estimates for Gavin.\n\n\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nq9\nq10\nq11\nq12\nq13\nq14\nq15\n\n\n\n\n-12\n-3\n0\n-10\n-8\n-5\n-7\n-4\n7\n3\n8\n-3\n1\n-4\n-6\n\n\n\n\n\n\nWe can also make confidence intervals for the questions using the confint function. Figure 2 plots confidence intervals for all the \\(\\beta\\)s along with their estimates \\(\\hat{\\beta}\\).\n\n\nPlot of parametes and error bars.\nexped = exp(confint(mod))\nconfints = rbind(exped[1:2, ], c(1, 1), exped[3:14, ])\nrownames(confints) <- 1:15\nparams <- setNames(c(coef(mod)[1:2], 1, coef(mod)[3:14]), 1:15)\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 2: Plot of parametes and error bars for Gavin.\n\n\n\n\nThe \\(95\\%\\) confidence intervals are approximately equally wide on the log-scale, with the exception of question 3, which is fixed to \\(1\\). Let’s take a look at question 11, that of Shannon’s A Mathematical Theory of Communication. The confidence interval is\n\nconfints[11, ]\n\n      2.5 %      97.5 % \n   135.4981 150222.4654 \n\n\nThat’s really wide!\n\n\nAll the raters\nThe raters have IDs given in this table.\n\nx <- setNames(1:6, names(data_list))\nknitr::kable(t(x))\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nWe fit the model for all the raters and plot the resulting estimates in Figure 3. Notice the log-scale.\n\n\nFunction for plotting results for all raters.\nparameters = sapply(\n  data_list,\n  \\(data) {\n    coefs <- exp(coef(pairwise_model(data)))\n    c(coefs[1:2], 1, coefs[3:14])\n  })\nmatplot(parameters, log = \"y\", type = \"b\", ylab = \"Values\")\n\n\n\n\n\nFigure 3: Plot of parametes estimates for all raters.\n\n\n\n\nIt seems that the raters agree quite a bit."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "href": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring agreement",
    "text": "Measuring agreement\nOne of the easiest and most popular ways to measure agreement among two raters is Lin’s concordance coefficient (aka quadratically weighted Cohen’s kappa). It has an unpublished multirater generalization \\[\\frac{1^{T}\\Sigma1-\\text{tr}\\Sigma}{(R-1)\\text{tr}\\Sigma+R^{2}\\left(\\overline{\\mu^{2}}-\\overline{\\mu}^{2}\\right)}\\] Where \\(\\Sigma\\) is the covariance matrix of the estimated log rating, \\(\\mu_i\\) is the mean log rating by the \\(i\\)th rater, and \\(R\\) is the number of raters. I can explain the reasoning behind this measure in more detail if you want, but it’s the essentially unique extension of Lin’s concordance coefficient to multiple raters, as several generalizations yield the same formula. It’s bounded above by \\(1\\), which signifies perfect agreement. It’s defined in a way that’s very similar to the \\(R^2\\), so it’s OK to interpret the numbers as you would have interpreted an \\(R^2\\).\n\n\nCalculate concordance of the parameters.\nconcordance = function(x) {\n  n = nrow(x)\n  r = ncol(x)\n  sigma = cov(x) * (n - 1) / n\n  mu = colMeans(x)\n  trace = sum(diag(sigma))\n  top = sum(sigma) - trace\n  bottom = (r - 1) * trace + r ^ 2 * (mean(mu^2) - mean(mu)^2)\n  top / bottom\n}\nconcordance(log(parameters))\n\n\n[1] 0.698547\n\n\nI’m impressed by the level of agreement among the raters.\nWe can also construct a matrix of pairwise agreements.\n\n\nDefines the concordanc matrix using Lin’s coefficient.\nconcordances <- outer(seq(6), seq(6), Vectorize(\\(i,j) concordance(\n  cbind(log(parameters)[, i], log(parameters)[, j]))))\ncolnames(concordances) <- names(x)\nrownames(concordances) <- names(x)\nconcordances\n\n\n          linch      finn     gavin     jamie     misha     ozzie\nlinch 1.0000000 0.5442018 0.7562977 0.7705449 0.7779404 0.7016672\nfinn  0.5442018 1.0000000 0.4031429 0.5385316 0.4685602 0.5985986\ngavin 0.7562977 0.4031429 1.0000000 0.7237382 0.9106108 0.5996458\njamie 0.7705449 0.5385316 0.7237382 1.0000000 0.8253265 0.8926464\nmisha 0.7779404 0.4685602 0.9106108 0.8253265 1.0000000 0.7804517\nozzie 0.7016672 0.5985986 0.5996458 0.8926464 0.7804517 1.0000000\n\n\nNow we notice, e.g., that (i) Gavin agrees with Misha, (ii) Finn doesn’t agree much with anyone, (iii) Ozzie agrees with Jamie."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "href": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "title": "Estimating value from pairwise comparisons",
    "section": "Identification of the parameters",
    "text": "Identification of the parameters\nThe parameters \\(\\beta_\\star\\) are identified if and only if the question graph is connected. This has the practical consequence that the estimation is guaranteed to work whenever you design the question graph well enough. For instance, you do not need to think about avoiding cycles, having only one question per pair, etc.\nNow, it should be intuitively clear that \\(\\beta_\\star\\) cannot be identified when the graph fails to be connected, as there is no point(s) anchoring the scale of every \\(\\beta\\). Think about it this way. Suppose \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) form a connected component disconnected from \\(\\beta_{3}\\). If \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) satisfy \\(Y=B_{[1,2,4]}^{T}\\beta_{[1,2,4]}+\\sigma\\epsilon\\), where \\([1,2,3]\\) denotes the appropriate indexing, then surely \\(\\gamma_{i}=\\beta_{i}+c\\) does so too for any \\(c\\), as every row of \\(B_\\star^{T}\\beta\\) is a difference \\(\\beta_{i}-\\beta_{j}\\), hence \\(\\gamma_{i}-\\gamma_{j}=\\beta_{i}+c-(\\beta_{j}+c)=\\beta_{i}-\\beta_{j}\\). The other way around is slightly trickier. It’s a theorem of algebraic graph theory that the rank of \\(B\\) equals \\(k-c\\), where \\(c\\) is the number of connected components. Suppose the graph is connected, so that the rank of \\(B\\) is \\(k-1\\). Since \\(B\\) does not have full rank (i.e., \\(k\\)), every row can be written as a linear combination of two other rows. In particular, the row associated with the fixed element can be removed without affecting the rank, hence the rank of \\(B_\\star\\) is \\(k-1\\) too. But there are \\(k-1\\) rows in \\(B_\\star\\), hence \\(B_\\star\\) has full rank. It follows that the parameters are identified."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "href": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring inconsistency",
    "text": "Measuring inconsistency\nRecall the multiplicative equation for the reported distance: \\[D_{ij} = \\frac{X_i}{X_j}\\cdot e^{\\sigma \\epsilon_{ij}}\\] It’s clear that the distance will be noise-free if and only if \\(\\sigma = 0\\). Moreover, the distances will behave more and more erratically the larger \\(\\sigma\\) gets. If the distances tend to have erratically, the valuations will be inconsistent. Thus it’s natural to consider inconsistency estimators that are strictly increasing functions of \\(\\sigma\\). We’ll just use \\(\\sigma\\) for simplicity’s sake.\nThe consistencies of our 6 player are\n\n\nDefines the consistencies of all raters.\nconsistencies = lapply(data_list, \\(data) summary(pairwise_model(data))$sigma)\nknitr::kable(tibble::as_tibble(consistencies), digits = 2)\n\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n0.87\n1.04\n2.22\n0.86\n0.87\n0.93\n\n\n\n\n\nAll of these are roughly the same, except Gavin’s. That might be surprising since Nuño claimed Gavin is the most consistent of the raters. His inconsistency score is probably unfavourable since he has some serious outliers in his ratings, not because he’s inconsistent across the board. Ratings \\(33\\) and \\(36\\) appear to be especially inconsistent.\n\nplot(mod, which = 1, main = \"Plot for Gavin\")\n\n\n\n\nCompared it to the same plot for Jaime Sevilla.\n\nplot(pairwise_model(jamie), which = 1, main = \"Plot for Jaime\")\n\n\n\n\nLet’s see what happens if we remove the observations \\(31, 33, 34, 36\\) from Gavin’s data then.\n\ngavin_ <- gavin[setdiff(seq(nrow(gavin)), c(31, 33, 34, 36)), ]\nplot(pairwise_model(gavin_), which = 1, main = \"Plot for Gavin with outliers removed\")\n\n\n\n\nThe residual plot looks better now, and the inconsistency score becomes \\(\\sigma \\approx 0.87\\), in line with the other participants.\nMy take-away is that it would be beneficial to use robust linear regressions when estimating \\(\\beta\\). I’m not prioritizing studying this right now, but if someone were to invest serious amount of time in studying and applying statistical methods for this problem, I would strongly suggest taking a look at e.g. rlm.\n\nYou shouldn’t strive for consistency\nStriving for consistency requires you to follow a method. For instance, you can write down or try hard to remember what you have answered on previous questions, then use the right formula to deduce a consistent answer. I would advice against doing this though. When you compare two items against each other, just follow the priming of the shown items and let the statistical method do its work! If you’re trying hard to be consistent you’ll probably introduce some sort of bias, as you’ll essentially make the ratings dependent on their ordering. Also see the crowd within. The value-elicitation framework is similar to psychometrics, where you want every measurement to be as independent of every other measurement as possible when you condition on the latent variables.\nI also see little reason to use algorithms that prohibits cyclical comparisons, as there is no statistical reason to avoid them. (Only a psychological one, if you feel like you have to be consistent.) It’s also fine the ask the same question more than once – at least if you add some addition correlation term into the model. And have some time distance between the questions."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#aggregation",
    "href": "posts/value-estimation/value-estimation.html#aggregation",
    "title": "Estimating value from pairwise comparisons",
    "section": "Aggregation",
    "text": "Aggregation\nWe estimate \\(\\beta\\) using a mixed effect model. \\[\\begin{eqnarray*}\nY_{j} & = & D_{j}^{T}\\beta_{j}+\\sigma\\epsilon,\\\\\n\\beta_{j} & \\sim & N(\\beta,\\Sigma).\n\\end{eqnarray*}\\]\nConceptually, this model implies that there is a true underlying \\(\\beta\\) for each question, but the raters only have incomplete access to it when they form their subjective valuation. So we have two sources of noise: First, the raters have a latent, noisy and subjective estimate of \\(\\beta\\), which we call \\(\\beta_j\\). Second, we only observe noisy measurements of \\(\\beta_j\\)s through our pairwise comparisons model. The matrix \\(\\Sigma\\) can be constrained to be diagonal, which makes estimation go faster.\nUsing lme4, I made a function pairwise_mixed_model that fits a mixed effects model to the data without an intercept. Check out the source if you want to know exactly what I’ve done.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3)\n\nboundary (singular) fit: see help('isSingular')\n\n\nUsing the mod object, we can plot (Figure 4) confidence intervals and estimates for the aggregate ratings.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[16:29, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 4: Confidence bars mixed effects model with uncorrelated random effects.\n\n\n\n\nThe confidence intervals in the plot are reasonably sized, but remember the \\(y\\)-axis is on the log-scale. Let’s take a look at the confidence interval for A Mathematical Theory of Communication again:\n\n\nConfidence interval for Mathematical Theory of Communication with uncorrelated random effects.\nround(exp(confint(mod, method = \"Wald\")[16:29, ])[10, ])\n\n\n 2.5 % 97.5 % \n    44  10740 \n\n\nThe uncertainty of the aggregate value is smaller than that of Gavin’s subjective value. But the uncertainty is still very, very large. I think the level of uncertainty is wrong though. Fixing it would probably require a modification of the model to allow for items of different difficulty, or maybe a non-multiplicative error structure. But there is also a counterfactual aspect here. It’s hard to say how quickly someone else would’ve invented information theory weren’t it for A Mathematical Theory. Different “concepts” about counterfactuals could potentially lead to different true \\(\\beta\\)s, as some readers consider them and some don’t. (See Linch’s comment)\n\nUsing correlated random effects\nWe can run a model with correlated random effects too.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3, uncorrelated = FALSE)\n\nboundary (singular) fit: see help('isSingular')\n\n\nThe corresponding plot (Figure 5) is similar but not indistinguishable from Figure 4. The confidence intervals appear to smaller, but I don’t know if this is an estimation artifact or not. I also don’t know which model is better. Or if it will ever matter in practice which you use, but the uncorrelated model is faster to fit, as the correlated takes a couple of seconds. And that might matter in a production setting.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[(120 - 13):120, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x =  1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 5: Confidence bars mixed effects model with correlated random effects.\n\n\n\n\nThe resulting confidence interval for A Mathematical Theory of Communication gets smaller now.\n\n\nConfidence interval for Mathematical Theory of Communication with correlated random effects.\nround(exp(confint(mod, method = \"Wald\")[116, ]))\n\n\n 2.5 % 97.5 % \n    68   9602"
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "href": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "title": "Estimating value from pairwise comparisons",
    "section": "Incorporating uncertainty",
    "text": "Incorporating uncertainty\nInstead of rating the ratio \\(\\eta_i/\\eta_j\\) with a number, you might want give a distribution over \\(\\eta_i/\\eta_j\\), indicating your uncertainty, as done in e.g. this post. How could we work with such uncertain measurements? One possibility is to extract a log-mean and log-standard deviation from the distributions and then use the same method as I’ve described, but with weighted least squares instead of least squares. The weights will be \\(1/\\sigma_{ij}\\), where \\(\\sigma_{ij}\\) are the log-standard deviations of the distributions.\nThe formal reasoning behind this proposal goes as follows. If \\(\\eta_i/\\eta_j\\) is log-normal with some log-mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\), its logarithm is normal with mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\). This model implies that \\[\\frac{\\eta_i}{\\eta_j} = e^{\\mu_{ij}}e^{\\sigma_{ij}\\cdot\\epsilon_{ij}},\\] for some normally distributed \\(\\epsilon_{ij}\\). Taking logarithms on both sides, we obtain $$\n\\[\\beta_ - \\beta_j = \\mu_{ij} + \\sigma_{ij}\\epsilon_{ij}.\\] Since \\(\\epsilon_{ij}\\) is symmetric around \\(0\\), we have \\(\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{ij}\\epsilon_{ij}\\). If we add the “safety measure constant” \\(\\psi\\), with \\[\\mu_{ij} = \\beta_i - \\beta_j \\psi\\sigma_{ij}\\epsilon_{ij}, \\tag{1}\\] this is a weighted least squares problem, with weights equal to \\(1/\\sigma_{ij}\\) and residual standard deviation \\(\\psi\\). It’s not clear to me if distributions are worth it.\nIn the summary I wrote that it’s not clear to me that it’s worth it to ask for distributions in pairwise comparisons, as your uncertainty level can be modeled implicitly by comparing different pairwise comparisons. What does this mean? Let’s simplify the model in Equation 1 so it contains two error terms, \\(\\sigma_i\\) and \\(\\sigma_j\\), one belonging to each question.\n\\[\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{i}\\epsilon_{i}+\\sigma_{j}\\epsilon_{j}. \\tag{2}\\]\nThis models allows you to have different uncertainties for different items, but doesn’t allow for idiosyncratic errors depending on interactions between the \\(i\\)th and \\(j\\)th items. Estimation of the model in Equation 2 easy to do efficiently, but I haven’t looked at the details. An idea would be to use the equation \\(\\operatorname{Var}(\\mu_{ij})=\\sigma_{i}^{2}+\\sigma_{j}^{2}\\) and iteratively refit weighted least squares models."
  }
]