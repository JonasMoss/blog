[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Jonas Moss, a statistics post doc at BI Norwegian Business School. I’m interested in programming, the replication crisis in psychology, theoretical statistics, forecasting, effective altruism, and rationalism (as in Lesswrong)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonas Moss' blog",
    "section": "",
    "text": "An errors-in-variables model\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nMatching quantiles to the Kumaraswamy distribution\n\n\n\n\n\n\n\neffective altruism\n\n\nfermi estimates\n\n\nstatistics\n\n\nforecasting\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nSome tips for the master thesis\n\n\n\n\n\n\n\nstatistics\n\n\neducation\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nCorrelation with two lines!\n\n\n\n\n\n\n\nstatistics\n\n\njoking\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nDeriving distributions from quantiles\n\n\n\n\n\n\n\neffective altruism\n\n\nstatistics\n\n\nfermi estimates\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nA peek at pairwise preference estimation in economics, marketing, and statistics\n\n\n\n\n\n\n\neffective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\neconomics\n\n\nmarketing\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nInference for correlations corrected for attenuation\n\n\n\n\n\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nEstimating value from pairwise comparisons\n\n\n\n\n\n\n\neffective altruism\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nMonolithic education\n\n\n\n\n\n\n\nteaching\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nJonas Moss\n\n\n\n\n\n\n\n\nNo one would have invented coefficient alpha today\n\n\n\n\n\n\n\nstatistics\n\n\npsychometrics\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nJonas Moss\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html",
    "href": "posts/alpha-and-omega/alpha-and-omega.html",
    "title": "No one would have invented coefficient alpha today",
    "section": "",
    "text": "Coefficient alpha is the most famous coefficient in psychometrics – Cronbach’s paper Coefficient alpha and the internal structure of tests has been cited around \\(60,00\\) times after all. It’s supposed to measure reliability. What does that mean? Intuitively, a psychometric scale is supposed to measure some kind of psychological construct, such as intelligence, in a reliable way. You don’t want it to be noisy. You don’t want two intelligence tests administered at slightly different times to give widely different results. You also want the test to actually measure intelligence, and not something else, such emotionality. But that’s validity, not reliability.\nIn classical test theory they think about psychometric tests in the context of a true score \\(T\\) and observed score \\(X\\). Then they write \\(X = T +\\epsilon\\) for some error term \\(\\epsilon\\). (This is always possible). For instance, if \\(X\\) is the observed score on an IQ test, \\(T\\) is the true, underlying intelligence.\nNow we’re ready to define classical reliability! The reliability is defined as \\(R=\\text{Cor}(X, T)^2\\), the squared correlation between the true score and the observed score. That’s a pretty reasonable definition! But the problem is that you don’t know the true score, so you can’t estimate this correlation directly.\nAnd that’s where coefficient alpha comes in. Suppose that \\(X\\) is a sum score, i.e., on the form \\(X = X_1 + X_2 + \\ldots + X_k\\). Then assume that \\[X_i = \\mu_i + \\lambda_i T + \\sigma_i \\epsilon_i \\tag{1}\\] for some error terms with variance equal to \\(1\\). In addition, assume that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) when \\(i = j\\), i.e., the error terms are uncorrelated. Then \\(X\\) follows the congeneric measurement model. Now define coefficient alpha as \\[\\alpha=\\frac{k}{k-1}\\left(1-\\frac{\\text{tr}\\Sigma}{{1}^{T}\\Sigma{1}}\\right),\\] where \\(\\Sigma\\) is the covariane matrix of the \\(X_i\\)s. This alpha has two important properties:\nThe condition of \\(\\tau\\)-equivalence means that all the factor loadings \\(\\lambda_i\\) are equal, i.e., the congeneric model eq. 1 is reduced to \\[X_i = \\mu_i + \\lambda T + \\sigma_i \\epsilon_i. \\tag{2}\\] There is widespread agreement that \\(\\tau\\)-equivalence never holds.\nAlpha is also the mean of all possible split-half reliabilities, but no one seem to care much about that today. It might have been important earlier on though. For more details see (Cho 2021)."
  },
  {
    "objectID": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "href": "posts/alpha-and-omega/alpha-and-omega.html#why-no-one-would-have-invented-alpha-today",
    "title": "No one would have invented coefficient alpha today",
    "section": "Why no one would have invented alpha today",
    "text": "Why no one would have invented alpha today\nTake a look at the congeneric model (eq. 1) again. This is a linear one-factor model with no correlation among the errors. In the early 20th century, there was no feasible way to estimate such models, at least not for ordinary psychometric researchers. But that’s not the case anymore. Anyone able to install R and run the easiest lavaan script can estimate such a model. For example, we can estimate the parameters for the agreeableness part of the psychTools::bfi data.\n\nlibrary(\"lavaan\")\nmodel <- \" f =~ A1 + A2 + A3 + A4 + A5 \"\nbfi <- psychTools::bfi[, 1:5]\nbfi[, 1] <- -bfi[, 1] # Reverse-coded question.\nobj <- lavaan::cfa(model, data = bfi, std.lv = TRUE)\nknitr::kable(round(coef(obj), 3))\n\n\n\n\n\nx\n\n\n\n\nf=~A1\n0.528\n\n\nf=~A2\n0.774\n\n\nf=~A3\n0.994\n\n\nf=~A4\n0.717\n\n\nf=~A5\n0.791\n\n\nA1~~A1\n1.693\n\n\nA2~~A2\n0.784\n\n\nA3~~A3\n0.714\n\n\nA4~~A4\n1.694\n\n\nA5~~A5\n0.965\n\n\n\n\n\nThe std.lv = TRUE argument forces the latent variable \\(T\\) to have variance equal to \\(1\\). This option makes it easier to interpret the remaining parameters. Using the estimated parameters of the lavaan object we can estimate the reliabiltiy \\(R\\) with little problems. Straight-forward calculations show that, when we assume the congeneric measurement model, \\[R = \\text{Cor}^{2}(X_1 + X_2+\\cdots+X_k,T)=\\frac{k\\overline{\\lambda}^{2}}{k\\overline{\\lambda}^{2}+\\overline{\\sigma^{2}}}, \\tag{3}\\] where \\(\\overline{x}\\) denotes the mean of the vector \\(x\\).\nUsing this formulation of the reliability in terms of \\(\\lambda\\) and \\(\\sigma\\), the natural estimator if the reliability is the plug-in estimator \\[\\hat{R} = \\frac{k\\overline{\\hat{\\lambda}}^{2}}{k\\overline{\\hat{\\lambda}}^{2}+\\overline{\\hat{\\sigma}^{2}}},\\] where \\(\\hat{\\lambda}\\) and \\(\\hat{\\sigma}\\) are estimators of your choice. If you’re using lavaan, you would calculate it using something like the following function.\n\n#' Estimate the reliability from a `lavaan` object.\n#' \n#' Estimate the reliability assuming a congeneric measurement model using the\n#'   plug-in estimator.\n#' @param obj A `lavaan` object.\n#' @return The estimated reliability coefficient.\nreliability <- function(obj) {\n  params <- lavaan::lavInspect(obj, what = \"coef\")\n  lambda <- params$lambda\n  sigma2 <- diag(params$theta)\n  k <- length(lambda)\n  k * mean(lambda) ^ 2 / (k * mean(lambda) ^ 2 + mean(sigma2))\n}\n\nFor the agreeableness data of psychTools::bfi, our reliability becomes\n\nreliability(obj)\n\n[1] 0.712129\n\n\nIt’s not hard to do inference for the reliability coefficient either – it is merely an application of the delta method. Moreover, by the invariance principle of maximum likelihood estimation, it is the maximum likelihood estimator of the reliability provided \\(\\lambda\\) and \\(\\sigma\\) are estimated using maximum likelihood. In this sense the estimator is natural.\nTo sum up:\n\nThere exists a natural estimator of the reliability under the congeneric measurement model.\nIt is easy to estimate and it’s efficient under normality.\nIts asymptotic theory is well-understood. That’s just a by-product of the immense amount of research on asymptotic theory for general structural equation models.\n\nI doubt anyone would have wanted to investigate alternatives to this 100% reasonable, simple, and efficient estimator. For why would they? It’s like finding an alternative estimator of the \\(R^2\\) that requires a little less computation power but is only consistent for the \\(R^2\\) under extraordinarily unlikely assumptions.\nMaybe someone would have uncovered coefficient alpha today, but its discovery would be treated as a novelty, not something of widespread importance, worth \\(60,000\\) citations."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html",
    "href": "posts/attenuation-survey/attenuation-survey.html",
    "title": "Inference for correlations corrected for attenuation",
    "section": "",
    "text": "You have two psychometric instruments, \\(\\hat{Z_1}\\) and \\(\\hat{Z_2}\\), measuring the true scores \\(Z_1\\) and \\(Z_2\\) with error. The estimators are linear in \\(Z_1,Z_2\\) with independent error terms, i.e. \\(\\hat{Z_1} = Z_1 + \\epsilon_1\\) and \\(\\hat{Z_2} = Z_2 + \\epsilon_2\\). You only observe the correlation between the measurements \\(\\hat{Z_1}\\) and \\(\\hat{Z_2}\\), but you’re interested in the correlation between the true scores \\(Z_1\\) and \\(Z_2\\). What should you do? The Spearman (Spearman 1904) attenuation formula states that \\[\\operatorname{Cor}(Z_1, Z_2) = \\frac{\\operatorname{Cor}(\\hat{Z_1}, \\hat{Z_2})}{\\operatorname{Cor}(Z_1,\\hat{Z_1})\\operatorname{Cor}(Z_2,\\hat{Z_2})}\\]A lot has been written about correction for attenuation. For instance, many people care about the easily verifiable and veritable horror that the sample disattenuated correlationmay be greater than \\(1\\)! But there’s not a lot written much about inference. This is a very short review of what I’ve read.\n\n\nThe Spearman attenuation formula does a *double correction* as it’s a formula for the correlation between both true scores \\(Z_1\\) and \\(Z_2\\). But it’s also possible to calculate the correlation between an estimator \\(\\hat{Z}_1\\) and the true score \\(Z_2\\). Then the single correction formula states\n\\[\\operatorname{Cor}(\\hat{Z}_1, Z_2) = \\frac{\\operatorname{Cor}(\\hat{Z_1}, \\hat{Z_2})}{\\operatorname{Cor}(Z_2,\\hat{Z_2}).} \\tag{1}\\] motivation for using the single correction is simple enough. The correlation between two variables quantifies how well you can predict one from the other. If you want to predict the true score, say intelligence, from an estimated score of openness, the single prediction formula should be used (Guilford 1954, 401; cited in Muchinsky 1996)."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#inference",
    "href": "posts/attenuation-survey/attenuation-survey.html#inference",
    "title": "Inference for correlations corrected for attenuation",
    "section": "Inference",
    "text": "Inference\nThere are some factors that make inference difficult:\n\nDifferent sample sizes. The sample sizes \\(n_1,n_2,n_3\\) for the two reliabilities and the correlation in the numerator can be different. This should be taken into account when constructing confidence intervals.\nWhich reliability method was used? Different reliability methods will not have the same asymptotic variance. This becomes clear when reading the ancient literature on this topic, as people still used split-half reliabilities at the time of writing of, e.g., Forsyth and Feldt (1969).\nWhat kind of model assumptions can we make? Simple asymptotics for the most commonly used reliability coefficient, coefficient alpha, is only available for pseudo-elliptically distributed variables under the parallel model (Yuan and Bentler 2002). But in order to use that, we must know the common kurtosis coefficient in addition to the value of alpha, and that is never reported. The asymptotically distribution-free interval is consistent in general, but is almost never used, but we could in principle deduce the asymptotic variance of the reliability estimator from a common asymptotic normality based interval if we wanted to. If we assume multivariate normality and the parallel model, the asymptotics of coefficient alpha is really simple though, depending only on the value of coefficient alpha itself (Zyl, Neudecker, and Nel 2000), and that’s probably the most reasonable thing to do in practice.\n\nThe most widely used inference method is the Hunter–Schmidt method (Schmidt and Hunter 1999), which ignores the errors in the reliability estimates, i.e., the estimators of \\(\\operatorname{Cor}(Z_1,\\hat{Z_1})\\) and \\(\\operatorname{Cor}(Z_2,\\hat{Z_2})\\). This method performs pretty well, at least when the sample size is sufficiently large. But it’s very crude. And come on – this is not such a hard problem that it can’t be properly solved, taking the variability of the reliabilities into account!"
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-hakstian1988-gz-paper",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-hakstian1988-gz-paper",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The Hakstian, Schroeder, and Rogers (1988) paper",
    "text": "The Hakstian, Schroeder, and Rogers (1988) paper\nThe authors take a look at an estimator that looks slightly different from what I am used to. Citing Rogers (1976), they employ the formula\n\\[\\hat{\\rho}(Z_{1},Z_{2})=\\frac{\\frac{1}{2}\\left[\\hat{\\rho}(\\hat{Z}_{1}^{1},\\hat{Z}_{2}^{1})+\\hat{\\rho}(\\hat{Z}_{1}^{2},\\hat{Z}_{2}^{2})\\right]}{\\hat{\\rho}(Z_{1},\\hat{Z}_{1})\\hat{\\rho}(Z_{2},\\hat{Z}_{2})} \\tag{2}\\] where \\(\\hat{\\rho}\\) denotes an estimator of the correlation. As before, the correlations in the denominator (i.e., the roots of the reliabilities) can’t be estimated directly, but the correlations in the numerator can.\nThe estimator of the correlation is a mean of two independently obtained estimators of the correlation. Which is fair enough, provided one has two samples with equally many participants in both… Which never happens! They do not mention the problem of different \\(n_1\\) and \\(n_2\\). Honestly, I don’t think there is much to gain from this paper. The paper of Rogers (1976) is similar; not much to pick up."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-charles2005-ze-paper",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-charles2005-ze-paper",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The Charles (2005) paper",
    "text": "The Charles (2005) paper\nThis paper has a decent literature overview, but its technical contributions are not very strong, It’s founded on a misconception:\n\nBy their original conception, confidence intervals give bounds for sample values likely to be produced by a population with known parameters (Neyman, 1934/1967), and I believe this is what past attempts at creating confidence intervals for [correction for attenuation due to measurement error] have been.\n\nNow this is just wrong. A confidence interval for a parameter \\(\\theta\\) is a random set \\(C\\) so that \\(P_\\theta(\\theta\\in C)\\geq 1-\\alpha\\). It’s not a bound for sample value likely to produced by population with known parameters."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#the-moss2019-kb-preprint",
    "href": "posts/attenuation-survey/attenuation-survey.html#the-moss2019-kb-preprint",
    "title": "Inference for correlations corrected for attenuation",
    "section": "The (Moss 2019) preprint",
    "text": "The (Moss 2019) preprint\nI wrote a note about correcting for attenuation in 2017. It uses an unconventional method to construct the confidence sets which is guaranteed to have coverage larger than the nominal (modulo uniformity requirements), but it’s extremely conservative and basically useless."
  },
  {
    "objectID": "posts/attenuation-survey/attenuation-survey.html#what-now",
    "href": "posts/attenuation-survey/attenuation-survey.html#what-now",
    "title": "Inference for correlations corrected for attenuation",
    "section": "What now?",
    "text": "What now?\nWe can use the multivariate central limit theorem to deduce the limiting distribution of three independent measurements, the correlation, reliability 1, and reliability 2, even when their sample sizes are different. (I didn’t realize that when I wrote my preprint above.) Then we can use the delta method along with desired transformations, such as the Fisher transform, to make reasonable confidence sets."
  },
  {
    "objectID": "posts/errors-in-variables/index.html",
    "href": "posts/errors-in-variables/index.html",
    "title": "An errors-in-variables model",
    "section": "",
    "text": "Suppose we wish to estimate the regression coefficient for\n\\[\nY_{0}\\mid X_{0}=\\alpha_{0}+\\beta_{0}X_{0}+\\sigma_{0}\\epsilon_{0},\\label{eq:original}\n\\] where \\(E[\\epsilon\\mid X_{0}]=0\\).\nHowever, we do not observe \\(Y_{0}\\) and \\(X_{0}\\). Instead, we observe \\[\nY_{1}=Y_{0}+S_{Y}\\epsilon_{Y}\n\\] for some \\(\\delta\\) with \\(E[\\delta\\mid Y_{0}]=0\\) for some random variable \\(S\\) and \\[\nX_{1}=X_{0}+S_{X}\\epsilon_{X},\n\\] for some \\(\\eta\\) with \\(E[\\delta\\mid Y_{0}]=0\\).\nAs is well known, the regression coefficient \\(\\beta_{0}=\\frac{\\text{Cov}(X_{1},Y_{1})}{\\text{Var} X_{1}}\\). However, \\(\\text{Cov}(Y_{1},X_{1})\\) equals \\(\\text{Cov}(Y_{0},X_{0})\\). About \\(\\text{Var} X_{1}\\), we can employ the Law of Total Variance \\[\n\\text{Var} X_{1}=E\\text{Var}(X\\mid S_{X})+\\text{Var} E(X_{0}\\mid S_{X}).\n\\] The term \\(\\text{Var} E(X_{0}\\mid S_{X})\\) vanishes, as \\(E(X_{0}\\mid S_{X})\\) is constant. Moreover, \\[\n\\text{Var} X_{1}=\\text{Var} X_{0}+\\text{Var} S_{X},\n\\] hence \\[\\begin{equation}\n\\text{Var} X_{0}=\\text{Var} X_{1}-\\text{Var} S_{X}.\\label{eq:adjusted variance}\n\\end{equation}\\]\nDefine the regression model \\[\nY_{1}=\\alpha_{1}+\\beta_{1}X_{1}+(S_{Y}\\delta+\\sigma_{0}\\eta)\n\\]\nIt follows that \\[\n\\beta_{0}=\\frac{\\text{Cov}(Y_{1},X_{1})}{\\text{Var} X_{1}-\\text{Var} S_{X}}=\\beta_{1}\\frac{\\text{Var} X_{1}}{\\text{Var} X_{1}-\\text{Var} S_{X}},\n\\] Moreover, \\[\\begin{eqnarray*}\n\\alpha_{0} & = & EY_{0}-\\beta_{0}EX_{0},\\\\\n& = & EY_{1}-\\beta_{1}\\frac{\\text{Var} X_{1}}{\\text{Var} X_{1}-\\text{Var} S_{X}}EX_{1}.\n\\end{eqnarray*}\\] If \\(EX_{1}\\) has been normalized to \\(0\\), then \\(\\alpha_{0}=EY_{1}=\\alpha_{1}\\).\nNotice that \\(Y_{1}\\) has known errors. This makes it – perhaps – possible to estimate \\(\\beta_{1}\\) with additional precision, using something similar to weighted least squares. The weights would be \\(\\sqrt{S_{Y}^{2}+\\sigma^{2}}\\). However, as \\(\\sigma^{2}\\) is unknown, the resulting regression would not truly be weighted least squares."
  },
  {
    "objectID": "posts/errors-in-variables/index.html#verification",
    "href": "posts/errors-in-variables/index.html#verification",
    "title": "An errors-in-variables model",
    "section": "Verification",
    "text": "Verification\nLet’s simulate a bunch of values from the model.\n\nn = 1000000\ns_x = sqrt(3)*rexp(n)\ns_y = 3*rexp(n)\ns_0 = 1\n\nx_0 = rnorm(n, 1, 2)\nx_1 = x_0 + s_x * rnorm(n)\ny_0 = 0.8 + 0.5 * x_0 + s_0*rnorm(n)\ny_1 = y_0 + s_y * rnorm(n)\n\nThe calculated coefficients are\n\nbeta0_hat = cov(y_1, x_1)/(var(x_1) - 2*var(s_x))\nalpha0_hat = mean(y_1) - beta0_hat * mean(x_1)\nc(alpha0_hat, beta0_hat)\n\n[1] 0.7964205 0.4951465\n\n\nBut the naive regression \\(Y_1 \\sim \\alpha_1 + \\beta_1X_1\\) yields\n\nlm(y_1 ~ x_1)\n\n\nCall:\nlm(formula = y_1 ~ x_1)\n\nCoefficients:\n(Intercept)          x_1  \n     1.0927       0.1988  \n\n\nOn the other hand, the correct (but unobserved) regression yields\n\nlm(y_0~x_0)\n\n\nCall:\nlm(formula = y_0 ~ x_0)\n\nCoefficients:\n(Intercept)          x_0  \n     0.7987       0.4999"
  },
  {
    "objectID": "posts/errors-in-variables/index.html#inference-and-literature",
    "href": "posts/errors-in-variables/index.html#inference-and-literature",
    "title": "An errors-in-variables model",
    "section": "Inference and literature",
    "text": "Inference and literature\nTo do inference on this method, use the delta method and large-sample theory (together with the studentized bootstrap), or perhaps the bias-corrected accelerated bootstrap (BCa). The delta method should be fairly easy to derive using the formulation of the “covariance of the covariance” foundin e.g. Magnus and Neudecker’s Matrix differential calculus.\nThere is a sizable literature on error-in-variable models, and inference for this simple model has probably been worked out, but a very rudimentary search yielded nothing for me. I think it’s uncommon to know the variances of the \\(X\\) errors. Moreover, the problem can probably be cast in the language of structrual equation models. But I’m unsure if software (such as lavaan) will help, because you don’t know the item variances in a typical application of structural equations models.\nA final option is to assume bivariate normality and use maximum likelihood. This is also likely to be possible using an R package, but I’m not sure the estimates would be consistent. Probably you’d have to use a sandwich matrix for correct standard errors.\nTo make things easy on yourself, if you’re faced with a problem of this kind, I would suggest just going with the BCa + the equations above. The equations are trivial to compute and BCa will be fairly simple as well; it might be possible to calculate using packages such as bootstrap. Do something else only if the reviewers demand it."
  },
  {
    "objectID": "posts/kumaraswamy/kumaraswamy.html",
    "href": "posts/kumaraswamy/kumaraswamy.html",
    "title": "Matching quantiles to the Kumaraswamy distribution",
    "section": "",
    "text": "Sometimes, in particular when doing informal forecasting, you would like to match quantiles to distributions. For real data, the most common choice is the normal distribution. For positive data with “multiplicative qualities”, so to speak, the most common choice is the log-normal, but the truncated normal can also be reasonable. But what should you do when dealing with data on the \\([0,1]\\)?\nDistributions of (transformed) location-scale families are easy to match quantiles to, involving only algebraic manipulations with the cumulative distribution function. And there are infinitely many transformed location-scale families living on \\([0,1]\\): For any location-scale family distribution \\(X\\), such as the normal, and cumulative distribution function \\(G\\) (on \\(\\mathbb{R}\\)), you can define the transformed family \\(G(\\mu + \\sigma X)\\), living on \\([0,1]\\). Then you can then match quantiles to the original family by mapping \\(x\\to G^{-1}(x)\\) and \\(y\\to G^{-1}(y)\\). The most natural candidate here is, perhaps, the logit-normal. These distributions sometimes have poor looks, see e.g., the plot in the linked article, but could be an excellent choice.\nAside from transformed location-scale families, the most obvious way would be to match quantiles using the beta distribution. But the beta distribution has a troublesome cumulative distribution function that is not available in every programming language. Moreover, for the beta distribution, matching quantiles is not a simple algebraic manipulation. Instead, you need to do numerical optimization. This increases the complexity of the procedure, both in terms of resources consumed and reliability, as it is hard to make sure the numerical optimization works all the time.\nIn this post I describe how to efficiently match quantiles for the Kumaraswamy distribution. The Kumaraswamy distribution is visually indistinguishable from the Beta distribution, but is sometimes easier to work with mathematically. In particular, the cumulative distribution function of a Kumaraswamy distribution with parameters \\(a,b\\) is \\(F(x;a,b) = 1-(1-x^a)^b\\). Compare this to the Beta distribution, whose cumulative distribution function is the incomplete regularized beta function - the difference in complexity is striking. The Kumaraswamy distribution has some downsides though, most notably that the rôles of \\(a\\) and \\(b\\) aren’t symmetric. In other words, so you can’t be sure that the Kumaraswamy with equal \\(a\\) and \\(b\\) parameters is symmetric. Moreover, it’s not a conjugate distribution for the binomial, and the expectations doesn’t have as simple closed form as the beta distribution.\nBut if your goal is to match quantiles to a general purpose and reasonable-looking distribution, then look no further!"
  },
  {
    "objectID": "posts/kumaraswamy/kumaraswamy.html#doing-the-matching",
    "href": "posts/kumaraswamy/kumaraswamy.html#doing-the-matching",
    "title": "Matching quantiles to the Kumaraswamy distribution",
    "section": "Doing the matching",
    "text": "Doing the matching\nWe’ll do the matching using Newton–Raphson, but first we’ll cover the following steps:\n\nReduce the number of equations. By finding an expression for \\(b\\) in terms of \\(a\\), we can reduce the problem from a two-dimensional Newton–Raphson to a one-dimensional Newton–Raphson.\nDo a rough analysis of the objective function. A rough analysis of the objective function suggests a very natural starting value.\nDo Taylor expansions to prevent numerical instability. The objective function and its derivative are sometimes poorly behaved. We use Taylor expansions to prevent numerical instability.\n\nThe resulting is a function matchkumar(x,y,alpha,beta) that is both fast and reasonably reliable.\n\n1. Reduce the number of equations\nThe cumulative distribution function is \\(F(x)=1-(1-x^{a})^{b}\\). Suppose we want to match the quantiles \\(\\alpha\\) and \\(\\beta\\) to \\(x\\) and \\(y\\). That’s equivalent to \\[\\begin{eqnarray*}\nF(x_{0}) & = & \\alpha=1-(1-x^{a})^{b},\\\\\nF(x_{1}) & = & \\beta=1-(1-y^{a})^{b}.\n\\end{eqnarray*}\\] Now we need to solve for \\(a,b\\). To do this, we’ll first solve for \\(b\\), yielding \\[\\begin{eqnarray*}\n1-\\alpha & = & (1-x^{a})^{b},\\\\\n\\log(1-\\alpha) & = & b\\log(1-x^{a}).\n\\end{eqnarray*}\\] Writing \\(b\\) as a function of \\(a\\), we find \\[\nb(a)=\\frac{\\log(1-\\alpha)}{\\log(1-x^{a})}.\n\\] Now solve for \\(a\\) and find \\[\na(b)=\\frac{\\log\\left(1-(1-\\beta)^{1/b}\\right)}{\\log(y)}.\n\\] Plug in the definition of \\(b(a)\\) into the definition of \\(a(b)\\) to find \\[\na=\\frac{\\log\\left(1-(1-\\beta)^{\\frac{\\log(1-x^{a})}{\\log(1-\\alpha)}}\\right)}{\\log(y)}\n\\] Our goal is to solve this equation for \\(a\\), then plug the result into \\(b(a)\\) to find \\(b\\). But we can’t solve for \\(b\\) analytically.\nTo do this, let’s define \\[\n\\delta=(1-\\beta)^{1/\\log(1-\\alpha)},\\quad g(a)=\\delta^{\\log(1-x^{a})},\n\\] and we need to solve \\[\nf(a)=a\\log y-\\log(1-g(a))=0.\n\\] The derivative of this one is \\[\nf'(a)=\\log y-\\log(x)\\log\\delta\\frac{x^{a}g(a)}{(1-x^{a})(1-g(a))}.\n\\]\n\n\n2. Function analysis\nThe function \\(f(a)\\) has an oblique asymptote as \\(a\\to\\infty\\). Using Taylor expansions, one may verify that its slope is \\(\\log(y/x)\\) and its intercept \\(-\\log\\log\\delta.\\) In other words, \\(\\lim_{a\\to\\infty}f(a)-h(a)=0\\), where \\(h(a)=-\\log\\log\\delta+\\log(y/x)a\\). From what I can see, \\(h(a)\\) approximates \\(f(a)\\) from below. This is handy, as it is guaranteed that \\(f(a_{0})>0\\), where \\(a_{0}=\\log\\log\\delta/\\log(y/x)\\) is the solution to \\(h(a)=0\\). This will typically provide an excellent starting point.\n\n\n3. Taylor approximations\nThe function \\(g(a)\\) is behaves poorly numerically when \\(x^a\\) becomes small, as it gets to close to \\(1\\) and promotes numerical instability. To fix this, we use Taylor approximations for \\(f\\) and \\(f'\\) when \\(x^a\\) is sufficiently small.\nThe first-order Taylor expansion of \\(\\log(1-\\delta^{\\log(1-x^a)})\\) (around \\(x^a=0\\)), used in \\(f(a)\\), is \\[\na\\log x+\\log\\log\\delta+\\frac{1}{2}x^{a}(1-\\log\\delta)+O(x^{a})\n\\]\nThe first-order Taylor expansion of \\(f'(x^a)\\) as a function of \\(x^a\\) is \\[\\log y - \\log x \\cdot \\frac{1}{2}(1+\\log\\delta)x^a+O(x^{2a})\\] We will use these approximations when \\(x^a<4\\cdot10^{-6}\\).\nIn addition, we will employ several other tricks to ensure numerical stability."
  },
  {
    "objectID": "posts/kumaraswamy/kumaraswamy.html#code-for-matchkumar",
    "href": "posts/kumaraswamy/kumaraswamy.html#code-for-matchkumar",
    "title": "Matching quantiles to the Kumaraswamy distribution",
    "section": "Code for matchkumar",
    "text": "Code for matchkumar\nBelow is R code for matchkumar. The function does not use any R-specific functionality, so it should be easily portable to any language of your choosing, such as JavaScript, Rust, or Crystal. The vectorization used in the functions f and f_deriv below is not strictly necessary, as they are only called using scalar arguments.\n\n\nCode for matchkumar\n#' Match quantiles for the Kumaraswamy distributon.\n#'\n#' @param alpha,beta Upper and lower quantiles to match.\n#' @param x,y Values at those quantiles.\n#' @param eps,iter_lim Used in the Newton-Raphson algorithm.\n#' @param bound When x^a is smaller than this bound, use first-order\n#'    Taylor approximation.\n#' @return The  a and b parameters for the Kumaraswamy distribution so that\n#'  extraDistr::qkumar(alpha, a, b) = x and\n#'  extraDistr::qkumar(beta, a, b) = y\n\nmatchkumar <- \\(alpha, beta, x, y, eps = 1e-04, iter_lim = 100, bound = 4e-6) {\n  log_y <- log(y)\n  log_x <- log(x)\n  g <- \\(x_a) (1 - beta)^(log(1 - x_a) / log(1 - alpha))\n  log_delta <- log(1 - beta) / log(1 - alpha)\n\n  f <- \\(a) {\n    f_taylor <- \\(a) {\n      log_y * a - (a * log(x) + log(log_delta) + 0.5 * x^a * (1 - log_delta))\n    }\n\n    f_not_taylor <- \\(a) {\n      log_y * a - log(1 - g(x^a))\n    }\n\n    vec <- (x^a < bound)\n    out <- rep(0, length(a))\n    out[vec] <- f_taylor(a[vec])\n    out[!vec] <- f_not_taylor(a[!vec])\n    out\n  }\n\n  f_deriv <- \\(a) {\n    f_deriv_not_taylor <- \\(a) {\n      x_a <- x^a\n      delta_xa <- g(x_a)\n      log_y - log_x * log_delta * x_a * delta_xa / ((1 - x_a) * (1 - delta_xa))\n    }\n\n    f_deriv_taylor <- \\(a) {\n      x_a <- x^a\n      -log_x + log_y + 0.5 * (log_delta - 1) * log_x * x_a\n    }\n\n    vec <- (x^a < bound)\n    out <- rep(0, length(a))\n    out[vec] <- f_deriv_taylor(a[vec])\n    out[!vec] <- f_deriv_not_taylor(a[!vec])\n    out\n  }\n\n  minimum <- .Machine$double.eps\n\n  a_to_b <- \\(a) {\n    x_a <- x^a\n    attempt <- log(1 - alpha) / log(1 - x_a)\n    result <- if (is.infinite(attempt)) {\n      log(1 - alpha) * (-1 / x_a + 1 / 2 + x_a / 12 + x_a^2 / 24)\n    } else {\n      attempt\n    }\n    max(result, minimum)\n  }\n\n  a0 <- log(log_delta) / log(y / x)\n\n  for (i in seq(iter_lim)) {\n    a1_ <- a0 - f(a0) / f_deriv(a0)\n    a1 <- if (is.na(a1_)) minimum else max(a1_, minimum)\n    rel_error <- (a0 - a1) / a1\n    if (abs(rel_error) < eps | a0 == minimum) break\n    a0 <- a1\n  }\n\n  result <- c(a = a0, b = a_to_b(a0))\n  attr(result, \"iter\") <- i\n  result\n}"
  },
  {
    "objectID": "posts/kumaraswamy/kumaraswamy.html#example",
    "href": "posts/kumaraswamy/kumaraswamy.html#example",
    "title": "Matching quantiles to the Kumaraswamy distribution",
    "section": "Example",
    "text": "Example\nLet’s see how the function performs on an example:\n\nx <- 0.55\ny <- 0.6\nalpha <- 0.1\nbeta <- 0.90\nmatched <- matchkumar(alpha, beta, x, y)\nresults <- c(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n\nNow we see that the results are as intended:\n\nresults\n\nx_matched y_matched \n     0.55      0.60 \n\n\nWe typically do not need a large number of iterations; in this case it’s just 1!\n\nattr(matched, \"iter\")\n\n[1] 1\n\n\nHere’s a plot of the resulting density."
  },
  {
    "objectID": "posts/kumaraswamy/kumaraswamy.html#performance",
    "href": "posts/kumaraswamy/kumaraswamy.html#performance",
    "title": "Matching quantiles to the Kumaraswamy distribution",
    "section": "Performance",
    "text": "Performance\nThe example above is not sufficient to prove the method works. To verify a solution works, let’s check the squared distance between the implied quantiles and the desired quantiles.\n\nverify <- \\(args) {\n  x <- args[1]\n  y <- args[2]\n  alpha <- args[3]\n  beta <- args[4]\n  matched <- matchkumar(alpha, beta, x, y)\n  x_matched <- extraDistr::qkumar(alpha, matched[1], matched[2])\n  y_matched <- extraDistr::qkumar(beta, matched[1], matched[2])\n  (x - x_matched)^2 + (y - y_matched)^2\n}\n\nGenerating a bunch of random values here.\n\nset.seed(313)\nn <- 1000\nargs <- matrix(replicate(n, c(sort(runif(2)), sort(runif(2)))),\n               ncol = 4, byrow = TRUE)\nresults <- apply(args, 1, verify)\nplot(results, log = \"y\", ylab = \"Error\", xlab = \"Index\")\n\n\n\n\n\nSources of problems\nThe function works very well for most randomly generated values, but not for absolutely all of them. Sadly, I don’t think it’s possible to solve this problem without using arbitrary precision numbers. For the problem lies not in the calculation of \\(a\\) itself, which works very well, but in the calculation of \\(b\\). For some values of \\(a\\) we are not able to calculate the value of \\(b\\) sufficiently precisely using floating point arithmetic. Moreover, extraDistr::qkumar is not able to handle these extreme inputs. I strongly doubt this is would be a problem in practice, as the random values exhibiting these problems are quite unrealistic, and can be avoided altogether by scaling the Kumaraswamy distribution.\nConsider the following.\n\nx = 0.6503457\ny = 0.6656772\nalpha = 0.2139453\nbeta = 0.894129\n\nHere \\(x\\) and \\(y\\) are very close, but \\(\\alpha\\) and \\(\\beta\\) far apart, which forces a large amount of mass in a very small space. In these cases \\(a\\) appears to be calculated to great accuracy.\n\nmatched = matchkumar(alpha, beta, x, y)\nmatched \n\n           a            b \n9.583478e+01 1.944493e+17 \nattr(,\"iter\")\n[1] 1\n\n\nBut the exceedingly large \\(a,b\\)s cannot be handled by the extraDistr::qkumar.\n\nc(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n\nx_matched y_matched \n        0         0 \n\n\nAnother sort of trouble-maker occurs when \\(a\\) is too close to \\(0\\), i.e., closer than the machine epsilon.\n\nx = 0.1383447\ny = 0.794063\nalpha = 0.9214318\nbeta = 0.9266641\n\nIt’s not a surprise that these values are hard to fit, as the \\(x,y\\) values are far apart but \\(\\alpha\\) and \\(\\beta\\) very, very, close. In this case, matchkumar yields\n\nmatched = matchkumar(alpha,beta,x,y)\nmatched\n\n           a            b \n2.220446e-16 7.195903e-02 \nattr(,\"iter\")\n[1] 11\n\n\nBut the matched quantiles are not as close as we would like.\n\nc(x_matched = extraDistr::qkumar(alpha, matched[1], matched[2]),\n  y_matched = extraDistr::qkumar(beta, matched[1], matched[2]))\n\nx_matched y_matched \n0.1353353 0.3678794 \n\n\nBut as the Kumaraswamy distribution is parameterized, we can’t get the \\(a\\) much closer to 0 than this.\n\n\nNumber of iterations\nMost of the time, the number of iterations (capped at \\(100\\)) is small. However, the cap is sometimes reached. In most cases, this is not cause for alarm. This is because we evaluate the solutions using the distance from the desired quantiles, but they are evaluated using the relative error in the Newton–Raphson loop. And the first will often be small even if the second is “large”. Moreover, the large numbers of iterations happens when \\(a\\) is very, very small, which is unlikely to happen in applications."
  },
  {
    "objectID": "posts/monolithic-education/monolithic-education.html",
    "href": "posts/monolithic-education/monolithic-education.html",
    "title": "Monolithic education",
    "section": "",
    "text": "A course coordinator at the university has too many responsibilities. Instead of doing one thing, hopefully well, he must do four.\n\nCurriculum. What should you teach the students? It’s not possible for a non-expert to know what’s important for him to learn; so an expert or group of expert is necessary here. But that person is not typically the same as the one who’s best suited to teach.\nTeaching system. How should the students learn whatever it is you want to teach? With lectures, guided exercise solution, class discussion, or something else? Maybe you should go for something along the lines Bikini Calculus.\nTeaching material. Someone has to make the teaching material. And what should that look like? No matter what format you choose, this is a whole lot of work.\nEvaluation. How should it be tested? All year round or just at a certain time and place? Should the students pay a fee for the exam? Can SRS be used somehow? Multiple choice?\n\nThese four pillars are not separated enough in practice. Even though they are clearly distinct and require different talents and different interests.\nIn high schools these components are, at least partly, at least in Norway, handled by different people. The curriculum is designed by the government. The teaching material is usually books, perhaps 2-3 to choose from for each subject. Evaluation is handled by the government. So you can, as a teacher, spend all your time teaching.\nIn universities, on the other hand, one professor is often tasked with doing everything. Design the curriculum. Figure out the teaching system. Perhaps even develop his own teaching materials – at the very least his own slides. And, of course, he must make his own exams.\n\nOn curriculum development\nI’m placing curriculum at the top of the list as it is both the most important and the most neglected. How often do you see students call for better curricula? If you ignore calls for anti-colonialization, probably never. But this is where you find the greatest potential for improvement. You’ll often find that introductory math and engineering courses, for instance, follow a pattern that was well-established even back in the 80s. They involve giving lip service to intuitions and proofs, focussing the entirety of the curriculum on gaining enough practice with a couple of calculation techniques required to pass the exam. And why is that? Probably because someone has to make the exam, and the more “examy” stuff the students know, the easier it is to make one.\nLet’s take a classical calculus course for example. A calculus course will often teach you how to solve integrals using partial fraction expansions. That is probably because it is a simple technique that does help you solve, by hand, a larger class of integrals than what you would have been able to had you not knowm it, thus expanding the pool of possible exercises to give the students at the exam. But does it actually help the student?\nPartial fraction expansion is not a “deep” part of integration, such as substitution and integration by parts. You need to learn these as they are essential both for conceptual understanding and most proofs. But partical fraction expansion is not. The student can use WolframAlpha when he’s calculating integrals on his own.\nDesigning a curriculum should involve carefully picking out the parts of a subject that are most important to understand and master in order to achieve a set of goals. The goals of calculus are, roughly, (i) to build mathematical maturity, (ii) to be comfortable with what limits, derivatives and integrals are, e.g., develop an inuitive understanding of why they are linear, (iii) understand their basic applications in optimization problems and physics, and then, (iv) to gain an intuition of what integrals are easy to calculate and when that matters.\nI’ve only spend 5 minutes thinking about these goals, and I’m open to counters. But calculus should not be about doing the maximal amount of integral calculations, competing in how fast you can use the chain rule, and so on."
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html",
    "href": "posts/peek-pairwise/peek-pairwise.html",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "",
    "text": "I had a peek at value estimation in economics and marketing. There is a sizable literature here, and more work is needed to figure out what exactly is relevant for effective altruists. Discrete choice models are applied a lot in economics, but these models are not able to estimate the scaling of the values. Marketing researchers prefer graded pairwise comparisons, which is equivalent to the pairwise method used here, but with limits on how much you can prefer one choice to another.\nI’m enthusiastic about the prospects of doing larger-scale paired comparison studies on EA topics. The first step would be to finish the statistical framework I started on here, then do a small-scale study suitable for a methodological journal in e.g. psychology or economics. Then we could run a study on a larger scale.\nMost examples I’ve seen in health economics, environmental economics, and marketing are only tangentially related to effective altruism. (I don’t claim they don’t exist – there’s probably many studies in health economics relevant to EA). But the topics of cognitive burden and experimental design is relevant for anyone who’s involved with value estimation. It would be good to have at least a medium effort report on these topics – I would certainly appreciate it! The literature probably contains a good deal of valuable insights for those sufficiently able and motivated to trudge through it.\nThere is a reasonable number of statistical papers on the graded comparisons. But mostly from the \\(50\\)s - \\(70\\)s. These will be very difficult to read unless you’re at the level of a capable master student of statistics. But summarizing and extending their research could potentially be an effective thesis!"
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html#context",
    "href": "posts/peek-pairwise/peek-pairwise.html#context",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "Context",
    "text": "Context\nIn my earlier post Estimating value from pairwise comparisons I wrote about a reasonable statistical model for the pairwise comparison experiments that Nuño Sempere at QURI have been doing (see also his sequence on estimating value). While writing that post I started thinking about fields where utility extraction is important, and decided to take a look at health economics and environmental economics. This post is a write-up of my attempt at a light survey of the literature on this topic, with particular attention paid on pairwise experiments.\nWhat do I mean by pairwise comparisons? Suppose I ask you “Do you prefer to lose your arm or your leg?” That’s a binary pairwise comparison between the two outcomes \\(A\\), \\(B\\), where \\(A = \\text{lose a leg}\\) and \\(B=\\text{lose an arm}\\). Such comparison studies are truly wide-spread, going back at least to Mcfadden (1973), which has \\(23000\\) Google Scholar citations! Models such as these are called discrete choice models, and I will refer to them as dichotomous (binary) comparisons as well, which is terminology I’ve seen in the economics literature. These models cannot measure the scale of the preferences though, only their ordering. There are many reasons why we care about the scale of preferences/utilities. For instance, we need scaling to compare preferences between different studies, and we need scales when we face uncertainty, as part of expected utility theory.\nTo take scale into account we can ask questions such as “How many times worse would it be to lose an arm than losing a leg?”. Then you might answer, say, \\(1/10\\), so you think losing a leg is ten times worse than losing an arm. Or \\(10\\), so you think losing an arm is ten times worse than losing a leg. These questions are harder than the corresponding binary questions though, and I can image respondents being flabbergasted by them. Questions of this kind are called graded (or ratio) comparisons in the literature. The idea is old – it goes way back to Thurstone (1927)!\nWe can mix graded and binary comparisons using stock standard maximum likelihood theory or Bayesian statistics. I haven’t figured out the exact conditions, but assuming we have enough graded question, we will be able to fix the scale reasonably well and gain information through the binary part only, reducing the cognitive load of the participants.\nI’m excited about the prospect of using pairwise comparisons on a large scale. Here are some applications:\n\nEstimate the value of research, both in the context of academia and effective altruism. This post presents a small-scale experiment in the EA context. It would be interesting to do a similar experiment inside of academia. Probably more rigorous and lengthy though. In my experience many academics do not feel that their or other people’s work is important. They research whatever is publishable since it’s their job. Attempting to quantify researchers understanding of the value of their and other people’s research could at least potentially push some researchers into a more effective direction.\nEstimating the value of EA projects. This should be pretty obvious. One of the potentials of the pairwise value estimation method is crowd-sourcing – since it’s so easy to say “I prefer \\(A\\) to \\(B\\)”, or perhaps “\\(A\\) is \\(10\\) times better than \\(B\\)” – the bar for participation is likely to be lower than, say, participating in Metaculus, which is a real hassle. Possible applications would be crowd-sourcing of valuation of small projects, e.g. something like Relative Impact of the First 10 EA Forum Prize Winners.\nDescriptive ethics. You could estimate moral weights for various species. You could get an understanding about how people vary in the their moral valuations. You could run experiments akin to the experiments underlying moral foundations theory, but with a much more quantitative flavor. I haven’t thought deeply about it, but I imagine studies of this sort would be important in the context of moral uncertainty.\n\nMoreover, I’m thinking about making pairwise estimation into an academic medium-sized project of its own. Very roughly, I’m thinking two steps would have to be completed.\n\nDevelopment of reasonable statistical techniques. The statistics aren’t that hard, it’s basically elementary techniques such as linear regression and Probit regression. Combined too! But it could be challenging to find optimal allocations of questions. This is the part where I, as a statistician, shine. Of course, it’s important to familiarize oneself with the literature on the topic one wishes to work on. Hence this post.\nDesign studies to see how well pairwise value estimation works. Is it worth bothering with it at all? Or maybe just in a couple of contexts? A possible course of action would be to conduct serious interviews with some professors about what research is valuable and why, then go around and estimate the pairwise model on PhD students. It would be nice to have a problem with both a ground truth and incentives to perform – sports prediction might be a better place to start. I don’t shine at stuff like this and would need help."
  },
  {
    "objectID": "posts/peek-pairwise/peek-pairwise.html#small-literature-survey",
    "href": "posts/peek-pairwise/peek-pairwise.html#small-literature-survey",
    "title": "A peek at pairwise preference estimation in economics, marketing, and statistics",
    "section": "Small literature survey",
    "text": "Small literature survey\nI’ve spent a couple of hours surfing through the literature on choice modeling, estimation of value, and so on. I followed no methodology in choosing which papers to write about.\n\nGraded paired comparisons in statistics\nA Google Scholar search reveals plenty of statistical papers written in the 50s-70s, including a paper by the great statistician Scheffé (1952), who studies problems on the form\n\nIn a 7-point scoring system the judge presented with the ordered pair \\((i,j)\\) makes one of the following 7 statements:\n\n(3) I prefer \\(i\\) to \\(j\\) strongly.\n(2) I prefer \\(i\\) to \\(j\\) moderately.\n(1) I prefer \\(i\\) to \\(j\\) slightly.\n(0) No preference.\n(-1) I prefer \\(j\\) to \\(i\\) slightly.\n(-2) I prefer\\(j\\) to \\(i\\) moderately.\n(-3) I prefer \\(j\\) to \\(i\\) strongly.\n\n\nObserve that these numbers correspond roughly to taking logs of ratios, as I did in my previous post. He proceeds to analyze the problem in essentially the same way as I did, but he uses another “contrast” (in the notation of the post linked above, I force \\(\\beta_i\\) = \\(1\\) for some fixed \\(i\\), but he makes \\(\\sum \\beta_i = 0\\) instead; this should be familiar if you have taken a course in linear regression that included categorical covariates). But he also adds a test for what he calls “subtractivity”, i.e., the assumption that there is no interaction term involved in ratios. I’m very ready to just assume this, however. I don’t understand why he needs to restrict the possible values to \\(\\{-3,-2,-1,0,1,2,3\\}\\) though. It just doesn’t seem to matter, so I suppose it’s for he presentation’s sake.\nThere has been done plenty of methodological work on these methods in statistics. Sufficiently capable and interestedd methodologists should probably look at the literature and see what insights it contains. This will be hard though, as the literature is old, hard to read, and probably written in an unnecessarily convoluted way (at least judging from Scheffé’s paper) with lots of sums of squares and similar horrors.\n\n\nMarketing research\nGraded paired comparison studies are popular in marketing research for evaluating brand preference, but they are presented in a slightly different way: Now you have a fixed number of points to divide between two options, usually \\(9\\), I think. If you have \\(3\\) points to distribute, it would be formally equivalent to Scheffé’s setup above, but with the potential benefit that there is no implicit order of comparison. I think Scholz, Meissner, and Decker (2010) might be a decent entry-point to the marketing literature. Consider the following example from De Beuckelaer, Toonen, and Davidov (2013):\n\nPlease consider all presented bottled water brands [Spa, Sourcy, Evian, Chaudfontaine, and Vittel] equal in price and content (0.5 L). Which brand do you prefer? Please distribute x preference point/points.” (with x being replaced by one, five, seven, nine, or eleven).\n\nMaybe this is just a bad example, but I honestly can’t see the point of using the points here. Why not just ask which brand of bottled water you prefer? Also, why do you need a fixed number of points. Maybe to prevent respondents from saying “I like Coke \\(1000\\) times more than Pepsi!!!”? Or perhaps to emulate the feeling of having a fixed amount of money to spend?\nMuch of the methodological work in marketing appears to be about “complex” products with several attributes to rate. See e.g. this image from Scholz, Meissner, and Decker (2010), about the different attributes of a phone (display, brand, navigation). Is this relevant to EA? I’d say that it’s very likely to be relevant – we do care about attributes such as scale, neglectedness, and tractability after all.\nGraded paired comparisons are used in psychometrics too. Here they are used to reduce “cheating” in e.g. personality surveys due to social desirability bias, and the fact that you have fixed pool of points to spend matters. Brown and Maydeu-Olivares (2018) and Bürkner (2022) studied the statistical aspects. I can’t see any immediate application of these models for effective altruism though.\n\n\nEconomics\nI have comments on four papers. Baker et al. (2010), Vass, Rigby, and Payne (2017), and Ryan, Watson, and Entwistle (2009) are from health economics and Hanley, Mourato, and Wright (2001) from environmental economics.\n\nThe report of Baker et al. (2010)\nI think this report is about using surveying techniques to figure of if a “QALY is a QALY”, e.g., are some people’s QALYs woth less due to their age? But it’s pretty long and I only looked somewhat closely at their methodology. This report discusses two kinds of studies.\nDiscrete choice studies. (Chapter 4). Recall the definition of this kind of model. Here you choose between \\(A\\) and \\(B\\). You don’t say “\\(A\\) is ten times as good as \\(B\\)!” You just choose one of them. In this paper they use the discrete choice model to estimate the quality of life (QoL) using surveys.\nMatching studies. (Chapter 5). You have two options \\(A\\) and \\(B\\), and you’re asked which you prefer. If you say \\(A\\) , you’ll asked if prefer \\(\\frac{1}{2}A\\) to \\(B\\). We continue modifying \\(A\\) and \\(B\\) until you don’t prefer one to the other. This seems to be equivalent to graded comparisons, but the report is so verbose I can barely comprehend what they are doing. Their application is a kind of empirical ethics regarding the definition of QALYs.\n\nTo recap, a QALY is 1 year in full health and years spent in less than full health are ‘quality adjusted’ in order to take account of the morbidity associated with disability, disease or illness. As QALYs combine morbidity (QoL) and mortality (length of life) on a single scale, they allow comparisons to be made across interventions with different types of health outcomes (e.g. QoL enhancing versus life extending). In the standard ‘unweighted’ QALY model, all QALYs are of equal value. For example, the age of recipients does not matter, as long as the QALY gain is the same. Likewise, the standard model assumes that equal QALY gains are of equal value regardless of how severely ill the patients are prior to treatment. The aim of the matching – and DCE studies – is then to address the question of whether a QALY is a QALY is a QALY. Or put another way, is the ‘standard’ model correct?\n\nI haven’t read how they actually estimate these modified QALYs though.\n\n\nThe survey of Vass, Rigby, and Payne (2017)\nVass, Rigby, and Payne (2017) discusses the use of qualitative research in discrete choice experiments. This seems like a decent entry point to the literature in health economics, and I will probably rely on it in the future, if only for their reference list. The topic is also plausibly relevant to EAs, as the qualitative aspects of a discrete choice experiment essentially lies in its preparation – find the right questions, ask the right people, and so on.\nThey have a reasonably clear motivation for the use of discrete choice models in medicine.\n\nIn healthcare, decision making may involve careful assessment of the health benefits of an intervention. However, decision makers may wish to go beyond traditional clinical measures and incorporate ‘’non-health’’ values such as those derived from the process of healthcare delivery. DCEs allow for estimation of an individual’s preferences for both health benefits and non-health benefits and can explain the relative value of the different sources.\n\nThey mention, along with many other authors, the problem of cognitive burden.\n\nAny increases in the cognitive burden of the task could result in poorer quality data and should be considered carefully.\n\nAnd that’s the reason why don’t think it’s a good idea to ask respondents distributions, and would prefer to use discrete choice models as much as possible. (For remember that we can mix them with graded response models to keep the scale fixed).\n\n\nThe paper of Ryan, Watson, and Entwistle (2009)\nThis is a paper about how value elicitation experiments often reveal values incompatible with expected utility theory. They also include a discussion of methodologies used.\n\nThe four most commonly applied stated preference methods in health economics are contingent valuation (CV), discrete choice experiments (DCEs), standard gamble (SG), and time trade-off (TTO). Researchers have used quantitative tests to investigate whether responses to tasks set within each of these methods are consistent with the axioms of utility theory.\n\nThey include some examples of these methods in the context of statistical testing.\n\nContingent valuation. Hanley, Wright, and Koop (2002) use the terminology “choice experiment” instead. And when they describe their method it’s plain to see that they actually use a discrete choice model. But Ryan, Watson, and Entwistle (2009) later write that “CV asks respondents to trade the health good and money”. The term is really bad, as “contingent” can mean just about anything without proper context.\nDiscrete choice experiments. “DCEs ask respondents to trade between attributes.”\nStandard gamble and time trade-off. “SG and TTO test whether respondents make trade-offs between health status and risk or time”.\n\nStatistically speaking, CV, DCE, SG and TTO should be understood as the same thing, the only difference being “what is compared to what”, where DCE compares attributes to attributes and CV compares attributes vs money. Perhaps nice to know if you’re going to dive into the literature on this topic.\n\n\nPaper of Hanley, Mourato, and Wright (2001)\nThe author discuss the “contingent valuation method”, which appears to be a willingness-to-pay experiment (Ahlert, Breyer, and Schwettmann 2013). I don’t think we can make use of that though.\n\nBy means of an appropriately designed questionnaire, a hypothetical market is described where the good or service in question can be traded. […] Respondents are then asked to express their maximum willingness to pay […]\n\nThen they discuss choice modelling.\n\n[Choice modelling] is a family of survey-based methodologies for modelling preferences for goods, where goods are described in terms of their attributes and of the levels these take.\n\nIn Table 2 they describe four kinds of choice modelling:\n\nChoice experiments. This is equivalent to dichotomous comparisons.\nContingent ranking. Rank a bunch of different alternatives, e.g. “What do you prefer? Losing an arm, a leg, or a tooth? Rank from best to worst.”.\nContingent rating. This is direct scoring of values. Hanley, Mourato, and Wright (2001) mentions a \\(10\\) point scale, but we would probably allow them to use \\(\\mathbb{R}^+\\) instead.\nPaired comparisons. This is graded comparisons.\n\nThey claim that option (1) yields “welfare consistent estimates”, but the others probably don’t. I don’t know if this is important or not, as I don’t know what it means.\nAll of these models are possible to combine, but we won’t be able to fix the scale without using graded paired comparisons. Option (2) might possibly be worth investigating, especially if the number of rankings are small, where the additional cognitive burden is low. But Hanley, Mourato, and Wright (2001) states that “choices seem to be unreliable and inconsistent across ranks,” making it less likely that this is worth exploring. The Schulze method is also based on ranks, and could be relevant here."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#summary",
    "href": "posts/quantiles/quantiles.html#summary",
    "title": "Deriving distributions from quantiles",
    "section": "1 Summary",
    "text": "1 Summary\n\nGood methods for eliciting densities from quantiles should satisfy six conditions.\nI propose a class of method for constructing densities for quantiles based on transformations and penalized monotone B-splines. It satisfies 3 or 4 of the conditions with appropriate tweaking.\nSome of the weaknesses of my proposed method may be ameliorated. I make some suggestions about how to do this.\nThere are infinitely many ways to translate quantiles to densities. I sketch another one at the end."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#introduction",
    "href": "posts/quantiles/quantiles.html#introduction",
    "title": "Deriving distributions from quantiles",
    "section": "2 Introduction",
    "text": "2 Introduction\nWhile doing Fermi estimation (“guesstimation”) you often want to construct a distribution from quantile knowledge. Dealing with two quantiles is quite easy, as you have plenty of distribution families that are easy to fit. Location-scale families, such as the normal distribution, logistic distribution, Cauchy distribution, or the shifted exponential distribution are particularly easy to fit. Moreover, a monotonically transformed location-scale distribution is equally easy to work with, e.g. the log-normal and log-logistic distributions.\n\n\n\n\n\n\nSome details on location-scale distributions\n\n\n\n\n\nA variable \\(Y\\) belongs to a location-scale family of distributions if it can be written on the form \\(Y=\\mu+\\sigma X\\). If \\(F\\) is the distribution function \\(X\\), then \\(F\\left(\\frac{y-\\mu}{\\sigma}\\right)\\) is the distribution function of \\(Y\\), as \\[P(Y\\leq y)    =   P(\\mu+\\sigma X\\leq y)\n    =   P(X\\leq(y-\\mu)/\\sigma)\n    =   F\\left(\\frac{y-\\mu}{\\sigma}\\right).\\]\nWe also find that the quantile function of \\(Y\\) equals \\(Q_Y(p)=\\mu+\\sigma Q(p)\\), where \\(Q\\) is the quantile function of \\(X\\).\nWe can derive \\(\\mu\\) and \\(\\sigma\\) by matching quantiles: \\[\\begin{eqnarray*}\nq_{1} & = & \\mu+\\sigma F^{-1}(p_{1}),\\\\\nq_{2} & = & \\mu+\\sigma F^{-1}(p_{2}).\n\\end{eqnarray*}\\]\nIf \\(g\\) is a strictly increasing transformation then \\(Y=g(\\mu + \\sigma X)\\) is a transformed location-scale distribution. You can derive its quantiles in the same way as you would a location-scale distribution, just be sure to apply \\(g^{-1}\\). Usually \\(g=\\exp\\) and \\(X\\) is normal, yielding the log-normal distribution.\n\n\n\nBut what do we do with more than two quantiles? We could fit a \\(k\\)-parameter distribution, but there are few canonical densities to choose from with \\(k>2\\) parameters. And it’s hard to justify your choice in any case. For two (or one) parameters, the normal distribution can often be justified for data on \\([-\\infty, \\infty]\\) based on the central limit theorem; likewise, the log-normal can be justified based on a product argument – and the exponential and half-normal with their own reasoning methods. Three or more though? There’s just no standard way to do it."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#what-would-we-like",
    "href": "posts/quantiles/quantiles.html#what-would-we-like",
    "title": "Deriving distributions from quantiles",
    "section": "3 What would we like?",
    "text": "3 What would we like?\nHow should we go about deciding on a class of distribution to fit quantiles to? Here are 6 conditions I like.\n\nControl of tail behavior. The exact shape of the distributions “in the middle” – where we are likely to have the strongest intuitions – often doesn’t matter too much. If we’re dealing with, say, insurance, having control of the tails is just as important as fine-grained control over how the curve looks in the center.\nGood looks. This is a vague requirement, but can probably be replaced with the almost equally vague requirement of the densities being smooth. Smoothness is not just about aesthetics, as we expect the best predictive densities to be smooth, with continuous second derivatives at the very least. We would also like to avoid multimodality when possible. It’s easy to make densities with bad looks, and severe discontinuities, see e.g. Figure 2. We don’t want those\nPrincipled derivation. We do not want the densities to be ad-hoc but derived using sound principles such as maximum entropy. It’s not trivial to combine this with the good-looks demand, as maximum entropy + quantile information yields discontinuous densities!\nBounded away from \\(0\\). We don’t want out densities to be \\(0\\) at arbitrary places! See Figure 6 for an example of such bad behavior.\nEasy to sample from. Especially relevant for Squiggle applications.\nQuick to calculate, cheap to store, etc. We always want convenience, but it’s hard to say as an outsider what is most important! Quick computation would matter for interfaces used by products such as Metaculus.\n\nHow can you construct densities that match aribtrary quantiles? The most obvious starting points arew mixture distributions and quantile mixtures.\nMixture distributions can be written on the form \\(G(x) = \\sum_i^n \\lambda_i F_i(x, \\theta_i)\\) for a collection of parameterized basis distributions \\(F_i(x;\\theta_i)\\). By making \\(n\\) big enough and \\(F_i(\\theta_i)\\) sufficiently flexible, we can match any quantile to it. The most famous member of this family are normal mixtures. Mixture distributions have been studied a lot. There might be reasonably simple iterative methods to fit mixtures of location-scale distributions, but it seems hard, as the quantile function has no closed form. But different forms of “basis distributions” can be fitted quite easily, at least on \\([0,1]\\). Perhaps best known among these are the Bernstein polynomials (themselves special instances of the Beta distribution), which will fit any distribution arbitrarily well in the limit by the Weierstrass approximation theorem. Another good option is to use splines, which can be constrained to be positive and increasing without too much difficulty.\nQuantile-parameterized distributions have their quantile distributions written on the form \\(Q(x) = \\sum_i^n \\lambda_i q_i(x; \\theta_i)\\) instead. This class of distributions purportedly includes the metalog distribution with its bloated wikipedia page, but in reality they require minor a conceptual modification to fit into the framework, as its component are not bona fide quantile functions. In principle, such “quantile mixtures” can be matched to any quantile using non-negative least squares, provided the basis quantiles are flexible enough. This is entirely feasible to do – provided the domain of the vaiables are bounded on \\([0,1]\\), using them same methods as we would use for fitting cumulative distribution functions. For quantile functions and distributions functions on \\([0,1]\\) are the same thing - increasing and positive functions bounded by \\([0,1]\\), so we have freedom in choosing which to model.\nSince it is possible to fit easily fit quantiles on the unit interval the following strategy, as illustrated in Figure 1, sounds promising:\n\nStart out with a base distribution, such as the log-normal, with distribution function \\(F\\), and density function \\(f\\). Then transform the quantiles \\(q=q_{1},q_{2},\\ldots,q_{k}\\) to the unit interval using \\(F\\); call the transformed quantiles \\(x\\) and the associated probabilities \\(y\\). We can then use these quantiles to construct a distribution \\(S\\) (and associated density \\(s\\)) on the unit interval with the desired quantiles.\nFit the quantiles on the unit interval. We will look at the piecewise linear approximation, monotone interpolating splines, and penalized monotone B-splines. The penalized monotone B-splines is the only promising approach.\nTransform the fitted distribution back, using the quantile function. This yields the density \\(g(x)=f(x)s(F(x))\\), cumulativedistribution function \\(G(x)=S(F(x))\\), and quantile function \\(G^{-1}(x)=F^{-1}(S^{-1}(x))\\).\n\n\n\n\n\n\nflowchart TD\n  B([Quantiles]) --> C\n  A([Base \\n distribution])-->C[Transformed \\n quantiles]\n  D([Method to create\\n distribution on\\n unit interval]) --> E[Distribution on\\n unit interval]\n  C --> E\n  A --> F[Final distribution]\n  E --> F[Final distribution]\n  \n\n\n\n\n\nFigure 1: Diagram of the proposed method. Boxes with rounded edges are input variables. Most of the complexity lies in the method used to create a distribution on the unit interval.\n\n\n\n\nLet’s take a look at the piecewise linear approach and the monotone interpolating splines to see why they are not promising.\n\n3.1 Piecewise linear\nOur base distribution is the log-normal.\n\np <- plnorm; d <- dlnorm; q <- qlnorm\n\nThe quantiles are semi-randomly chosen to be as follows\n\nqq <- c(0, q(0.1), q(0.85), q(0.9), Inf)\ny <- c(0, 0.1, 0.5, 0.9, 1)\nx <- p(qq)\nround(qq, 2)\n\n[1] 0.00 0.28 2.82 3.60  Inf\n\n\nI’ll use the log-normal base distribution and these quantiles throughout this post.\nThe piecewise linear approximation and its associated histogram can be calculated using splines.\n\n\nFunction to the piecewise linear approximation and its associated histogram. The implementation was fast to write, but there are more efficient implementations available.\npiecewise <- function(x, y) {\n  sf <- \\(y,...) splines2::iSpline(\n    y, \n    intercept = TRUE, \n    knots = x[2:(length(x) - 1)],\n    degree = 0,\n    Boundary.knots = c(x[1], x[length(x)]),\n    ...)\n  xmat <- sf(x)\n  coefs <- nnls::nnls(A = xmat, b = y)$x\n  pdf_untrans = \\(w) sf(w, derivs = 1) %*% coefs\n  cdf_untrans = \\(w) sf(w) %*% coefs\n  list(\n    x = x,\n    y = y,\n    pdf = \\(w, d = dunif, p = punif) d(w) * pdf_untrans(p(w)),\n    cdf = \\(w, p = punif) cdf_untrans(p(w))\n  )\n}\n\n\nThe resulting distribution, density, and target density can be seen below.\n\n\nCode for plotting the piecewise linear CDF and PDF on the transformed scale and the PDF on the untransformed scale.\nobj <- piecewise(x, y)\nlayout(matrix(c(1,2,3,3, 3, 3), nrow = 3, ncol = 2, byrow = TRUE), \n       heights = c(3, 2))\nz <- seq(0, 1, by = 0.001)\nplot(z, obj$cdf(z), ylim = c(0, 1), type = \"l\", ylab = \"Cumulative probability\", xlab = \"x\",\n     main = \"Transformed distribution function\")\nabline(h = y, v = x, lty = 3)\nplot(z, obj$pdf(z), type = \"l\", ylab = \"Density\", xlab = \"x\", \n     main = \"Transformed density\")\n\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 2: Plot of a piecewise linear transformed CDF and piecewise densities.\n\n\n\n\nThe target density is discontinuous.\n\n\n3.2 Monotone interpolation splines\nMonotone interpolation splines are used to interpolate a sequence of points with a monotone function. There are two variants implemented in R, the Hyman (1983) splines and the Fritsch–Carlson (1980) splines. Both yield functions with continuous first derivative. Since we’re fitting the cumulative distribution function, this guarantees that the density function is continuous.\n\n\nCode for Hyman splines and Fritsch–Carlson splines\nf1 <- splinefun(x, y, method = \"hyman\")\nf2 <- splinefun(x, y, method = \"monoH.FC\")\nz <- seq(0, 7, by = 0.001)\nplot(z, dlnorm(z) * f1(plnorm(z), deriv = 1), type = \"l\", \n     main = \"Target density with Hyman (black) and Fritsch-Carlson (blue) splines\")\nlines(z, dlnorm(z) * f2(plnorm(z), deriv = 1), type = \"l\", col = \"blue\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 3: Plot of target densities with Hyman splines and Fritsch–Carlson splines.\n\n\n\n\nBoth methods yield ugly target densities, probably because the resulting interpolating splines only have continuous first derivatives, not continuous second derivatives (as most cubic splines have). Moreover, the Hyman method yields a density that fails to be bounded away from \\(0\\). Since there is no obvious way to fix these splines, I’m moving on to another method."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#fitting-monotone-b-splines-with-penalties",
    "href": "posts/quantiles/quantiles.html#fitting-monotone-b-splines-with-penalties",
    "title": "Deriving distributions from quantiles",
    "section": "4 Fitting monotone B-splines with penalties",
    "text": "4 Fitting monotone B-splines with penalties\nRecall that \\(x_i\\) are the transformed quantiles. We need to find and \\(F\\) that satisfies \\(F(x_i) = y_i\\). Since this isn’t enough to identify the \\(F\\), we can try minimize the squared distance between \\(F\\) and the identity function \\(\\iota\\) defined by \\(\\iota(x) = x\\) for some function class \\(\\mathcal{F}\\) that is contained in the set of increasing functions. \\[\\min_{F(x_{i})=y_{i},F\\in\\mathcal{F}}\\|F-\\iota \\|^2 \\tag{1}\\]\nMy choice of \\(\\mathcal{F}\\) is the set of monotone B-splines with \\(m\\) uniformly spaced internal knots and boundary knots at \\(0\\) and \\(1\\). These splines are most likely guaranteed to interpolate your quantiles when \\(m\\) is large enough, but I don’t know the details. The result is a quadratic program with linear constraints; see the code in Section 4.1. (I can add mathematical details upon request.)\nWhen using splines of degree \\(3\\) and \\(m = 40\\) on the quantiles \\(x\\) introduced above, we get the following result.\n\n\n\n\n\nFigure 4: Monotone B-sline density without penalization.\n\n\n\n\nThe second bump looks beautiful and smooth! But the first bump is to irregular, lacking smoothness at its end and going up and down too much. Moreover, the density is \\(0\\) almost all the way from \\(0\\) to \\(3\\), which we typically do not want.\nTo fix the smoothness problems we add a penalty term for the integrated squared second derivative together with a parameter \\(\\lambda\\) regulating the influence of the penalty term. (Just like the \\(\\lambda\\) in ridge regression regression). To fix \\(0\\) problem, we add a penalty term to the integrated squared first derivative, parameterized by \\(\\mu\\). Both of these terms can be added to the quadratic implied by Equation 1.\n\n4.1 The fitter function\nThe R function below fits the splines using the arguments \\(m\\) (number of knots), the degree of the B-splines (degree), \\(\\lambda\\) (the penalty for the second derivative), \\(\\mu\\) (the penalty for the first derivative).\n\n\nThe fitter functions and its helpers.\n#' Fits a density to quantiles.\n#'\n#' The method is based on monotone splines. We minimize the squared distance\n#'    between the spline approximation and the identity function. Thus you\n#'    minimize the distance between the CDF and the uniform CDF in a\n#'    q-transformed space. The parameters `lambda` and `mu` are to tweak the\n#'    resulting density into the desired shape.\n#'\n#' The function returns\n#' @param x,y Vector of probabilities (`x`) and quantiles (`y`). The quantiles\n#'    must have been transformed to the unit interval using your desired `q`.\n#' @param m Number of internal knots in the spline.\n#' @param degree Degree of the spline function. Defaults to `3`, which\n#'   corresponds to cubic splines.\n#' @param lambda The penalty term for the squared second derivative.\n#' @param mu The penalty term for the squared derivative.\n#' @return A list containing the fitted spline together with density functions,\n#'   and so on.\n\nfitter <- function(x, y, m = 20, degree = 3, lambda = 0.5, mu = 0) {\n  knots <- (2:m) / (m + 1)\n  boundary_knots <- c(y[1], y[length(y)])\n  \n  sf <- \\(x, derivs = 0) splines2::bSpline(\n    x,\n    intercept = TRUE,\n    knots = knots,\n    degree = degree,\n    Boundary.knots = boundary_knots,\n    derivs = derivs\n  )\n  \n  matrices <- get_penalties(knots, boundary_knots, degree)\n  x_mat <- sf(x)\n  f <- Vectorize(\\(x, i) x * sf(x)[i])\n  k <- ncol(x_mat)\n  sx <- sapply(seq(k), \\(i) integrate(f, lower = 0, upper = 1, i = i)$value)\n  constraints <- increasing(ncol(x_mat) - 1)\n  \n  fit <- quadprog::solve.QP(\n    Dmat = matrices$s2_mat + lambda * matrices$p_mat + mu * matrices$p1_mat,\n    dvec = sx,\n    Amat = t(rbind(x_mat, constraints$amat)),\n    bvec = c(y, constraints$bvec),\n    meq = length(y)\n  )\n  \n  coefs <- fit$solution\n  pdf_untrans = \\(w) sf(w, derivs = 1) %*% coefs\n  cdf_untrans = \\(w) sf(w) %*% coefs\n  \n  list(\n    x = x,\n    y = y,\n    m = m,\n    degree = degree,\n    lambda = lambda,\n    mu = mu,\n    fit = fit,\n    pdf = \\(w, d = dunif, p = punif) d(w) * pdf_untrans(p(w)),\n    cdf = \\(w, p = punif) cdf_untrans(p(w))\n  )\n}\n\n#' Get penalty matrices for the fitter.\n#' @param knots The internal knots.\n#' @param boundary_knots The boundary knots.\n#' @param degree Degree of the B-spline.\n#' @return List of penalty matrices.\nget_penalties <- function(knots, boundary_knots, degree) {\n  basis_obj <- fda::create.bspline.basis(\n    rangeval = boundary_knots,\n    norder = degree + 1,\n    breaks = c(boundary_knots[1], knots, boundary_knots[2])\n  )\n  inds <- seq(basis_obj$nbasis)\n  list(\n    s2_mat = fda::bsplinepen(basis_obj, Lfdobj = 0)[inds, inds],\n    p_mat = fda::bsplinepen(basis_obj, Lfdobj = 2)[inds, inds],\n    p1_mat = fda::bsplinepen(basis_obj, Lfdobj = 1)[inds, inds]\n  )\n}\n\n#' Increasing constraints matrix and vector\n#'\n#' @keywords internal\n#' @param m The number of knots in the spline.\n#' @param intercept If `TRUE`, the model includes an intercept.\n#' @return The increasing constraint matrix.\n\nincreasing <- function(m, intercept = TRUE) {\n  amat <- cbind(0, -cbind(diag(m - 1), 0) + cbind(0, diag(m - 1)))\n  amat <- rbind(0, amat)\n  amat <- rbind(amat, 0)\n  amat[m + 1, c(1, m + 1)] <- -1\n  amat[1, 2] <- 1\n  amat <- rbind(0, amat)\n  amat[1, 1] <- 1\n  \n  if (!intercept) {\n    amat <- amat[2:nrow(amat), 2:ncol(amat)]\n  }\n  \n  bvec <- rep(0, nrow(amat))\n  bvec[nrow(amat)] <- -1\n  \n  list(\n    amat = amat,\n    bvec = bvec\n  )\n}\n\n\n\n\n4.2 Fixing the density\nUsing \\(m = 40\\), a degree of \\(3\\), \\(\\lambda = 0.01\\), and \\(\\mu = 5\\) fixes Figure 4.\n\n\nConstructing and plotting a monotone B-spline with a good penalization.\nobj <- fitter(x, y, m = 40, degree = 3, lambda = 0.01, mu = 5)\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 5: A good-looking density.\n\n\n\n\nThis example suggests that the penalized monotone B-spline method can be good-looking, avoid the 0 problem, control the tail behavior, and be at least reasonably quick to calculate (as quadratic programs of this small size are fast to solve). I’m not sure how easy they will be to sample from though, and I’m even less sure they have a principled derivation. But their biggest problem is parameter tuning.\nWe can reintroduce the \\(0\\) problem by modifying \\(\\lambda\\). Setting \\(\\lambda = 0.03\\) instead of \\(\\lambda = 0.01\\) gives us the following. It appears that smoothing too much reintroduces the \\(0\\) problem. I’m not sure why though.\n\n\nConstructing and plotting a monotone B-spline with a bad penalization.\nobj <- fitter(x, y, m = 30, degree = 4, lambda = 0.03, mu = 5)\n\nlayout(matrix(c(1,2,3,3,3,3), nrow = 3, ncol = 2, byrow = TRUE))\nz <- seq(0, 1, by = 0.001)\nplot(z, obj$cdf(z), ylim = c(0, 1), type = \"l\", ylab = \"Cumulative probability\", xlab = \"x\",\n     main = \"Transformed distribution function\")\nabline(h = y, v = x, lty = 3)\n\nplot(z, obj$pdf(z), type = \"l\", ylab = \"Density\", xlab = \"x\", \n     main = \"Transformed density\")\n\nw <- seq(0, 7, by = 0.001)\nplot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\", xlab = \"x\",\n     main = \"Target density\")\nabline(v = q(x), lty = 2, col = \"red\")\n\n\n\n\n\nFigure 6: A bad density that’s equal to \\(0\\) at places we don’t want it to when smoothing too much.\n\n\n\n\n\n\n4.3 Changing the base distributions\nThe choice of base distribution has a big effect on the target distribution. Figure 7 shows an example.\n\n\nPlotting many target densities.\nplotter <- function(d, p, q, main) {\n  x <- p(c(0, 1, 2, 3, Inf))\n  y <- c(0, 0.1, 0.5, 0.9, 1)\n  obj <- fitter(x, y, m = 90, degree = 5, lambda = 0.1, mu = 5)\n  w <- seq(0, 5, by = 0.001)\n  plot(w, obj$pdf(w, d, p), type = \"l\", lwd = 2, ylab = \"Density\",\n       main = main)\n  abline(v = q(x), lty = 2, col = \"red\")\n}\n\nlayout(matrix(c(1,2,1,2,3,4,3,4,5,6,5,6), nrow = 6, ncol = 2, byrow = TRUE))\n\nplotter(extraDistr::drayleigh, extraDistr::prayleigh, extraDistr::qrayleigh, main = \"Rayleigh\")\nplotter(dexp, pexp, qexp, main = \"Exponential\")\nplotter(dlnorm, plnorm, qlnorm, main = \"Log-normal\")\nplotter(extraDistr::dhcauchy, extraDistr::phcauchy, extraDistr::qhcauchy, main = \"Half-Cauchy\")\nplotter(extraDistr::dhnorm, extraDistr::phnorm, extraDistr::qhnorm, main = \"Half-Normal\")\nplotter(extraDistr::dfrechet, extraDistr::pfrechet, extraDistr::qfrechet, main = \"Fr\\u{E9}chet\")\n\n\n\n\n\nFigure 7: Target densities based on \\(6\\) base distributions. The degree is \\(5\\), the number of internal knots is \\(m = 90\\), \\(\\lambda = 0.1\\), and \\(\\mu = 5\\).\n\n\n\n\n\n\n4.4 Potential modifications\nIt could be worth it to attempt a modification of the penalties. As currently computed, the penalties, involving the squared integrals \\(\\int [f^{(p)}(x)]^2dx\\), are calculated on the transformed level. They should ideally be calculated on the untransformed level instead, i.e., \\(\\int [g^{(p)}(x)]^2dx\\) in our notation. But this is unlikely to be feasible, at least in a production setting. (Numerical calculations could be possible, but are slow). Maybe it would work to use a finite difference approximations similar to the one used by \\(P\\)-splines instead.\nChange the minimand in Equation 1 to the piecewise linear approximation \\(F_l\\) instead of the identity function. It’s likely that this would remove the problem of the density being equal to \\(0\\) of Figure 1. We could also try to change the minimand something wild, such as the least concave majorant or the greatest convex minorant. The least concave majorant corresponds to the best-fitting increasing density compatible with the supplied quantiles.\nWe can guarantee identical tail behavior for the base distribution and the quantile-transformed distribution by forcing \\(S\\) to be linear in when \\(x\\) is sufficiently close to \\(0\\) or \\(1\\), e.g., \\(\\epsilon\\) away from the edges. That would involve modifying the input data and append lines to the edges of the output function\nIt is possible to enforce shape constraints on the densities, such as being increasing, decreasing, or unimodal. This can be done by adding linear constraints to the quadratic program.\n\n\n4.5 What remains to be done\n\nWe need to implement automatic scaling, i.e., we need to choose the appropriate parameters for the base distribution, probably by matching its parameters to the supplied quantiles. This is probably required for the methods to work in high generality (you can’t use the fitter function (Section 4.1) directly when the numbers are too large, e.g., choosing quantiles of several hundreds in Figure 7 won’t work).\nAutomatic selection of the tuning parameters \\(\\lambda\\), \\(\\mu\\), the number of knots \\(m\\), and the degree of the splines.\nInvert the function \\(S\\). We need the quantile function to generate random samples.\nInvestigate some of the proposals in the previous section."
  },
  {
    "objectID": "posts/quantiles/quantiles.html#fit-splines-without-transformations",
    "href": "posts/quantiles/quantiles.html#fit-splines-without-transformations",
    "title": "Deriving distributions from quantiles",
    "section": "5 Fit splines without transformations",
    "text": "5 Fit splines without transformations\nYou can fit splines without transformations too. This would involve choosing the form of the tails, then match the derivatives of the splines to the derivatives of the tails, on either side. This approach lets us enforce good looks on the right scale, and could work much better than the transform approach. But it should be somewhat harder to implement, as the derivative matching requires some manual work. The approach is also quite principled; it can be framed as an approximation to the maximum entropy solution to the elicitation problem when the tails are known with smoothness penalties."
  },
  {
    "objectID": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html",
    "href": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html",
    "title": "Some tips for the master thesis",
    "section": "",
    "text": "This is a collection of general tips for my master students in business analytics and data science. It’s far from exhaustive. In particular, it contains no tips about how to produce impressive content for a thesis, i.e., actually doing the research.\ntl;dr Write for your examiners, or, people who want to finish reading your thesis as quickly as possible. Read the grading guidelines. Make sure to handle references correctly."
  },
  {
    "objectID": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#venerate-your-examiners",
    "href": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#venerate-your-examiners",
    "title": "Some tips for the master thesis",
    "section": "Venerate your examiners",
    "text": "Venerate your examiners\nThe examiners responsible for evaluating your thesis are not interested in reading the entire document. They are likely to skim through some pages, skip others, and carefully read only a few. When reviewing your thesis, examiners will be looking for shallow signals that suggest the quality of your work. Some things they may consider include:\n\nWriting quality: Examiners will pay attention to the structure, grammar, and organization of your thesis. You can use tools like Wordtune or Grammarly to improve your writing. You should aim for a classic prose style.\nProfessional appearance: It’s important that your thesis looks polished and ready to be published. Consider using Latex or the WYSIWYG editor Lyx to give your work a professional look. Examiners may be slightly negative towards theses written in Word. Make sure your tables and figures are clear, non-redundant, and labeled appropriately. You can write an excellent master thesis and have it misunderstood because your figures are unreadable or you use different symbols for the same quantity in different sections.\nQuality of the basics: Examiners will closely scrutinize the parts of your thesis they are most familiar with, such as sections on portfolio optimization, logistic regression, linear regression, and the interpretation of the \\(AUC\\). Make sure these sections are perfect.\nReference list: Many reviewers will look at the reference list first. If it appears sloppy, it may create a negative first impression that is difficult to reverse. Make sure the reference list looks professional, and try to include DOIs.\nLength: While shorter theses can be acceptable for difficult subjects, you should aim for a respectable length. But avoid padding your thesis with unnecessary, wordy sentences. Instead, include more definitions, explanations of key concepts, or a longer literature review. Make sure you have an adequate number of references, especially for models and terms that are not explained or defined in your thesis. You should be able to ask your thesis advisor for guidance on this.\nRepeat information. Remember that your examiner will skim your thesis. For instance, when you write a proof, never refer to a complicated quantity defined 7 pages prior. Just write down the complicated quantity one more time. Keeping track of “equation (34)” and “definition 3” et cetera is mentally exhausting and destroys the flow of the reader when the flow is needed the most.\nDo not use abbreviations. Your reader doesn’t know your abbreviations and will not appreciate them. Use search and replace to remove all abbreviation before submitting your thesis. The only exception are universally used abbreviation, used across disciplines. Write CV instead of curriculum vitae. Write DNA, not deoxyribonucleic acid. Don’t write ML; write machine learning instead; don’t write CV, write cross-validation instead.\nBe sure to explain anything that isn’t data science. You’re writing about the efficient market hypothesis? Don’t assume your examiner knows it - probably he knows next to nothing about finance. Feel free to add asides and interesting digressions, but always provide appropriate context.\n\nIt may be helpful to think of your audience as a fellow student for two reasons. First, examiners are neither omniscient nor omnipotent. Second, they will be looking for clear explanations of the basics to confirm that you understand the material."
  },
  {
    "objectID": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#read-some-grading-guidelines",
    "href": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#read-some-grading-guidelines",
    "title": "Some tips for the master thesis",
    "section": "Read some grading guidelines",
    "text": "Read some grading guidelines\nMany examiners won’t care about the grading guidelines, but some will. In any case, it’s worth it to read some guidelines, as it gives you an indication about how examiners think. For instance, take a look at the NTNU master thesis guidelines.."
  },
  {
    "objectID": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#comments-about-citations",
    "href": "posts/some-tips-for-thesis/some-tips-for-the-thesis.html#comments-about-citations",
    "title": "Some tips for the master thesis",
    "section": "Comments about citations",
    "text": "Comments about citations\nProper use of citations in a master thesis is slightly different from proper use of citations in a journal paper. This matters, as you shouldn’t blindly following the conventions for citations in journal papers when you write your thesis. In a journal paper, the main reasons to cite papers are to please the reader, pay proper respect to the people who originally figured something out, and please the reviewer of the journal article.\nThe reader of your paper care about your citations for reasons such as\n\nThe want to verify a claim or understand a claim better;\nThey want to see if you have cited their favorite author’s work;\nThey want to get a good overview of a subfield. In that case, they need to know most of the classics and some recent papers.\nSomething you wrote caught their attention.\n\nThese points are not that important for you, as the sole reader of your master thesis is very unlikely to engage with your master thesis more than he absolutely needs to. Instead, your goal should be to make the examiner pleased. And you do that by adding references to every claim that isn’t public knowledge. You do that by sprinkling citations about insights and related work throughout the document.\nThere are some common sense rules about citations, in particular when it comes to what to cite as evidence of a claim.\n\nCite the original source of a claim or fact. Yes, this means you should cite ’Fisher, R. A. (1936). “The Use of Multiple Measurements in Taxonomic Problems” (PDF). Annals of Eugenics. 7 (2)” when you talk about discriminant analysis, despite it being published in a journal of eugenics. If the paper is old as hell, you’d probably want to add a modern reference too (e.g., Elements of Statistical Learning). Some people will subtract points when you don’t cite the original source!\nPrefer journal articles to monographs and monograph to textbooks. Try to avoid citing your textbooks, especially bachelor level textbooks, as it makes you look unprofessional. Exceptions include the universally cited textbooks, e.g., Elements of Statistical Learning and Lehmann’s Testing Statistical Hypotheses. Check the number of citations of your textbook and find a more standard reference if the textbook is never cited.\nAvoid non-academic resources. Sometimes it’s very hard to find references for a claim in a scholarly resource. For instance, you might only be able to find something on the website of the SAS Institute. It is probably OK to have 1 - 2 of those in your thesis, but I strongly advice against littering your thesis with them. Again, the problem is that it makes your thesis look unprofessional."
  },
  {
    "objectID": "posts/two-lines/index.html",
    "href": "posts/two-lines/index.html",
    "title": "Correlation with two lines!",
    "section": "",
    "text": "What is this? A rant about an unimportant problem. Read at your own risk!\nEveryone knows the correlation coefficient. It measures the degree of linear dependence between two random variables \\(X\\) and \\(Y\\). It finds the best-fitting line \\(Y=a+bX\\) and measures how far the observed values of \\(Y\\) are from the predicted values of \\(y\\) given \\(X\\).\nThat should be familiar. But what if there are TWO parallel lines in the data?!\nThe data has been simulated from the model \\[\\begin{eqnarray*}\ny\\mid x,z=0 & \\sim & 2x+1+\\frac{1}{2}\\epsilon,\\\\\ny\\mid x,z=1 & \\sim & 2x+2+\\frac{1}{2}\\epsilon.\n\\end{eqnarray*}\\] where \\(z\\) is a group indicator and \\(\\epsilon\\) is standard normal. Moreover, the \\(x\\)s are \\(50\\) observations uniformly spaced on \\([0,1]\\). What is the marginal model, with \\(z\\) integrated out? It’s \\(2x + 3/2\\), displayed on the right. The plot on the right certainly doesn’t look like it contains two lines.\nIf we know the joint distribution of \\((X,Y,Z)\\) , generalizing the correlation is easy! The ordinary correlation coefficient can be written as \\(\\operatorname{Cor}(X,Y)=\\operatorname{sign}\\beta\\sqrt{R^{2}}\\),where \\[R^{2}=1-\\frac{\\min_{a,b}E\\left[(Y-a-bX)^{2}\\right]}{\\min_{\\mu}E\\left[(Y-\\mu)^{2}\\right]},\\] and \\(\\beta\\) is the minimizing slope of the numerator. Now just add the covariate \\(1[Z=1]\\) to the linear regression model in the denominator, and the resulting correlation becomes \\[\\operatorname{Cor}(X,Y;Z)=\\operatorname{sign}\\beta\\sqrt{R_\\star^{2}},\\] where \\[R^{2}_\\star=1-\\frac{\\min_{a,b}E\\left[(Y-a_{0}-a_{1}1[Z=1]-bX)^{2}\\right]}{\\min_{\\mu}E\\left[(Y-\\mu)^{2}\\right]},\\] and \\(\\beta\\) is the minimizing slope of the numerator (the sign of the slope is unambiguous since we’ve assumed the lines are parallel).\nCalculating this correlation coefficient is easy as pie.\nAnd its much larger than the “one-line correlation”, going from \\(0.62\\) to \\(0.85\\). But we can’t calculate it without knowing \\(Z\\)!"
  },
  {
    "objectID": "posts/two-lines/index.html#defining-a-correlation-for-two-parallel-lines",
    "href": "posts/two-lines/index.html#defining-a-correlation-for-two-parallel-lines",
    "title": "Correlation with two lines!",
    "section": "Defining a correlation for two parallel lines",
    "text": "Defining a correlation for two parallel lines\nSo, we know the joint distribution of \\((x,y)\\), and nothing more. How can think about the “two-lines correlation”? I can only think of one reasonable way to solve the problem. We need to find lower and upper bounds for \\(\\operatorname{Cor}(X,Y;Z)\\). Because we might not know \\(Z\\), but maybe we can find bounds for the correlation \\(\\operatorname{Cor}(X,Y;Z)\\) that we could have calculate had we known \\(Z\\).\n\nFinding bounds\nI can’t think of an efficient way to calculate upper bounds, but the lower bound is simple enough (in the case when the correlation is positive): It equals \\(\\operatorname{Cor}(X,Y)\\). When dealing with estimation problems, a reasonable estimator of the upper bound would be to make every possible split of the data into into two categories (according to z)and fit the regression model y ~ x + z. But that would be \\(2^n\\) combinations, which is obviously intractable for large \\(n\\). It might be possible to find exact algorithms that aren’t exponential in time, indeed, I would expect it, but I can’t justify spending more time on this problem.\nHowever, consider the following data\n\nn <- 15\nx <- mtcars$drat[1:n]\ny <- mtcars$wt[1:n]\n\nSince \\(2^{15}= 32768\\) we don’t need to fit that many linear regressions to estimate the upper correlation bound in this case.\n\n\nCode estimating a bunch of regression models.\nn <- 15\nx <- mtcars$drat[1:n]\ny <- mtcars$wt[1:n]\nn <- length(x)\n\nmat <- cbind(1, x, 0)\nminimum = Inf\n\nfor(k in seq(0:n)) {\n  icomb <- arrangements::icombinations(n, k)\n  choose(n, k)\n  for(i in seq(choose(n, k))) {\n    indices <- icomb$getnext()\n    mat[, 3] <- 0\n    mat[indices, 3] <- 1\n    mod <- lm.fit(mat, y)\n    current <- sum(mod$residuals^2)\n    if (current < minimum) {\n      minimum = current\n      z = indices\n      coef <- mod$coefficients\n    }\n  }  \n}\n\ncor2 <- sqrt(1 - minimum/sum((y - mean(y))^2)) * sign(coef[2])\ncor1 <- cor(x, y)\n\n\nFrom the code above we find that lower and upper bounds for the two lines correlation:\n\nc(lower = cor2, upper = cor1)\n\n   lower.x      upper \n-0.8834406 -0.6358763 \n\n\nLet’s also have a look at the classification of points that maximizes this correlation.\n\n\nCode for plot.\ncol <- rep(\"blue\", n)\ncol[z] <- \"red\"\nplot(x, y, col = col, xlab = \"Rear axle ratio\", ylab = \"Weight\",\n     pch = 20, main = \"Two lines or one line?\")\nabline(a = coef[1], b = coef[2], col = \"blue\")\nabline(a = coef[1] + coef[3], b = coef[2], col = \"red\")\nabline(lm(y ~ x))\n\n\n\n\n\nPlot of correlation with two lines.\n\n\n\n\nThe two lines correlation for this optimal classification equals the lower bound above."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html",
    "href": "posts/value-estimation/value-estimation.html",
    "title": "Estimating value from pairwise comparisons",
    "section": "",
    "text": "How can you estimate the value of research output? You could use pairwise comparisons, e.g., to ask specialists how much more valuable Darwin’s The Original of Species is than Dembski’s Intelligent Design. Then you can use these relative valuations to estimate absolute valuations."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#summary",
    "href": "posts/value-estimation/value-estimation.html#summary",
    "title": "Estimating value from pairwise comparisons",
    "section": "Summary",
    "text": "Summary\n\nEstimating values is hard. One way to elicit value estimates is ask researchers to compare two different items \\(A\\) and \\(B\\), asking how much better \\(A\\) is than \\(B\\). This makes the problem more concrete than just asking “what is the value of \\(A\\)?”. The Quantified Uncertainty Institute has made an app for doing this kind of thing, described here.\nNuño Sempere had a post about eliciting comparisons of research value from \\(6\\) effective altruism researchers. This is a more recent post about AI risk, but it uses distributions instead of point estimates.\nThis post proposes some technical solutions to problems introduced to me in Nuño’s post. In particular, it includes principled ways to\n\nestimate subjective values,\nmeasure consistency in pairwise value judgments,\nmeasure agreement between the raters,\naggregate subjective values.\nI also propose to use weighted least squares when the raters supply distributions instead of numbers. It is not clear to me it is worth it to ask for distributions in these kinds of questions though, as your uncertainty level can be modelled implicitly by comparing different pairwise comparisons.\n\nI use these methods on the data from the 6 researchers post.\n\nI’m assuming you have read the 6 researchers post recently. I think this post will be hard to read if you haven’t.\nNote: Also posted at the EA Forum. This document is a compiled Quarto file with source functions outside of the main document. The functions can be found in the source folder for this post. Also, thanks to Nuño Sempere for his comments on a draft of the post! Update 6/10/2022. Modified Gavin’s confidence interval for A Mathematical Theory to show the correct number. Update 4/11/2022. Some spelling fixes.\n\nWhat’s this about\nTable 1 contains the first \\(6\\) out of \\(36\\) responses from Gavin Leech. As you can see, he values Superintelligence \\(100\\) more than the Global Priorities Institute’s Research Agenda.\n\n\nA list of questions in the data set.\nknitr::kable(head(gavin[, 1:3]))\n\n\n\n\nTable 1: First 6 questions Gavin answered.\n\n\n\n\n\n\n\nsource\ntarget\ndistance\n\n\n\n\nThinking Fast and Slow\nThe Global Priorities Institute’s Research Agenda\n100\n\n\nThe Global Priorities Institute’s Research Agenda\nThe Mathematical Theory of Communication\n1000\n\n\nSuperintelligence\nThe Mathematical Theory of Communication\n10\n\n\nCategorizing Variants of Goodhart’s Law\nThe Vulnerable World Hypothesis\n10\n\n\nShallow evaluations of longtermist organizations\nThe motivated reasoning critique of effective altruism\n10\n\n\nShallow evaluations of longtermist organizations\nCategorizing Variants of Goodhart’s Law\n100\n\n\n\n\n\n\nMy first goal is to take relative value judgments such these and use them to estimate the true subjective values. In this case, I want to estimate the value that Gavin Leech places on every article in the data set, as contained in Table 2.\n\n\nA list of questions in the data set.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nknitr::kable(cbind(1:15, levels))\n\n\n\n\nTable 2: All \\(15\\) articles valuated in the data set. Question \\(3\\) is fixed to \\(1\\).\n\n\n\n\n\n\n\nlevels\n\n\n\n\n1\nA comment on setting up a charity\n\n\n2\nA Model of Patient Spending and Movement Building\n\n\n3\nCategorizing Variants of Goodhart’s Law\n\n\n4\nCenter for Election Science EA Wiki stub\n\n\n5\nDatabase of orgs relevant to longtermist/x-risk work\n\n\n6\nExtinguishing or preventing coal seam fires is a potential cause area\n\n\n7\nReversals in Psychology\n\n\n8\nShallow evaluations of longtermist organizations\n\n\n9\nSuperintelligence\n\n\n10\nThe Global Priorities Institute’s Research Agenda\n\n\n11\nThe Mathematical Theory of Communication\n\n\n12\nThe motivated reasoning critique of effective altruism\n\n\n13\nThe Vulnerable World Hypothesis\n\n\n14\nThinking Fast and Slow\n\n\n15\nWhat are some low-information priors that you find practically useful for thinking about the world?\n\n\n\n\n\n\nThe article Categorizing Variants of Goodhart’s Law has value fixed to \\(1\\). I will use the numbering above throughout this post."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "href": "posts/value-estimation/value-estimation.html#a-model-with-multiplicative-error-terms",
    "title": "Estimating value from pairwise comparisons",
    "section": "A model with multiplicative error terms",
    "text": "A model with multiplicative error terms\n\nMotivation and setup\nLet \\(\\eta_i\\) be the true subjective value of item \\(i\\), counting starting from \\(1\\). We will let \\(\\eta_3=1\\) in our setup, as Manheim and Garabrant’s Categorizing Variants of Goodhart’s Law was fixed to \\(1\\) in Nuño’s study, but we could have fixed any other item if we wanted to. Ideally, we would have observed the “distances” \\(d_{ij}=\\eta_i/\\eta_j\\) directly, but we don’t. Instead, we observe the distances with noise, \\(\\hat{d}_{ij}\\). We’ll assume a multiplicative model for these noise measurements:\n\\[\n\\hat{d}_{ij} = \\frac{\\eta_i}{\\eta_j}\\cdot e^{\\sigma \\epsilon_{ij}},\n\\] where \\(e^{\\sigma \\epsilon_{ij}}\\) is a positive noise term with standard deviation \\(\\sigma\\) on the log-scale. Now define \\(Y_{ij} = \\log \\hat{d}_{ij}\\) and \\(\\beta_i = \\log \\eta_i\\). Observe that \\(\\beta_3 = 0\\) by assumption. Now take logarimths on both sides of the equation above to get\n\\[\nY_{ij} = \\beta_i - \\beta_j + \\sigma\\epsilon_{ij},\n\\]\nwhich is a linear regression model. It looks like a two-way analysis of variance, but isn’t quite that, as we are only dealing with one factor here (the evaluated research) which appears twice in each equation. That said, the only difficulty in estimating this model is to make a model matrix for the regression coefficients. Observe that the residual standard deviation is fixed across items. We’ll take a look at how to reasonably relax this later on.\n\n\nIncidence matrices\nThe questions Gavin answered in the table above can be understood as a directed graph; I’ll call it the question graph. Figure 1 below contains Gavin’s question graph.\n\n\nPlotting question graph for Gavin.\nlevels <- levels(as.factor(c(gavin$source, gavin$target)))\nsource <- as.numeric(factor(gavin$source, levels = levels))\ntarget <- as.numeric(factor(gavin$target, levels = levels))\ngraph <- igraph::graph_from_edgelist(cbind(source, target))\nplot(graph)\n\n\n\n\n\nFigure 1: Question graph for Gavin.\n\n\n\n\nDirected graphs can be defined by their incidence matrices. If \\(G\\) is a directed graph with \\(k\\) nodes and \\(n\\) edges its incidence matrix \\(B\\) is the \\(n\\times k\\) matrix with elements \\[B_{ij}=\\begin{cases}\n-1 & \\text{if edge }e_{j}\\text{ leaves vertex }v_{i},\\\\\n1 & \\text{if edge }e_{j}\\text{ enters vertex }v_{i},\\\\\n0 & \\text{otherwise.}\n\\end{cases}\\]\nTable 3 contains Gavin’s incidence matrix.\n\n\nCalculation of incidence matrix for Gavin.\nn <- nrow(gavin)\nk <- 15\nb <- matrix(data = 0, nrow = n, ncol = k)\n\nfor (i in seq(n)) {\n  b[i, source[i]] <- -1\n  b[i, target[i]] <- 1\n}\n\nknitr::kable(t(b))\n\n\n\n\nTable 3: Incidence matrix for Gavin.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n-1\n0\n0\n0\n\n\n0\n0\n0\n-1\n0\n1\n1\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n1\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n-1\n0\n0\n0\n0\n1\n1\n-1\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n-1\n-1\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n0\n-1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-1\n0\n0\n0\n0\n0\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n-1\n0\n0\n0\n-1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n-1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nNow we can verify that \\[Y = B^T\\beta + \\sigma \\epsilon.\\] But there is one more thing to handle: We need to remove the fixed \\(\\beta\\), in our case \\(\\beta_3\\), to estimate the model. Define \\(B_\\star\\) and \\(\\beta_\\star\\) as the incidence matrix and coefficient vector with the fixed item removed. Then \\(Y = B_\\star^T\\beta_\\star + \\sigma \\epsilon\\) is ready to be estimated using linear regression.\n\n\nExample\nWe fit a linear regression to Gavin’s data. Table 4 contains the resulting estimates on the log-scale, rounded to the nearest whole number.\n\n\nParameter estimates for Gavin.\nmod <- pairwise_model(gavin, fixed = 3, keep_names = FALSE)\nvals <- round(c(coef(mod)[1:2], q3 = 0, coef(mod)[3:14]))\nknitr::kable(t(vals))\n\n\n\n\nTable 4: Parameter estimates for Gavin.\n\n\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nq9\nq10\nq11\nq12\nq13\nq14\nq15\n\n\n\n\n-12\n-3\n0\n-10\n-8\n-5\n-7\n-4\n7\n3\n8\n-3\n1\n-4\n-6\n\n\n\n\n\n\nWe can also make confidence intervals for the questions using the confint function. Figure 2 plots confidence intervals for all the \\(\\beta\\)s along with their estimates \\(\\hat{\\beta}\\).\n\n\nPlot of parametes and error bars.\nexped = exp(confint(mod))\nconfints = rbind(exped[1:2, ], c(1, 1), exped[3:14, ])\nrownames(confints) <- 1:15\nparams <- setNames(c(coef(mod)[1:2], 1, coef(mod)[3:14]), 1:15)\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = confints[, 2], yminus = confints[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 2: Plot of parametes and error bars for Gavin.\n\n\n\n\nThe \\(95\\%\\) confidence intervals are approximately equally wide on the log-scale, with the exception of question 3, which is fixed to \\(1\\). Let’s take a look at question 11, that of Shannon’s A Mathematical Theory of Communication. The confidence interval is\n\nconfints[11, ]\n\n      2.5 %      97.5 % \n   135.4981 150222.4654 \n\n\nThat’s really wide!\n\n\nAll the raters\nThe raters have IDs given in this table.\n\nx <- setNames(1:6, names(data_list))\nknitr::kable(t(x))\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n\nWe fit the model for all the raters and plot the resulting estimates in Figure 3. Notice the log-scale.\n\n\nFunction for plotting results for all raters.\nparameters = sapply(\n  data_list,\n  \\(data) {\n    coefs <- exp(coef(pairwise_model(data)))\n    c(coefs[1:2], 1, coefs[3:14])\n  })\nmatplot(parameters, log = \"y\", type = \"b\", ylab = \"Values\")\n\n\n\n\n\nFigure 3: Plot of parametes estimates for all raters.\n\n\n\n\nIt seems that the raters agree quite a bit."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "href": "posts/value-estimation/value-estimation.html#measuring-agreement",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring agreement",
    "text": "Measuring agreement\nOne of the easiest and most popular ways to measure agreement among two raters is Lin’s concordance coefficient (aka quadratically weighted Cohen’s kappa). It has an unpublished multirater generalization \\[\\frac{1^{T}\\Sigma1-\\text{tr}\\Sigma}{(R-1)\\text{tr}\\Sigma+R^{2}\\left(\\overline{\\mu^{2}}-\\overline{\\mu}^{2}\\right)}\\] Where \\(\\Sigma\\) is the covariance matrix of the estimated log rating, \\(\\mu_i\\) is the mean log rating by the \\(i\\)th rater, and \\(R\\) is the number of raters. I can explain the reasoning behind this measure in more detail if you want, but it’s the essentially unique extension of Lin’s concordance coefficient to multiple raters, as several generalizations yield the same formula. It’s bounded above by \\(1\\), which signifies perfect agreement. It’s defined in a way that’s very similar to the \\(R^2\\), so it’s OK to interpret the numbers as you would have interpreted an \\(R^2\\).\n\n\nCalculate concordance of the parameters.\nconcordance = function(x) {\n  n = nrow(x)\n  r = ncol(x)\n  sigma = cov(x) * (n - 1) / n\n  mu = colMeans(x)\n  trace = sum(diag(sigma))\n  top = sum(sigma) - trace\n  bottom = (r - 1) * trace + r ^ 2 * (mean(mu^2) - mean(mu)^2)\n  top / bottom\n}\nconcordance(log(parameters))\n\n\n[1] 0.698547\n\n\nI’m impressed by the level of agreement among the raters.\nWe can also construct a matrix of pairwise agreements.\n\n\nDefines the concordanc matrix using Lin’s coefficient.\nconcordances <- outer(seq(6), seq(6), Vectorize(\\(i,j) concordance(\n  cbind(log(parameters)[, i], log(parameters)[, j]))))\ncolnames(concordances) <- names(x)\nrownames(concordances) <- names(x)\nconcordances\n\n\n          linch      finn     gavin     jamie     misha     ozzie\nlinch 1.0000000 0.5442018 0.7562977 0.7705449 0.7779404 0.7016672\nfinn  0.5442018 1.0000000 0.4031429 0.5385316 0.4685602 0.5985986\ngavin 0.7562977 0.4031429 1.0000000 0.7237382 0.9106108 0.5996458\njamie 0.7705449 0.5385316 0.7237382 1.0000000 0.8253265 0.8926464\nmisha 0.7779404 0.4685602 0.9106108 0.8253265 1.0000000 0.7804517\nozzie 0.7016672 0.5985986 0.5996458 0.8926464 0.7804517 1.0000000\n\n\nNow we notice, e.g., that (i) Gavin agrees with Misha, (ii) Finn doesn’t agree much with anyone, (iii) Ozzie agrees with Jamie."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "href": "posts/value-estimation/value-estimation.html#identification-of-the-parameters",
    "title": "Estimating value from pairwise comparisons",
    "section": "Identification of the parameters",
    "text": "Identification of the parameters\nThe parameters \\(\\beta_\\star\\) are identified if and only if the question graph is connected. This has the practical consequence that the estimation is guaranteed to work whenever you design the question graph well enough. For instance, you do not need to think about avoiding cycles, having only one question per pair, etc.\nNow, it should be intuitively clear that \\(\\beta_\\star\\) cannot be identified when the graph fails to be connected, as there is no point(s) anchoring the scale of every \\(\\beta\\). Think about it this way. Suppose \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) form a connected component disconnected from \\(\\beta_{3}\\). If \\(\\beta_{1},\\beta_{2},\\beta_{4}\\) satisfy \\(Y=B_{[1,2,4]}^{T}\\beta_{[1,2,4]}+\\sigma\\epsilon\\), where \\([1,2,3]\\) denotes the appropriate indexing, then surely \\(\\gamma_{i}=\\beta_{i}+c\\) does so too for any \\(c\\), as every row of \\(B_\\star^{T}\\beta\\) is a difference \\(\\beta_{i}-\\beta_{j}\\), hence \\(\\gamma_{i}-\\gamma_{j}=\\beta_{i}+c-(\\beta_{j}+c)=\\beta_{i}-\\beta_{j}\\). The other way around is slightly trickier. It’s a theorem of algebraic graph theory that the rank of \\(B\\) equals \\(k-c\\), where \\(c\\) is the number of connected components. Suppose the graph is connected, so that the rank of \\(B\\) is \\(k-1\\). Since \\(B\\) does not have full rank (i.e., \\(k\\)), every row can be written as a linear combination of two other rows. In particular, the row associated with the fixed element can be removed without affecting the rank, hence the rank of \\(B_\\star\\) is \\(k-1\\) too. But there are \\(k-1\\) rows in \\(B_\\star\\), hence \\(B_\\star\\) has full rank. It follows that the parameters are identified."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "href": "posts/value-estimation/value-estimation.html#measuring-inconsistency",
    "title": "Estimating value from pairwise comparisons",
    "section": "Measuring inconsistency",
    "text": "Measuring inconsistency\nRecall the multiplicative equation for the reported distance: \\[D_{ij} = \\frac{X_i}{X_j}\\cdot e^{\\sigma \\epsilon_{ij}}\\] It’s clear that the distance will be noise-free if and only if \\(\\sigma = 0\\). Moreover, the distances will behave more and more erratically the larger \\(\\sigma\\) gets. If the distances tend to have erratically, the valuations will be inconsistent. Thus it’s natural to consider inconsistency estimators that are strictly increasing functions of \\(\\sigma\\). We’ll just use \\(\\sigma\\) for simplicity’s sake.\nThe consistencies of our 6 player are\n\n\nDefines the consistencies of all raters.\nconsistencies = lapply(data_list, \\(data) summary(pairwise_model(data))$sigma)\nknitr::kable(tibble::as_tibble(consistencies), digits = 2)\n\n\n\n\n\nlinch\nfinn\ngavin\njamie\nmisha\nozzie\n\n\n\n\n0.87\n1.04\n2.22\n0.86\n0.87\n0.93\n\n\n\n\n\nAll of these are roughly the same, except Gavin’s. That might be surprising since Nuño claimed Gavin is the most consistent of the raters. His inconsistency score is probably unfavourable since he has some serious outliers in his ratings, not because he’s inconsistent across the board. Ratings \\(33\\) and \\(36\\) appear to be especially inconsistent.\n\nplot(mod, which = 1, main = \"Plot for Gavin\")\n\n\n\n\nCompared it to the same plot for Jaime Sevilla.\n\nplot(pairwise_model(jamie), which = 1, main = \"Plot for Jaime\")\n\n\n\n\nLet’s see what happens if we remove the observations \\(31, 33, 34, 36\\) from Gavin’s data then.\n\ngavin_ <- gavin[setdiff(seq(nrow(gavin)), c(31, 33, 34, 36)), ]\nplot(pairwise_model(gavin_), which = 1, main = \"Plot for Gavin with outliers removed\")\n\n\n\n\nThe residual plot looks better now, and the inconsistency score becomes \\(\\sigma \\approx 0.87\\), in line with the other participants.\nMy take-away is that it would be beneficial to use robust linear regressions when estimating \\(\\beta\\). I’m not prioritizing studying this right now, but if someone were to invest serious amount of time in studying and applying statistical methods for this problem, I would strongly suggest taking a look at e.g. rlm.\n\nYou shouldn’t strive for consistency\nStriving for consistency requires you to follow a method. For instance, you can write down or try hard to remember what you have answered on previous questions, then use the right formula to deduce a consistent answer. I would advice against doing this though. When you compare two items against each other, just follow the priming of the shown items and let the statistical method do its work! If you’re trying hard to be consistent you’ll probably introduce some sort of bias, as you’ll essentially make the ratings dependent on their ordering. Also see the crowd within. The value-elicitation framework is similar to psychometrics, where you want every measurement to be as independent of every other measurement as possible when you condition on the latent variables.\nI also see little reason to use algorithms that prohibits cyclical comparisons, as there is no statistical reason to avoid them. (Only a psychological one, if you feel like you have to be consistent.) It’s also fine the ask the same question more than once – at least if you add some addition correlation term into the model. And have some time distance between the questions."
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#aggregation",
    "href": "posts/value-estimation/value-estimation.html#aggregation",
    "title": "Estimating value from pairwise comparisons",
    "section": "Aggregation",
    "text": "Aggregation\nWe estimate \\(\\beta\\) using a mixed effect model. \\[\\begin{eqnarray*}\nY_{j} & = & D_{j}^{T}\\beta_{j}+\\sigma\\epsilon,\\\\\n\\beta_{j} & \\sim & N(\\beta,\\Sigma).\n\\end{eqnarray*}\\]\nConceptually, this model implies that there is a true underlying \\(\\beta\\) for each question, but the raters only have incomplete access to it when they form their subjective valuation. So we have two sources of noise: First, the raters have a latent, noisy and subjective estimate of \\(\\beta\\), which we call \\(\\beta_j\\). Second, we only observe noisy measurements of \\(\\beta_j\\)s through our pairwise comparisons model. The matrix \\(\\Sigma\\) can be constrained to be diagonal, which makes estimation go faster.\nUsing lme4, I made a function pairwise_mixed_model that fits a mixed effects model to the data without an intercept. Check out the source if you want to know exactly what I’ve done.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3)\n\nboundary (singular) fit: see help('isSingular')\n\n\nUsing the mod object, we can plot (Figure 4) confidence intervals and estimates for the aggregate ratings.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[16:29, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 4: Confidence bars mixed effects model with uncorrelated random effects.\n\n\n\n\nThe confidence intervals in the plot are reasonably sized, but remember the \\(y\\)-axis is on the log-scale. Let’s take a look at the confidence interval for A Mathematical Theory of Communication again:\n\n\nConfidence interval for Mathematical Theory of Communication with uncorrelated random effects.\nround(exp(confint(mod, method = \"Wald\")[16:29, ])[10, ])\n\n\n 2.5 % 97.5 % \n    44  10740 \n\n\nThe uncertainty of the aggregate value is smaller than that of Gavin’s subjective value. But the uncertainty is still very, very large. I think the level of uncertainty is wrong though. Fixing it would probably require a modification of the model to allow for items of different difficulty, or maybe a non-multiplicative error structure. But there is also a counterfactual aspect here. It’s hard to say how quickly someone else would’ve invented information theory weren’t it for A Mathematical Theory. Different “concepts” about counterfactuals could potentially lead to different true \\(\\beta\\)s, as some readers consider them and some don’t. (See Linch’s comment)\n\nUsing correlated random effects\nWe can run a model with correlated random effects too.\n\nmod <- pairwise_mixed_model(data_list, fixed = 3, uncorrelated = FALSE)\n\nboundary (singular) fit: see help('isSingular')\n\n\nThe corresponding plot (Figure 5) is similar but not indistinguishable from Figure 4. The confidence intervals appear to smaller, but I don’t know if this is an estimation artifact or not. I also don’t know which model is better. Or if it will ever matter in practice which you use, but the uncorrelated model is faster to fit, as the correlated takes a couple of seconds. And that might matter in a production setting.\n\n\nDefines confidence intervals and estimates used for plotting.\nconf <- confint(mod, method = \"Wald\")[(120 - 13):120, ]\nparams <- c(lme4::fixef(mod)[1:2], 0, lme4::fixef(mod)[3:14])\nexped <- rbind(exp(conf)[1:2, ], c(1,1), exp(conf)[3:14, ])\n\nHmisc::errbar(x = 1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              log = \"y\", ylab = \"Value\", xlab = \"Question index\", type = \"b\")\ngrid()\nHmisc::errbar(x =  1:15, y = exp(params), yplus = exped[, 2], yminus = exped[, 1],\n              add = TRUE)\n\n\n\n\n\nFigure 5: Confidence bars mixed effects model with correlated random effects.\n\n\n\n\nThe resulting confidence interval for A Mathematical Theory of Communication gets smaller now.\n\n\nConfidence interval for Mathematical Theory of Communication with correlated random effects.\nround(exp(confint(mod, method = \"Wald\")[116, ]))\n\n\n 2.5 % 97.5 % \n    68   9602"
  },
  {
    "objectID": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "href": "posts/value-estimation/value-estimation.html#incorporating-uncertainty",
    "title": "Estimating value from pairwise comparisons",
    "section": "Incorporating uncertainty",
    "text": "Incorporating uncertainty\nInstead of rating the ratio \\(\\eta_i/\\eta_j\\) with a number, you might want give a distribution over \\(\\eta_i/\\eta_j\\), indicating your uncertainty, as done in e.g. this post. How could we work with such uncertain measurements? One possibility is to extract a log-mean and log-standard deviation from the distributions and then use the same method as I’ve described, but with weighted least squares instead of least squares. The weights will be \\(1/\\sigma_{ij}\\), where \\(\\sigma_{ij}\\) are the log-standard deviations of the distributions.\nThe formal reasoning behind this proposal goes as follows. If \\(\\eta_i/\\eta_j\\) is log-normal with some log-mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\), its logarithm is normal with mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\). This model implies that \\[\\frac{\\eta_i}{\\eta_j} = e^{\\mu_{ij}}e^{\\sigma_{ij}\\cdot\\epsilon_{ij}},\\] for some normally distributed \\(\\epsilon_{ij}\\). Taking logarithms on both sides, we obtain $$\n\\[\\beta_ - \\beta_j = \\mu_{ij} + \\sigma_{ij}\\epsilon_{ij}.\\] Since \\(\\epsilon_{ij}\\) is symmetric around \\(0\\), we have \\(\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{ij}\\epsilon_{ij}\\). If we add the “safety measure constant” \\(\\psi\\), with \\[\\mu_{ij} = \\beta_i - \\beta_j +\\psi\\sigma_{ij}\\epsilon_{ij}, \\tag{1}\\] this is a weighted least squares problem, with weights equal to \\(1/\\sigma_{ij}\\) and residual standard deviation \\(\\psi\\). It’s not clear to me if distributions are worth it.\nIn the summary I wrote that it’s not clear to me that it’s worth it to ask for distributions in pairwise comparisons, as your uncertainty level can be modeled implicitly by comparing different pairwise comparisons. What does this mean? Let’s simplify the model in Equation 1 so it contains two error terms, \\(\\sigma_i\\) and \\(\\sigma_j\\), one belonging to each question.\n\\[\\mu_{ij} = \\beta_i - \\beta_j + \\sigma_{i}\\epsilon_{i}+\\sigma_{j}\\epsilon_{j}. \\tag{2}\\]\nThis models allows you to have different uncertainties for different items, but doesn’t allow for idiosyncratic errors depending on interactions between the \\(i\\)th and \\(j\\)th items. Estimation of the model in Equation 2 should be easy to do efficiently, but I haven’t looked at the details. An idea would be to use the equation \\(\\operatorname{Var}(\\mu_{ij})=\\sigma_{i}^{2}+\\sigma_{j}^{2}\\) and iteratively refit weighted least squares models."
  }
]